{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor, RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>veh_value</th>\n",
       "      <th>exposure</th>\n",
       "      <th>veh_body</th>\n",
       "      <th>veh_age</th>\n",
       "      <th>gender</th>\n",
       "      <th>area</th>\n",
       "      <th>agecat</th>\n",
       "      <th>engine_type</th>\n",
       "      <th>max_power</th>\n",
       "      <th>...</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>e_bill</th>\n",
       "      <th>time_of_week_driven</th>\n",
       "      <th>time_driven</th>\n",
       "      <th>trm_len</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>high_education_ind</th>\n",
       "      <th>clm</th>\n",
       "      <th>numclaims</th>\n",
       "      <th>claimcst0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.444504</td>\n",
       "      <td>SEDAN</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>147</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6pm - 12am</td>\n",
       "      <td>6</td>\n",
       "      <td>640.448137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.45</td>\n",
       "      <td>0.562183</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>12</td>\n",
       "      <td>683.749691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.90</td>\n",
       "      <td>0.465244</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>159</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6pm - 12am</td>\n",
       "      <td>6</td>\n",
       "      <td>653.656117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.271039</td>\n",
       "      <td>PANVN</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>petrol</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>12pm - 6pm</td>\n",
       "      <td>12</td>\n",
       "      <td>642.574671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.141624</td>\n",
       "      <td>SEDAN</td>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>petrol</td>\n",
       "      <td>126</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>6</td>\n",
       "      <td>647.175035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  veh_value  exposure veh_body  veh_age gender area  agecat engine_type  \\\n",
       "0   1       0.77  0.444504    SEDAN        4      M    D       3      petrol   \n",
       "1   2       4.45  0.562183    STNWG        1      M    A       3      petrol   \n",
       "2   3       4.90  0.465244    STNWG        1      F    A       3      petrol   \n",
       "3   4       0.48  0.271039    PANVN        4      M    A       4      petrol   \n",
       "4   5       0.85  0.141624    SEDAN        4      F    A       5      petrol   \n",
       "\n",
       "   max_power  ...  marital_status e_bill time_of_week_driven  time_driven  \\\n",
       "0        147  ...               S      1             weekday   6pm - 12am   \n",
       "1        158  ...               S      1             weekday   6am - 12pm   \n",
       "2        159  ...               M      1             weekday   6pm - 12am   \n",
       "3         80  ...               S      1             weekday   12pm - 6pm   \n",
       "4        126  ...               S      0             weekday   6am - 12pm   \n",
       "\n",
       "  trm_len credit_score  high_education_ind  clm  numclaims  claimcst0  \n",
       "0       6   640.448137                 1.0    0          0        0.0  \n",
       "1      12   683.749691                 0.0    0          0        0.0  \n",
       "2       6   653.656117                 1.0    0          0        0.0  \n",
       "3      12   642.574671                 0.0    0          0        0.0  \n",
       "4       6   647.175035                 0.0    0          0        0.0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/InsNova_data_2023_train.csv')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22619, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                       0\n",
       "veh_value                0\n",
       "exposure                 0\n",
       "veh_body                 0\n",
       "veh_age                  0\n",
       "gender                   0\n",
       "area                     0\n",
       "agecat                   0\n",
       "engine_type              0\n",
       "max_power                0\n",
       "driving_history_score    0\n",
       "veh_color                0\n",
       "marital_status           0\n",
       "e_bill                   0\n",
       "time_of_week_driven      0\n",
       "time_driven              0\n",
       "trm_len                  0\n",
       "credit_score             0\n",
       "high_education_ind       0\n",
       "clm                      0\n",
       "numclaims                0\n",
       "claimcst0                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'veh_value', 'exposure', 'veh_body', 'veh_age', 'gender', 'area',\n",
       "       'agecat', 'engine_type', 'max_power', 'driving_history_score',\n",
       "       'veh_color', 'marital_status', 'e_bill', 'time_of_week_driven',\n",
       "       'time_driven', 'trm_len', 'credit_score', 'high_education_ind', 'clm',\n",
       "       'numclaims', 'claimcst0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veh_value</th>\n",
       "      <th>exposure</th>\n",
       "      <th>veh_body</th>\n",
       "      <th>veh_age</th>\n",
       "      <th>gender</th>\n",
       "      <th>area</th>\n",
       "      <th>agecat</th>\n",
       "      <th>engine_type</th>\n",
       "      <th>max_power</th>\n",
       "      <th>driving_history_score</th>\n",
       "      <th>...</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>e_bill</th>\n",
       "      <th>time_of_week_driven</th>\n",
       "      <th>time_driven</th>\n",
       "      <th>trm_len</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>high_education_ind</th>\n",
       "      <th>clm</th>\n",
       "      <th>numclaims</th>\n",
       "      <th>claimcst0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.444504</td>\n",
       "      <td>SEDAN</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>147</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6pm - 12am</td>\n",
       "      <td>6</td>\n",
       "      <td>640.448137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.45</td>\n",
       "      <td>0.562183</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>158</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>12</td>\n",
       "      <td>683.749691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.90</td>\n",
       "      <td>0.465244</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>159</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6pm - 12am</td>\n",
       "      <td>6</td>\n",
       "      <td>653.656117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.271039</td>\n",
       "      <td>PANVN</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>petrol</td>\n",
       "      <td>80</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>12pm - 6pm</td>\n",
       "      <td>12</td>\n",
       "      <td>642.574671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.141624</td>\n",
       "      <td>SEDAN</td>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>petrol</td>\n",
       "      <td>126</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>6</td>\n",
       "      <td>647.175035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   veh_value  exposure veh_body  veh_age gender area  agecat engine_type  \\\n",
       "0       0.77  0.444504    SEDAN        4      M    D       3      petrol   \n",
       "1       4.45  0.562183    STNWG        1      M    A       3      petrol   \n",
       "2       4.90  0.465244    STNWG        1      F    A       3      petrol   \n",
       "3       0.48  0.271039    PANVN        4      M    A       4      petrol   \n",
       "4       0.85  0.141624    SEDAN        4      F    A       5      petrol   \n",
       "\n",
       "   max_power  driving_history_score  ... marital_status e_bill  \\\n",
       "0        147                   67.0  ...              S      1   \n",
       "1        158                   76.0  ...              S      1   \n",
       "2        159                   58.0  ...              M      1   \n",
       "3         80                   72.0  ...              S      1   \n",
       "4        126                   91.0  ...              S      0   \n",
       "\n",
       "   time_of_week_driven time_driven trm_len  credit_score  high_education_ind  \\\n",
       "0              weekday  6pm - 12am       6    640.448137                 1.0   \n",
       "1              weekday  6am - 12pm      12    683.749691                 0.0   \n",
       "2              weekday  6pm - 12am       6    653.656117                 1.0   \n",
       "3              weekday  12pm - 6pm      12    642.574671                 0.0   \n",
       "4              weekday  6am - 12pm       6    647.175035                 0.0   \n",
       "\n",
       "   clm  numclaims  claimcst0  \n",
       "0    0          0        0.0  \n",
       "1    0          0        0.0  \n",
       "2    0          0        0.0  \n",
       "3    0          0        0.0  \n",
       "4    0          0        0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['id'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22619 entries, 0 to 22618\n",
      "Data columns (total 21 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   veh_value              22619 non-null  float64\n",
      " 1   exposure               22619 non-null  float64\n",
      " 2   veh_body               22619 non-null  object \n",
      " 3   veh_age                22619 non-null  int64  \n",
      " 4   gender                 22619 non-null  object \n",
      " 5   area                   22619 non-null  object \n",
      " 6   agecat                 22619 non-null  int64  \n",
      " 7   engine_type            22619 non-null  object \n",
      " 8   max_power              22619 non-null  int64  \n",
      " 9   driving_history_score  22619 non-null  float64\n",
      " 10  veh_color              22619 non-null  object \n",
      " 11  marital_status         22619 non-null  object \n",
      " 12  e_bill                 22619 non-null  int64  \n",
      " 13  time_of_week_driven    22619 non-null  object \n",
      " 14  time_driven            22619 non-null  object \n",
      " 15  trm_len                22619 non-null  int64  \n",
      " 16  credit_score           22619 non-null  float64\n",
      " 17  high_education_ind     22619 non-null  float64\n",
      " 18  clm                    22619 non-null  int64  \n",
      " 19  numclaims              22619 non-null  int64  \n",
      " 20  claimcst0              22619 non-null  float64\n",
      "dtypes: float64(6), int64(7), object(8)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22619.000000\n",
       "mean         1.868948\n",
       "std          1.278588\n",
       "min          0.000000\n",
       "25%          1.070000\n",
       "50%          1.570000\n",
       "75%          2.260000\n",
       "max         24.510000\n",
       "Name: veh_value, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['veh_value'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['veh_value'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22619.000000\n",
       "mean         0.433038\n",
       "std          0.272899\n",
       "min          0.001754\n",
       "25%          0.203696\n",
       "50%          0.384313\n",
       "75%          0.643316\n",
       "max          0.999378\n",
       "Name: exposure, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['exposure'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['exposure'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SEDAN    7376\n",
       "HBACK    6305\n",
       "STNWG    5436\n",
       "UTE      1530\n",
       "TRUCK     586\n",
       "HDTOP     510\n",
       "COUPE     279\n",
       "MIBUS     253\n",
       "PANVN     248\n",
       "MCARA      40\n",
       "CONVT      28\n",
       "BUS        19\n",
       "RDSTR       9\n",
       "Name: veh_body, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['veh_body'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_bodies, counts = [], []\n",
    "value_counts = df['veh_body'].value_counts()\n",
    "\n",
    "for body in value_counts.keys():\n",
    "    veh_bodies.append(body)\n",
    "    counts.append(value_counts[body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(veh_bodies, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    6694\n",
       "4    6285\n",
       "2    5474\n",
       "1    4166\n",
       "Name: veh_age, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['veh_age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_ages, counts = [], []\n",
    "value_counts = df['veh_age'].value_counts()\n",
    "\n",
    "for body in value_counts.keys():\n",
    "    veh_ages.append(body)\n",
    "    counts.append(value_counts[body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(veh_ages, counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F    12853\n",
       "M     9766\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    6944\n",
       "A    5450\n",
       "B    4466\n",
       "D    2620\n",
       "E    1951\n",
       "F    1188\n",
       "Name: area, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    5383\n",
       "3    5304\n",
       "2    4360\n",
       "5    3544\n",
       "6    2168\n",
       "1    1860\n",
       "Name: agecat, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['agecat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "petrol      14146\n",
       "dissel       4603\n",
       "hybrid       2107\n",
       "electric     1763\n",
       "Name: engine_type, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['engine_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['max_power'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['driving_history_score'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "white     4873\n",
       "gray      4766\n",
       "black     4359\n",
       "blue      2002\n",
       "silver    1818\n",
       "red       1360\n",
       "green     1316\n",
       "brown     1142\n",
       "yellow     983\n",
       "Name: veh_color, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['veh_color'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M    12401\n",
       "S    10218\n",
       "Name: marital_status, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14404\n",
       "0     8215\n",
       "Name: e_bill, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['e_bill'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weekday    18105\n",
       "weekend     4514\n",
       "Name: time_of_week_driven, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['time_of_week_driven'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12pm - 6pm     9102\n",
       "6am - 12pm     9000\n",
       "6pm - 12am     3385\n",
       "12am - 6 am    1132\n",
       "Name: time_driven, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['time_driven'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    16941\n",
       "6      5678\n",
       "Name: trm_len, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['trm_len'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['credit_score'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    19970\n",
       "1.0     2649\n",
       "Name: high_education_ind, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['high_education_ind'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21077\n",
       "1     1542\n",
       "Name: clm, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21077\n",
       "1     1439\n",
       "2       94\n",
       "3        9\n",
       "Name: numclaims, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['numclaims'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['claimcst0'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veh_value</th>\n",
       "      <th>exposure</th>\n",
       "      <th>veh_age</th>\n",
       "      <th>agecat</th>\n",
       "      <th>max_power</th>\n",
       "      <th>driving_history_score</th>\n",
       "      <th>e_bill</th>\n",
       "      <th>trm_len</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>high_education_ind</th>\n",
       "      <th>clm</th>\n",
       "      <th>numclaims</th>\n",
       "      <th>claimcst0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>veh_value</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000279</td>\n",
       "      <td>-0.539635</td>\n",
       "      <td>-0.066263</td>\n",
       "      <td>0.190229</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>0.126045</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>-0.008438</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.002859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exposure</th>\n",
       "      <td>-0.000279</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.038373</td>\n",
       "      <td>0.027080</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>-0.005429</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>0.402730</td>\n",
       "      <td>-0.081266</td>\n",
       "      <td>0.527214</td>\n",
       "      <td>0.116118</td>\n",
       "      <td>0.117565</td>\n",
       "      <td>0.039957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veh_age</th>\n",
       "      <td>-0.539635</td>\n",
       "      <td>0.038373</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028051</td>\n",
       "      <td>0.059412</td>\n",
       "      <td>-0.004086</td>\n",
       "      <td>-0.162433</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.030029</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.007544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agecat</th>\n",
       "      <td>-0.066263</td>\n",
       "      <td>0.027080</td>\n",
       "      <td>0.028051</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.049019</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>-0.017738</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.021883</td>\n",
       "      <td>-0.030845</td>\n",
       "      <td>-0.029969</td>\n",
       "      <td>-0.021520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_power</th>\n",
       "      <td>0.190229</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>0.059412</td>\n",
       "      <td>-0.049019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.004979</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.008413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>driving_history_score</th>\n",
       "      <td>0.010630</td>\n",
       "      <td>-0.005429</td>\n",
       "      <td>-0.004086</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>-0.001114</td>\n",
       "      <td>-0.003909</td>\n",
       "      <td>-0.003035</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.012507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e_bill</th>\n",
       "      <td>0.126045</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>-0.162433</td>\n",
       "      <td>-0.017738</td>\n",
       "      <td>0.004979</td>\n",
       "      <td>-0.004984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.012550</td>\n",
       "      <td>0.017685</td>\n",
       "      <td>-0.006264</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>-0.001108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trm_len</th>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.402730</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>-0.000865</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>-0.001114</td>\n",
       "      <td>-0.012550</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.007846</td>\n",
       "      <td>0.005382</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.006479</td>\n",
       "      <td>-0.000671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit_score</th>\n",
       "      <td>-0.002800</td>\n",
       "      <td>-0.081266</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>-0.003909</td>\n",
       "      <td>0.017685</td>\n",
       "      <td>-0.007846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.046448</td>\n",
       "      <td>-0.013757</td>\n",
       "      <td>-0.013117</td>\n",
       "      <td>-0.006191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_education_ind</th>\n",
       "      <td>-0.008438</td>\n",
       "      <td>0.527214</td>\n",
       "      <td>0.030029</td>\n",
       "      <td>0.021883</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>-0.003035</td>\n",
       "      <td>-0.006264</td>\n",
       "      <td>0.005382</td>\n",
       "      <td>-0.046448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.080958</td>\n",
       "      <td>0.082107</td>\n",
       "      <td>0.024714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clm</th>\n",
       "      <td>0.005282</td>\n",
       "      <td>0.116118</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>-0.030845</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>-0.007247</td>\n",
       "      <td>-0.013757</td>\n",
       "      <td>0.080958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965062</td>\n",
       "      <td>0.473932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numclaims</th>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.117565</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>-0.029969</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>-0.006479</td>\n",
       "      <td>-0.013117</td>\n",
       "      <td>0.082107</td>\n",
       "      <td>0.965062</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.476412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claimcst0</th>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.039957</td>\n",
       "      <td>0.007544</td>\n",
       "      <td>-0.021520</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>-0.001108</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>-0.006191</td>\n",
       "      <td>0.024714</td>\n",
       "      <td>0.473932</td>\n",
       "      <td>0.476412</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       veh_value  exposure   veh_age    agecat  max_power  \\\n",
       "veh_value               1.000000 -0.000279 -0.539635 -0.066263   0.190229   \n",
       "exposure               -0.000279  1.000000  0.038373  0.027080  -0.001208   \n",
       "veh_age                -0.539635  0.038373  1.000000  0.028051   0.059412   \n",
       "agecat                 -0.066263  0.027080  0.028051  1.000000  -0.049019   \n",
       "max_power               0.190229 -0.001208  0.059412 -0.049019   1.000000   \n",
       "driving_history_score   0.010630 -0.005429 -0.004086  0.000759   0.002779   \n",
       "e_bill                  0.126045 -0.000743 -0.162433 -0.017738   0.004979   \n",
       "trm_len                -0.000240  0.402730  0.016479 -0.000865   0.001494   \n",
       "credit_score           -0.002800 -0.081266  0.001467  0.000132   0.002260   \n",
       "high_education_ind     -0.008438  0.527214  0.030029  0.021883   0.002135   \n",
       "clm                     0.005282  0.116118  0.001758 -0.030845  -0.003863   \n",
       "numclaims               0.002191  0.117565  0.004123 -0.029969   0.000056   \n",
       "claimcst0               0.002859  0.039957  0.007544 -0.021520   0.008413   \n",
       "\n",
       "                       driving_history_score    e_bill   trm_len  \\\n",
       "veh_value                           0.010630  0.126045 -0.000240   \n",
       "exposure                           -0.005429 -0.000743  0.402730   \n",
       "veh_age                            -0.004086 -0.162433  0.016479   \n",
       "agecat                              0.000759 -0.017738 -0.000865   \n",
       "max_power                           0.002779  0.004979  0.001494   \n",
       "driving_history_score               1.000000 -0.004984 -0.001114   \n",
       "e_bill                             -0.004984  1.000000 -0.012550   \n",
       "trm_len                            -0.001114 -0.012550  1.000000   \n",
       "credit_score                       -0.003909  0.017685 -0.007846   \n",
       "high_education_ind                 -0.003035 -0.006264  0.005382   \n",
       "clm                                 0.008172  0.002568 -0.007247   \n",
       "numclaims                           0.008004  0.000235 -0.006479   \n",
       "claimcst0                           0.012507 -0.001108 -0.000671   \n",
       "\n",
       "                       credit_score  high_education_ind       clm  numclaims  \\\n",
       "veh_value                 -0.002800           -0.008438  0.005282   0.002191   \n",
       "exposure                  -0.081266            0.527214  0.116118   0.117565   \n",
       "veh_age                    0.001467            0.030029  0.001758   0.004123   \n",
       "agecat                     0.000132            0.021883 -0.030845  -0.029969   \n",
       "max_power                  0.002260            0.002135 -0.003863   0.000056   \n",
       "driving_history_score     -0.003909           -0.003035  0.008172   0.008004   \n",
       "e_bill                     0.017685           -0.006264  0.002568   0.000235   \n",
       "trm_len                   -0.007846            0.005382 -0.007247  -0.006479   \n",
       "credit_score               1.000000           -0.046448 -0.013757  -0.013117   \n",
       "high_education_ind        -0.046448            1.000000  0.080958   0.082107   \n",
       "clm                       -0.013757            0.080958  1.000000   0.965062   \n",
       "numclaims                 -0.013117            0.082107  0.965062   1.000000   \n",
       "claimcst0                 -0.006191            0.024714  0.473932   0.476412   \n",
       "\n",
       "                       claimcst0  \n",
       "veh_value               0.002859  \n",
       "exposure                0.039957  \n",
       "veh_age                 0.007544  \n",
       "agecat                 -0.021520  \n",
       "max_power               0.008413  \n",
       "driving_history_score   0.012507  \n",
       "e_bill                 -0.001108  \n",
       "trm_len                -0.000671  \n",
       "credit_score           -0.006191  \n",
       "high_education_ind      0.024714  \n",
       "clm                     0.473932  \n",
       "numclaims               0.476412  \n",
       "claimcst0               1.000000  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN MODEL METRIC (between -1 and 1, 1 is perfect, 0 is like random guessing): \n",
    "def calculate_normalized_gini_regression(true_values, predicted_values):\n",
    "    combined_data = list(zip(true_values, predicted_values))\n",
    "    combined_data.sort(key=lambda x: x[1])\n",
    "    \n",
    "    n = len(combined_data)\n",
    "    \n",
    "    cumulative_true = [0]\n",
    "    cumulative_pred = [0]\n",
    "\n",
    "    for i in range(n):\n",
    "        cumulative_true.append(cumulative_true[i] + combined_data[i][0])\n",
    "        cumulative_pred.append(cumulative_pred[i] + combined_data[i][1])\n",
    "    \n",
    "    total_area_perfect_equality = cumulative_true[-1] * cumulative_pred[-1]\n",
    "    auc = (total_area_perfect_equality - 0.5 * (cumulative_true[n] * cumulative_pred[n])) / total_area_perfect_equality\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=['veh_body', 'veh_age', 'gender', 'area', 'agecat', 'engine_type', 'veh_color', 'marital_status', 'e_bill', 'time_of_week_driven', 'time_driven', 'trm_len', 'high_education_ind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    21077\n",
      "1     1542\n",
      "Name: clm, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "clm_df = df_dummies.drop(columns=['numclaims', 'claimcst0'])\n",
    "\n",
    "print(clm_df['clm'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21077, 64)\n",
      "(1542, 64)\n"
     ]
    }
   ],
   "source": [
    "majority_class = clm_df[clm_df['clm'] == 0]\n",
    "minority_class = clm_df[clm_df['clm'] == 1]\n",
    "\n",
    "print(majority_class.shape)\n",
    "print(minority_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21077, 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_minority = resample(minority_class, replace=True, n_samples=len(majority_class))\n",
    "\n",
    "oversampled_minority.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veh_value</th>\n",
       "      <th>exposure</th>\n",
       "      <th>max_power</th>\n",
       "      <th>driving_history_score</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>clm</th>\n",
       "      <th>veh_body_BUS</th>\n",
       "      <th>veh_body_CONVT</th>\n",
       "      <th>veh_body_COUPE</th>\n",
       "      <th>veh_body_HBACK</th>\n",
       "      <th>...</th>\n",
       "      <th>time_of_week_driven_weekday</th>\n",
       "      <th>time_of_week_driven_weekend</th>\n",
       "      <th>time_driven_12am - 6 am</th>\n",
       "      <th>time_driven_12pm - 6pm</th>\n",
       "      <th>time_driven_6am - 12pm</th>\n",
       "      <th>time_driven_6pm - 12am</th>\n",
       "      <th>trm_len_6</th>\n",
       "      <th>trm_len_12</th>\n",
       "      <th>high_education_ind_0.0</th>\n",
       "      <th>high_education_ind_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.88</td>\n",
       "      <td>0.914029</td>\n",
       "      <td>152</td>\n",
       "      <td>99.0</td>\n",
       "      <td>645.024556</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.54</td>\n",
       "      <td>0.171395</td>\n",
       "      <td>139</td>\n",
       "      <td>99.0</td>\n",
       "      <td>649.726753</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.38</td>\n",
       "      <td>0.808889</td>\n",
       "      <td>169</td>\n",
       "      <td>38.0</td>\n",
       "      <td>650.312402</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.04</td>\n",
       "      <td>0.054398</td>\n",
       "      <td>218</td>\n",
       "      <td>94.0</td>\n",
       "      <td>640.376676</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.074504</td>\n",
       "      <td>131</td>\n",
       "      <td>90.0</td>\n",
       "      <td>659.310066</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   veh_value  exposure  max_power  driving_history_score  credit_score  clm  \\\n",
       "0       2.88  0.914029        152                   99.0    645.024556    1   \n",
       "1       1.54  0.171395        139                   99.0    649.726753    0   \n",
       "2       1.38  0.808889        169                   38.0    650.312402    1   \n",
       "3       2.04  0.054398        218                   94.0    640.376676    1   \n",
       "4       0.70  0.074504        131                   90.0    659.310066    0   \n",
       "\n",
       "   veh_body_BUS  veh_body_CONVT  veh_body_COUPE  veh_body_HBACK  ...  \\\n",
       "0             0               0               0               0  ...   \n",
       "1             0               0               0               0  ...   \n",
       "2             0               0               0               0  ...   \n",
       "3             0               0               0               0  ...   \n",
       "4             0               0               0               1  ...   \n",
       "\n",
       "   time_of_week_driven_weekday  time_of_week_driven_weekend  \\\n",
       "0                            1                            0   \n",
       "1                            1                            0   \n",
       "2                            1                            0   \n",
       "3                            1                            0   \n",
       "4                            1                            0   \n",
       "\n",
       "   time_driven_12am - 6 am  time_driven_12pm - 6pm  time_driven_6am - 12pm  \\\n",
       "0                        0                       1                       0   \n",
       "1                        0                       1                       0   \n",
       "2                        0                       0                       0   \n",
       "3                        0                       1                       0   \n",
       "4                        0                       1                       0   \n",
       "\n",
       "   time_driven_6pm - 12am  trm_len_6  trm_len_12  high_education_ind_0.0  \\\n",
       "0                       0          0           1                       0   \n",
       "1                       0          1           0                       1   \n",
       "2                       1          0           1                       1   \n",
       "3                       0          1           0                       1   \n",
       "4                       0          0           1                       1   \n",
       "\n",
       "   high_education_ind_1.0  \n",
       "0                       1  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clm_balanced_df = pd.concat([majority_class, oversampled_minority])\n",
    "clm_balanced_df = clm_balanced_df.sample(frac=1)\n",
    "clm_balanced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "clm_balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42154, 63)\n",
      "(42154,)\n"
     ]
    }
   ],
   "source": [
    "clm_x, clm_y = clm_balanced_df.drop(columns=['clm']), clm_balanced_df['clm']\n",
    "\n",
    "print(clm_x.shape)\n",
    "print(clm_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28102, 63) (14052, 63) (28102,) (14052,)\n"
     ]
    }
   ],
   "source": [
    "clm_x_train, clm_x_test, clm_y_train, clm_y_test = train_test_split(clm_x, clm_y, test_size=1 / 3)\n",
    "\n",
    "print(clm_x_train.shape, clm_x_test.shape, clm_y_train.shape, clm_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm_scaler = StandardScaler()\n",
    "clm_x_train = clm_scaler.fit_transform(clm_x_train)\n",
    "clm_x_test = clm_scaler.transform(clm_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/clm_scaler.pickle', 'wb') as f:\n",
    "    pickle.dump(clm_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_clm_x, original_clm_y = clm_df.drop(columns=['clm']), clm_df['clm']\n",
    "original_clm_x = clm_scaler.transform(original_clm_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num runs: 243\n",
      "Best results so far: train = 0.7470998505444453, test = 0.7329205807002562\n",
      "Remaining runs: 242\n",
      "Remaining runs: 241\n",
      "Remaining runs: 240\n",
      "Remaining runs: 239\n",
      "Remaining runs: 238\n",
      "Remaining runs: 237\n",
      "Remaining runs: 236\n",
      "Best results so far: train = 0.7979503238203687, test = 0.7771847423854256\n",
      "Remaining runs: 235\n",
      "Remaining runs: 234\n",
      "Best results so far: train = 0.9570493203330723, test = 0.9157415314545972\n",
      "Remaining runs: 233\n",
      "Remaining runs: 232\n",
      "Remaining runs: 231\n",
      "Remaining runs: 230\n",
      "Remaining runs: 229\n",
      "Best results so far: train = 0.960785709202192, test = 0.9216481639624253\n",
      "Remaining runs: 228\n",
      "Remaining runs: 227\n",
      "Remaining runs: 226\n",
      "Remaining runs: 225\n",
      "Remaining runs: 224\n",
      "Remaining runs: 223\n",
      "Remaining runs: 222\n",
      "Remaining runs: 221\n",
      "Remaining runs: 220\n",
      "Remaining runs: 219\n",
      "Remaining runs: 218\n",
      "Remaining runs: 217\n",
      "Remaining runs: 216\n",
      "Best results so far: train = 0.9816739022133656, test = 0.9500426985482494\n",
      "Remaining runs: 215\n",
      "Remaining runs: 214\n",
      "Remaining runs: 213\n",
      "Remaining runs: 212\n",
      "Remaining runs: 211\n",
      "Remaining runs: 210\n",
      "Remaining runs: 209\n",
      "Remaining runs: 208\n",
      "Remaining runs: 207\n",
      "Remaining runs: 206\n",
      "Remaining runs: 205\n",
      "Remaining runs: 204\n",
      "Remaining runs: 203\n",
      "Remaining runs: 202\n",
      "Remaining runs: 201\n",
      "Remaining runs: 200\n",
      "Remaining runs: 199\n",
      "Remaining runs: 198\n",
      "Remaining runs: 197\n",
      "Remaining runs: 196\n",
      "Remaining runs: 195\n",
      "Remaining runs: 194\n",
      "Remaining runs: 193\n",
      "Remaining runs: 192\n",
      "Remaining runs: 191\n",
      "Remaining runs: 190\n",
      "Best results so far: train = 0.9846985979645577, test = 0.951821804725306\n",
      "Remaining runs: 189\n",
      "Remaining runs: 188\n",
      "Remaining runs: 187\n",
      "Remaining runs: 186\n",
      "Remaining runs: 185\n",
      "Remaining runs: 184\n",
      "Remaining runs: 183\n",
      "Remaining runs: 182\n",
      "Remaining runs: 181\n",
      "Remaining runs: 180\n",
      "Remaining runs: 179\n",
      "Remaining runs: 178\n",
      "Remaining runs: 177\n",
      "Remaining runs: 176\n",
      "Remaining runs: 175\n",
      "Remaining runs: 174\n",
      "Remaining runs: 173\n",
      "Remaining runs: 172\n",
      "Remaining runs: 171\n",
      "Remaining runs: 170\n",
      "Remaining runs: 169\n",
      "Remaining runs: 168\n",
      "Remaining runs: 167\n",
      "Remaining runs: 166\n",
      "Remaining runs: 165\n",
      "Remaining runs: 164\n",
      "Remaining runs: 163\n",
      "Best results so far: train = 0.9919222831115223, test = 0.9666951323654995\n",
      "Remaining runs: 162\n",
      "Remaining runs: 161\n",
      "Remaining runs: 160\n",
      "Remaining runs: 159\n",
      "Remaining runs: 158\n",
      "Remaining runs: 157\n",
      "Remaining runs: 156\n",
      "Remaining runs: 155\n",
      "Remaining runs: 154\n",
      "Remaining runs: 153\n",
      "Remaining runs: 152\n",
      "Remaining runs: 151\n",
      "Remaining runs: 150\n",
      "Remaining runs: 149\n",
      "Remaining runs: 148\n",
      "Remaining runs: 147\n",
      "Remaining runs: 146\n",
      "Remaining runs: 145\n",
      "Remaining runs: 144\n",
      "Remaining runs: 143\n",
      "Remaining runs: 142\n",
      "Remaining runs: 141\n",
      "Remaining runs: 140\n",
      "Remaining runs: 139\n",
      "Remaining runs: 138\n",
      "Remaining runs: 137\n",
      "Remaining runs: 136\n",
      "Remaining runs: 135\n",
      "Remaining runs: 134\n",
      "Remaining runs: 133\n",
      "Remaining runs: 132\n",
      "Remaining runs: 131\n",
      "Remaining runs: 130\n",
      "Remaining runs: 129\n",
      "Remaining runs: 128\n",
      "Remaining runs: 127\n",
      "Remaining runs: 126\n",
      "Remaining runs: 125\n",
      "Remaining runs: 124\n",
      "Remaining runs: 123\n",
      "Remaining runs: 122\n",
      "Remaining runs: 121\n",
      "Remaining runs: 120\n",
      "Remaining runs: 119\n",
      "Remaining runs: 118\n",
      "Remaining runs: 117\n",
      "Remaining runs: 116\n",
      "Remaining runs: 115\n",
      "Remaining runs: 114\n",
      "Remaining runs: 113\n",
      "Remaining runs: 112\n",
      "Remaining runs: 111\n",
      "Remaining runs: 110\n",
      "Remaining runs: 109\n",
      "Remaining runs: 108\n",
      "Remaining runs: 107\n",
      "Remaining runs: 106\n",
      "Remaining runs: 105\n",
      "Remaining runs: 104\n",
      "Remaining runs: 103\n",
      "Remaining runs: 102\n",
      "Remaining runs: 101\n",
      "Remaining runs: 100\n",
      "Remaining runs: 99\n",
      "Remaining runs: 98\n",
      "Remaining runs: 97\n",
      "Remaining runs: 96\n",
      "Remaining runs: 95\n",
      "Remaining runs: 94\n",
      "Remaining runs: 93\n",
      "Remaining runs: 92\n",
      "Remaining runs: 91\n",
      "Remaining runs: 90\n",
      "Remaining runs: 89\n",
      "Remaining runs: 88\n",
      "Remaining runs: 87\n",
      "Remaining runs: 86\n",
      "Remaining runs: 85\n",
      "Remaining runs: 84\n",
      "Remaining runs: 83\n",
      "Remaining runs: 82\n",
      "Remaining runs: 81\n",
      "Remaining runs: 80\n",
      "Remaining runs: 79\n",
      "Remaining runs: 78\n",
      "Remaining runs: 77\n",
      "Remaining runs: 76\n",
      "Remaining runs: 75\n",
      "Remaining runs: 74\n",
      "Remaining runs: 73\n",
      "Remaining runs: 72\n",
      "Remaining runs: 71\n",
      "Remaining runs: 70\n",
      "Best results so far: train = 0.9980784285815956, test = 0.9826359237119271\n",
      "Remaining runs: 69\n",
      "Remaining runs: 68\n",
      "Remaining runs: 67\n",
      "Remaining runs: 66\n",
      "Remaining runs: 65\n",
      "Remaining runs: 64\n",
      "Remaining runs: 63\n",
      "Remaining runs: 62\n",
      "Remaining runs: 61\n",
      "Remaining runs: 60\n",
      "Remaining runs: 59\n",
      "Remaining runs: 58\n",
      "Remaining runs: 57\n",
      "Remaining runs: 56\n",
      "Remaining runs: 55\n",
      "Remaining runs: 54\n",
      "Remaining runs: 53\n",
      "Remaining runs: 52\n",
      "Remaining runs: 51\n",
      "Remaining runs: 50\n",
      "Remaining runs: 49\n",
      "Remaining runs: 48\n",
      "Remaining runs: 47\n",
      "Remaining runs: 46\n",
      "Remaining runs: 45\n",
      "Remaining runs: 44\n",
      "Remaining runs: 43\n",
      "Remaining runs: 42\n",
      "Remaining runs: 41\n",
      "Remaining runs: 40\n",
      "Remaining runs: 39\n",
      "Remaining runs: 38\n",
      "Remaining runs: 37\n",
      "Remaining runs: 36\n",
      "Remaining runs: 35\n",
      "Remaining runs: 34\n",
      "Remaining runs: 33\n",
      "Remaining runs: 32\n",
      "Remaining runs: 31\n",
      "Remaining runs: 30\n",
      "Remaining runs: 29\n",
      "Remaining runs: 28\n",
      "Remaining runs: 27\n",
      "Remaining runs: 26\n",
      "Remaining runs: 25\n",
      "Remaining runs: 24\n",
      "Remaining runs: 23\n",
      "Remaining runs: 22\n",
      "Remaining runs: 21\n",
      "Remaining runs: 20\n",
      "Remaining runs: 19\n",
      "Remaining runs: 18\n",
      "Remaining runs: 17\n",
      "Remaining runs: 16\n",
      "Remaining runs: 15\n",
      "Remaining runs: 14\n",
      "Remaining runs: 13\n",
      "Remaining runs: 12\n",
      "Remaining runs: 11\n",
      "Remaining runs: 10\n",
      "Remaining runs: 9\n",
      "Remaining runs: 8\n",
      "Remaining runs: 7\n",
      "Remaining runs: 6\n",
      "Remaining runs: 5\n",
      "Remaining runs: 4\n",
      "Remaining runs: 3\n",
      "Remaining runs: 2\n",
      "Remaining runs: 1\n",
      "Remaining runs: 0\n",
      "0.9826359237119271\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 20, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 5, 'min_samples_split': 3, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "all_combos = []\n",
    "\n",
    "for n_estimators in [5, 10, 15, 20, 25, 50, 100, 200, 300]:\n",
    "    for min_samples_leaf in [5, 10, 15, 20, 25, 50]:\n",
    "        for max_depth in [3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "            for min_samples_split in [2, 3, 4, 5, 10, 15]:\n",
    "                for criterion in ['gini', 'entropy', 'log_loss']:\n",
    "                    all_combos.append((n_estimators, min_samples_leaf, max_depth, min_samples_split, criterion))\n",
    "\n",
    "percentage_to_try = 0.025\n",
    "n_runs = int(percentage_to_try * len(all_combos))\n",
    "combos_to_try = random.sample(all_combos, n_runs)\n",
    "print(f'Num runs: {n_runs}')\n",
    "\n",
    "best_test_accuracy, clm_rf_model = -np.inf, None\n",
    "\n",
    "for n_estimators, min_samples_leaf, max_depth, min_samples_split, criterion in combos_to_try:\n",
    "    curr_model = RandomForestClassifier(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion)\n",
    "    curr_model.fit(clm_x_train, clm_y_train)\n",
    "\n",
    "    clm_y_train_pred = curr_model.predict(clm_x_train)\n",
    "    clm_y_test_pred = curr_model.predict(clm_x_test)\n",
    "\n",
    "    train_accuracy = accuracy_score(clm_y_train, clm_y_train_pred)\n",
    "    test_accuracy = accuracy_score(clm_y_test, clm_y_test_pred)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy, clm_rf_model = test_accuracy, curr_model\n",
    "        print(f'Best results so far: train = {train_accuracy}, test = {test_accuracy}')\n",
    "\n",
    "        # Early stopping\n",
    "        if best_test_accuracy >= 0.99:\n",
    "            break\n",
    "\n",
    "    n_runs -= 1\n",
    "    print(f'Remaining runs: {n_runs}')\n",
    "\n",
    "print(best_test_accuracy)\n",
    "print(clm_rf_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9911136655024537\n",
      "(array([0, 1]), array([20904,  1715]))\n",
      "0    21077\n",
      "1     1542\n",
      "Name: clm, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "original_clm_rf_pred = clm_rf_model.predict(original_clm_x)\n",
    "\n",
    "print(accuracy_score(original_clm_y, original_clm_rf_pred))\n",
    "print(np.unique(original_clm_rf_pred, return_counts=True))\n",
    "print(original_clm_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/clm_rf.pickle', 'wb') as f:\n",
    "    pickle.dump(clm_rf_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num runs: 97\n",
      "Best results so far: train = 0.9919222831115223, test = 0.9516083119840593\n",
      "Remaining runs: 96\n",
      "Best results so far: train = 0.9973311508077717, test = 0.9743808710503843\n",
      "Remaining runs: 95\n",
      "Best results so far: train = 0.9997153227528289, test = 0.9837033874181611\n",
      "Remaining runs: 94\n",
      "Best results so far: train = 1.0, test = 0.987688585254768\n",
      "Remaining runs: 93\n",
      "Remaining runs: 92\n",
      "Remaining runs: 91\n",
      "Remaining runs: 90\n",
      "Remaining runs: 89\n",
      "Remaining runs: 88\n",
      "Remaining runs: 87\n",
      "Remaining runs: 86\n",
      "Best results so far: train = 1.0, test = 0.9940222032450897\n",
      "0.9940222032450897\n",
      "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 5, 'min_samples_split': 3, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 300, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "all_combos = []\n",
    "\n",
    "for n_estimators in [5, 10, 15, 20, 25, 50, 100, 200, 300]:\n",
    "    for min_samples_leaf in [5, 10, 15, 20, 25, 50]:\n",
    "        for max_depth in [3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "            for min_samples_split in [2, 3, 4, 5, 10, 15]:\n",
    "                for learning_rate in [0.01, 0.1, 0.2]:\n",
    "                    all_combos.append((n_estimators, min_samples_leaf, max_depth, min_samples_split, learning_rate))\n",
    "\n",
    "percentage_to_try = 0.01\n",
    "n_runs = int(percentage_to_try * len(all_combos))\n",
    "combos_to_try = random.sample(all_combos, n_runs)\n",
    "print(f'Num runs: {n_runs}')\n",
    "\n",
    "best_test_accuracy, clm_gb_model = -np.inf, None\n",
    "\n",
    "for n_estimators, min_samples_leaf, max_depth, min_samples_split, learning_rate in combos_to_try:\n",
    "    curr_model = GradientBoostingClassifier(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_depth=max_depth, min_samples_split=min_samples_split, learning_rate=learning_rate)\n",
    "    curr_model.fit(clm_x_train, clm_y_train)\n",
    "\n",
    "    clm_y_train_pred = curr_model.predict(clm_x_train)\n",
    "    clm_y_test_pred = curr_model.predict(clm_x_test)\n",
    "\n",
    "    train_accuracy = accuracy_score(clm_y_train, clm_y_train_pred)\n",
    "    test_accuracy = accuracy_score(clm_y_test, clm_y_test_pred)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy, clm_gb_model = test_accuracy, curr_model\n",
    "        print(f'Best results so far: train = {train_accuracy}, test = {test_accuracy}')\n",
    "\n",
    "        # Early stopping\n",
    "        if best_test_accuracy >= 0.99:\n",
    "            break\n",
    "\n",
    "    n_runs -= 1\n",
    "    print(f'Remaining runs: {n_runs}')\n",
    "\n",
    "print(best_test_accuracy)\n",
    "print(clm_gb_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9962863079711747\n",
      "(array([0, 1]), array([20993,  1626]))\n",
      "0    21077\n",
      "1     1542\n",
      "Name: clm, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "original_clm_gb_pred = clm_gb_model.predict(original_clm_x)\n",
    "\n",
    "print(accuracy_score(original_clm_y, original_clm_gb_pred))\n",
    "print(np.unique(original_clm_gb_pred, return_counts=True))\n",
    "print(original_clm_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/clm_gb.pickle', 'wb') as f:\n",
    "    pickle.dump(clm_gb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22619,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The gradient boosting model seems to be slighly better for claim predictions\n",
    "clm_model, clm_pred = clm_gb_model, original_clm_gb_pred\n",
    "clm_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### numclaims model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21077\n",
       "1     1439\n",
       "2       94\n",
       "3        9\n",
       "Name: numclaims, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_claims_df = df_dummies.drop(columns=['clm', 'claimcst0'])\n",
    "\n",
    "num_claims_df['numclaims'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn on the impure claim predictions\n",
    "assert len(num_claims_df) == len(clm_pred)\n",
    "\n",
    "num_claims_df['clm'] = clm_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21077, 65)\n",
      "(1439, 65)\n",
      "(94, 65)\n",
      "(9, 65)\n"
     ]
    }
   ],
   "source": [
    "majority_class = num_claims_df[num_claims_df['numclaims'] == 0]\n",
    "minority_class_1 = num_claims_df[num_claims_df['numclaims'] == 1]\n",
    "minority_class_2 = num_claims_df[num_claims_df['numclaims'] == 2]\n",
    "minority_class_3 = num_claims_df[num_claims_df['numclaims'] == 3]\n",
    "\n",
    "print(majority_class.shape)\n",
    "print(minority_class_1.shape)\n",
    "print(minority_class_2.shape)\n",
    "print(minority_class_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21077, 65)\n",
      "(21077, 65)\n",
      "(21077, 65)\n"
     ]
    }
   ],
   "source": [
    "oversampled_minority_1 = resample(minority_class_1, replace=True, n_samples=len(majority_class))\n",
    "oversampled_minority_2 = resample(minority_class_2, replace=True, n_samples=len(majority_class))\n",
    "oversampled_minority_3 = resample(minority_class_3, replace=True, n_samples=len(majority_class))\n",
    "\n",
    "print(oversampled_minority_1.shape)\n",
    "print(oversampled_minority_2.shape)\n",
    "print(oversampled_minority_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veh_value</th>\n",
       "      <th>exposure</th>\n",
       "      <th>max_power</th>\n",
       "      <th>driving_history_score</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>numclaims</th>\n",
       "      <th>veh_body_BUS</th>\n",
       "      <th>veh_body_CONVT</th>\n",
       "      <th>veh_body_COUPE</th>\n",
       "      <th>veh_body_HBACK</th>\n",
       "      <th>...</th>\n",
       "      <th>time_of_week_driven_weekend</th>\n",
       "      <th>time_driven_12am - 6 am</th>\n",
       "      <th>time_driven_12pm - 6pm</th>\n",
       "      <th>time_driven_6am - 12pm</th>\n",
       "      <th>time_driven_6pm - 12am</th>\n",
       "      <th>trm_len_6</th>\n",
       "      <th>trm_len_12</th>\n",
       "      <th>high_education_ind_0.0</th>\n",
       "      <th>high_education_ind_1.0</th>\n",
       "      <th>clm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.41</td>\n",
       "      <td>0.575309</td>\n",
       "      <td>201</td>\n",
       "      <td>56.0</td>\n",
       "      <td>644.589621</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.599629</td>\n",
       "      <td>140</td>\n",
       "      <td>88.0</td>\n",
       "      <td>645.528437</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.08</td>\n",
       "      <td>0.792841</td>\n",
       "      <td>157</td>\n",
       "      <td>98.0</td>\n",
       "      <td>646.673512</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.20</td>\n",
       "      <td>0.815227</td>\n",
       "      <td>144</td>\n",
       "      <td>75.0</td>\n",
       "      <td>652.849200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.69</td>\n",
       "      <td>0.292397</td>\n",
       "      <td>149</td>\n",
       "      <td>48.0</td>\n",
       "      <td>637.811984</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   veh_value  exposure  max_power  driving_history_score  credit_score  \\\n",
       "0       2.41  0.575309        201                   56.0    644.589621   \n",
       "1       0.95  0.599629        140                   88.0    645.528437   \n",
       "2       1.08  0.792841        157                   98.0    646.673512   \n",
       "3       1.20  0.815227        144                   75.0    652.849200   \n",
       "4       1.69  0.292397        149                   48.0    637.811984   \n",
       "\n",
       "   numclaims  veh_body_BUS  veh_body_CONVT  veh_body_COUPE  veh_body_HBACK  \\\n",
       "0          2             0               0               0               0   \n",
       "1          2             0               0               0               1   \n",
       "2          2             0               0               0               0   \n",
       "3          0             0               0               0               1   \n",
       "4          1             0               0               0               1   \n",
       "\n",
       "   ...  time_of_week_driven_weekend  time_driven_12am - 6 am  \\\n",
       "0  ...                            0                        0   \n",
       "1  ...                            1                        0   \n",
       "2  ...                            0                        0   \n",
       "3  ...                            0                        0   \n",
       "4  ...                            0                        0   \n",
       "\n",
       "   time_driven_12pm - 6pm  time_driven_6am - 12pm  time_driven_6pm - 12am  \\\n",
       "0                       1                       0                       0   \n",
       "1                       0                       1                       0   \n",
       "2                       0                       0                       1   \n",
       "3                       0                       1                       0   \n",
       "4                       1                       0                       0   \n",
       "\n",
       "   trm_len_6  trm_len_12  high_education_ind_0.0  high_education_ind_1.0  clm  \n",
       "0          0           1                       1                       0    1  \n",
       "1          0           1                       1                       0    1  \n",
       "2          0           1                       1                       0    1  \n",
       "3          0           1                       1                       0    0  \n",
       "4          0           1                       1                       0    1  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_claims_balanced_df = pd.concat([majority_class, oversampled_minority_1, oversampled_minority_2, oversampled_minority_3])\n",
    "num_claims_balanced_df = num_claims_balanced_df.sample(frac=1)\n",
    "num_claims_balanced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "num_claims_balanced_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84308, 64)\n",
      "(84308,)\n"
     ]
    }
   ],
   "source": [
    "num_claims_x, num_claims_y = num_claims_balanced_df.drop(columns=['numclaims']), num_claims_balanced_df['numclaims']\n",
    "\n",
    "print(num_claims_x.shape)\n",
    "print(num_claims_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56205, 64) (28103, 64) (56205,) (28103,)\n"
     ]
    }
   ],
   "source": [
    "num_claims_x_train, num_claims_x_test, num_claims_y_train, num_claims_y_test = train_test_split(num_claims_x, num_claims_y, test_size=1 / 3)\n",
    "\n",
    "print(num_claims_x_train.shape, num_claims_x_test.shape, num_claims_y_train.shape, num_claims_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_claims_scaler = StandardScaler()\n",
    "num_claims_x_train = num_claims_scaler.fit_transform(num_claims_x_train)\n",
    "num_claims_x_test = num_claims_scaler.transform(num_claims_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/num_claims_scaler.pickle', 'wb') as f:\n",
    "    pickle.dump(num_claims_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_num_claims_x, original_num_claims_y = num_claims_df.drop(columns=['numclaims']), num_claims_df['numclaims']\n",
    "original_num_claims_x = num_claims_scaler.transform(original_num_claims_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num runs: 486\n",
      "Best results so far: train = 0.8988879992883195, test = 0.8964167526598584\n",
      "Remaining runs: 485\n",
      "Best results so far: train = 0.9666755626723601, test = 0.9638116927018467\n",
      "Remaining runs: 484\n",
      "Remaining runs: 483\n",
      "Best results so far: train = 0.9990214393737212, test = 0.9989680816994627\n",
      "Remaining runs: 482\n",
      "Remaining runs: 481\n",
      "Remaining runs: 480\n",
      "Remaining runs: 479\n",
      "Remaining runs: 478\n",
      "Remaining runs: 477\n",
      "Remaining runs: 476\n",
      "Remaining runs: 475\n",
      "Remaining runs: 474\n",
      "Remaining runs: 473\n",
      "Remaining runs: 472\n",
      "Remaining runs: 471\n",
      "Remaining runs: 470\n",
      "Remaining runs: 469\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mymac/travelers/main.ipynb Cell 66\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y145sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m n_estimators, min_samples_leaf, max_depth, min_samples_split, criterion \u001b[39min\u001b[39;00m combos_to_try:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y145sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     curr_model \u001b[39m=\u001b[39m RandomForestClassifier(n_estimators\u001b[39m=\u001b[39mn_estimators, min_samples_leaf\u001b[39m=\u001b[39mmin_samples_leaf, max_depth\u001b[39m=\u001b[39mmax_depth, min_samples_split\u001b[39m=\u001b[39mmin_samples_split, criterion\u001b[39m=\u001b[39mcriterion)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y145sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     curr_model\u001b[39m.\u001b[39;49mfit(num_claims_x_train, num_claims_y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y145sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     num_claims_y_train_pred \u001b[39m=\u001b[39m curr_model\u001b[39m.\u001b[39mpredict(num_claims_x_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y145sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     num_claims_y_test_pred \u001b[39m=\u001b[39m curr_model\u001b[39m.\u001b[39mpredict(num_claims_x_test)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    460\u001b[0m )(\n\u001b[1;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    462\u001b[0m         t,\n\u001b[1;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    464\u001b[0m         X,\n\u001b[1;32m    465\u001b[0m         y,\n\u001b[1;32m    466\u001b[0m         sample_weight,\n\u001b[1;32m    467\u001b[0m         i,\n\u001b[1;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    472\u001b[0m     )\n\u001b[1;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    474\u001b[0m )\n\u001b[1;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    960\u001b[0m         X,\n\u001b[1;32m    961\u001b[0m         y,\n\u001b[1;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_combos = []\n",
    "\n",
    "for n_estimators in [5, 10, 15, 20, 25, 50, 100, 200, 300]:\n",
    "    for min_samples_leaf in [5, 10, 15, 20, 25, 50]:\n",
    "        for max_depth in [3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "            for min_samples_split in [2, 3, 4, 5, 10, 15]:\n",
    "                for criterion in ['gini', 'entropy', 'log_loss']:\n",
    "                    all_combos.append((n_estimators, min_samples_leaf, max_depth, min_samples_split, criterion))\n",
    "\n",
    "percentage_to_try = 0.05\n",
    "n_runs = int(percentage_to_try * len(all_combos))\n",
    "combos_to_try = random.sample(all_combos, n_runs)\n",
    "print(f'Num runs: {n_runs}')\n",
    "\n",
    "best_test_accuracy, num_claims_rf_model = -np.inf, None\n",
    "\n",
    "for n_estimators, min_samples_leaf, max_depth, min_samples_split, criterion in combos_to_try:\n",
    "    curr_model = RandomForestClassifier(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion)\n",
    "    curr_model.fit(num_claims_x_train, num_claims_y_train)\n",
    "\n",
    "    num_claims_y_train_pred = curr_model.predict(num_claims_x_train)\n",
    "    num_claims_y_test_pred = curr_model.predict(num_claims_x_test)\n",
    "\n",
    "    train_accuracy = accuracy_score(num_claims_y_train, num_claims_y_train_pred)\n",
    "    test_accuracy = accuracy_score(num_claims_y_test, num_claims_y_test_pred)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy, num_claims_rf_model = test_accuracy, curr_model\n",
    "        print(f'Best results so far: train = {train_accuracy}, test = {test_accuracy}')\n",
    "\n",
    "        # Early stopping\n",
    "        if best_test_accuracy >= 0.998:\n",
    "            break\n",
    "\n",
    "    n_runs -= 1\n",
    "    print(f'Remaining runs: {n_runs}')\n",
    "\n",
    "print(best_test_accuracy)\n",
    "print(num_claims_rf_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9962863079711747\n",
      "(array([0, 1, 2, 3]), array([20993,  1523,    94,     9]))\n",
      "0    21077\n",
      "1     1439\n",
      "2       94\n",
      "3        9\n",
      "Name: numclaims, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "original_num_claims_rf_pred = num_claims_rf_model.predict(original_num_claims_x)\n",
    "\n",
    "print(accuracy_score(original_num_claims_y, original_num_claims_rf_pred))\n",
    "print(np.unique(original_num_claims_rf_pred, return_counts=True))\n",
    "print(original_num_claims_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/num_claims_rf.pickle', 'wb') as f:\n",
    "    pickle.dump(num_claims_rf_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num runs: 97\n",
      "Best results so far: train = 0.9990570233964949, test = 0.9988969149201153\n",
      "Remaining runs: 96\n",
      "Best results so far: train = 1.0, test = 0.9989680816994627\n",
      "Remaining runs: 95\n",
      "Remaining runs: 94\n",
      "Remaining runs: 93\n",
      "Remaining runs: 92\n",
      "Remaining runs: 91\n",
      "Remaining runs: 90\n",
      "Remaining runs: 89\n",
      "Remaining runs: 88\n",
      "Remaining runs: 87\n",
      "Remaining runs: 86\n",
      "Remaining runs: 85\n",
      "Remaining runs: 84\n",
      "Remaining runs: 83\n",
      "Remaining runs: 82\n",
      "Remaining runs: 81\n",
      "Remaining runs: 80\n",
      "Remaining runs: 79\n",
      "Remaining runs: 78\n",
      "Remaining runs: 77\n",
      "Remaining runs: 76\n",
      "Remaining runs: 75\n",
      "Remaining runs: 74\n",
      "Remaining runs: 73\n",
      "Remaining runs: 72\n",
      "Remaining runs: 71\n",
      "Remaining runs: 70\n",
      "Remaining runs: 69\n",
      "Remaining runs: 68\n",
      "Remaining runs: 67\n",
      "Remaining runs: 66\n",
      "Remaining runs: 65\n",
      "Remaining runs: 64\n",
      "Best results so far: train = 1.0, test = 0.9990036650891364\n",
      "0.9990036650891364\n",
      "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.2, 'loss': 'log_loss', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 15, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "all_combos = []\n",
    "\n",
    "for n_estimators in [5, 10, 15, 20, 25, 50, 100, 200, 300]:\n",
    "    for min_samples_leaf in [5, 10, 15, 20, 25, 50]:\n",
    "        for max_depth in [3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "            for min_samples_split in [2, 3, 4, 5, 10, 15]:\n",
    "                for learning_rate in [0.01, 0.1, 0.2]:\n",
    "                    all_combos.append((n_estimators, min_samples_leaf, max_depth, min_samples_split, learning_rate))\n",
    "\n",
    "percentage_to_try = 0.01\n",
    "n_runs = int(percentage_to_try * len(all_combos))\n",
    "combos_to_try = random.sample(all_combos, n_runs)\n",
    "print(f'Num runs: {n_runs}')\n",
    "\n",
    "best_test_accuracy, num_claims_gb_model = -np.inf, None\n",
    "\n",
    "for n_estimators, min_samples_leaf, max_depth, min_samples_split, learning_rate in combos_to_try:\n",
    "    curr_model = GradientBoostingClassifier(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_depth=max_depth, min_samples_split=min_samples_split, learning_rate=learning_rate)\n",
    "    curr_model.fit(num_claims_x_train, num_claims_y_train)\n",
    "\n",
    "    num_claims_y_train_pred = curr_model.predict(num_claims_x_train)\n",
    "    num_claims_y_test_pred = curr_model.predict(num_claims_x_test)\n",
    "\n",
    "    train_accuracy = accuracy_score(num_claims_y_train, num_claims_y_train_pred)\n",
    "    test_accuracy = accuracy_score(num_claims_y_test, num_claims_y_test_pred)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy, num_claims_gb_model = test_accuracy, curr_model\n",
    "        print(f'Best results so far: train = {train_accuracy}, test = {test_accuracy}')\n",
    "\n",
    "        # Early stopping\n",
    "        if best_test_accuracy >= 0.999:\n",
    "            break\n",
    "\n",
    "    n_runs -= 1\n",
    "    print(f'Remaining runs: {n_runs}')\n",
    "\n",
    "print(best_test_accuracy)\n",
    "print(num_claims_gb_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9987621026570582\n",
      "(array([0, 1, 2, 3]), array([21049,  1467,    94,     9]))\n",
      "0    21077\n",
      "1     1439\n",
      "2       94\n",
      "3        9\n",
      "Name: numclaims, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "original_num_claims_gb_pred = num_claims_gb_model.predict(original_num_claims_x)\n",
    "\n",
    "print(accuracy_score(original_num_claims_y, original_num_claims_gb_pred))\n",
    "print(np.unique(original_num_claims_gb_pred, return_counts=True))\n",
    "print(original_num_claims_y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/num_claims_gb.pickle', 'wb') as f:\n",
    "    pickle.dump(num_claims_gb_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22619,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The gradient boosting model seems to be slightly better for num claim predictions\n",
    "num_claims_model, num_claims_pred = num_claims_gb_model, original_num_claims_gb_pred\n",
    "num_claims_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### claimcst0 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_df = df_dummies.drop(columns=['clm', 'numclaims'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn on the impure claim and num claim predictions\n",
    "assert len(clm_pred) == len(num_claims_pred) == len(cost_df)\n",
    "\n",
    "cost_df['clm'] = clm_pred\n",
    "cost_df['numclaims'] = num_claims_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21077, 64)\n",
      "(1542, 64)\n"
     ]
    }
   ],
   "source": [
    "majority_class = cost_df[cost_df['claimcst0'] == 0]\n",
    "minority_class = cost_df[cost_df['claimcst0'] != 0]\n",
    "\n",
    "print(majority_class.shape)\n",
    "print(minority_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21077, 64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_minority = resample(minority_class, replace=True, n_samples=len(majority_class))\n",
    "\n",
    "oversampled_minority.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21077, 64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_balanced_df = pd.concat([majority_class, oversampled_minority])\n",
    "cost_balanced_df = cost_balanced_df.sample(frac=1)\n",
    "cost_balanced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "cost_balanced_df[cost_balanced_df['claimcst0'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(cost_balanced_df['claimcst0'], bins=50)\n",
    "plt.hist(df['claimcst0'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42154, 63)\n",
      "(42154,)\n"
     ]
    }
   ],
   "source": [
    "cost_x, cost_y = cost_balanced_df.drop(columns=['claimcst0']), cost_balanced_df['claimcst0']\n",
    "\n",
    "print(cost_x.shape)\n",
    "print(cost_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28102, 63) (14052, 63) (28102,) (14052,)\n"
     ]
    }
   ],
   "source": [
    "cost_x_train, cost_x_test, cost_y_train, cost_y_test = train_test_split(cost_x, cost_y, test_size=1 / 3)\n",
    "\n",
    "print(cost_x_train.shape, cost_x_test.shape, cost_y_train.shape, cost_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_scaler = StandardScaler()\n",
    "cost_x_train = cost_scaler.fit_transform(cost_x_train)\n",
    "cost_x_test = cost_scaler.transform(cost_x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/cost_scaler.pickle', 'wb') as f:\n",
    "    pickle.dump(cost_scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_cost_x, original_cost_y = cost_df.drop(columns=['claimcst0']), cost_df['claimcst0']\n",
    "original_cost_x = cost_scaler.transform(original_cost_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 20:47:19.697408: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 [==============================] - ETA: 0s - loss: 10726703.0000 - mean_squared_error: 10726703.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 20:47:30.844178: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 10016076.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 14s 14ms/step - loss: 10726703.0000 - mean_squared_error: 10726703.0000 - val_loss: 10016076.0000 - val_mean_squared_error: 10016076.0000\n",
      "Epoch 2/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 10138661.0000 - mean_squared_error: 10138661.0000\n",
      "Epoch 2: val_loss improved from 10016076.00000 to 9883592.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 10114570.0000 - mean_squared_error: 10114570.0000 - val_loss: 9883592.0000 - val_mean_squared_error: 9883592.0000\n",
      "Epoch 3/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 9992807.0000 - mean_squared_error: 9992807.0000  \n",
      "Epoch 3: val_loss improved from 9883592.00000 to 9798135.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 9978156.0000 - mean_squared_error: 9978156.0000 - val_loss: 9798135.0000 - val_mean_squared_error: 9798135.0000\n",
      "Epoch 4/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 9911933.0000 - mean_squared_error: 9911933.0000\n",
      "Epoch 4: val_loss improved from 9798135.00000 to 9700555.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 9864385.0000 - mean_squared_error: 9864385.0000 - val_loss: 9700555.0000 - val_mean_squared_error: 9700555.0000\n",
      "Epoch 5/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 9754455.0000 - mean_squared_error: 9754455.0000\n",
      "Epoch 5: val_loss improved from 9700555.00000 to 9590515.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 9754455.0000 - mean_squared_error: 9754455.0000 - val_loss: 9590515.0000 - val_mean_squared_error: 9590515.0000\n",
      "Epoch 6/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 9609697.0000 - mean_squared_error: 9609697.0000\n",
      "Epoch 6: val_loss improved from 9590515.00000 to 9504827.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 9642570.0000 - mean_squared_error: 9642570.0000 - val_loss: 9504827.0000 - val_mean_squared_error: 9504827.0000\n",
      "Epoch 7/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 9538571.0000 - mean_squared_error: 9538571.0000\n",
      "Epoch 7: val_loss improved from 9504827.00000 to 9369064.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 9513764.0000 - mean_squared_error: 9513764.0000 - val_loss: 9369064.0000 - val_mean_squared_error: 9369064.0000\n",
      "Epoch 8/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 9331746.0000 - mean_squared_error: 9331746.0000\n",
      "Epoch 8: val_loss improved from 9369064.00000 to 9183506.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 9331442.0000 - mean_squared_error: 9331442.0000 - val_loss: 9183506.0000 - val_mean_squared_error: 9183506.0000\n",
      "Epoch 9/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 9038551.0000 - mean_squared_error: 9038551.0000\n",
      "Epoch 9: val_loss improved from 9183506.00000 to 8933323.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 9059842.0000 - mean_squared_error: 9059842.0000 - val_loss: 8933323.0000 - val_mean_squared_error: 8933323.0000\n",
      "Epoch 10/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 8723209.0000 - mean_squared_error: 8723209.0000\n",
      "Epoch 10: val_loss improved from 8933323.00000 to 8615758.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 8s 9ms/step - loss: 8725656.0000 - mean_squared_error: 8725656.0000 - val_loss: 8615758.0000 - val_mean_squared_error: 8615758.0000\n",
      "Epoch 11/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 8362351.0000 - mean_squared_error: 8362351.0000\n",
      "Epoch 11: val_loss improved from 8615758.00000 to 8262035.50000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 8357547.5000 - mean_squared_error: 8357547.5000 - val_loss: 8262035.5000 - val_mean_squared_error: 8262035.5000\n",
      "Epoch 12/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 7954120.0000 - mean_squared_error: 7954120.0000\n",
      "Epoch 12: val_loss improved from 8262035.50000 to 7887906.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 7954120.0000 - mean_squared_error: 7954120.0000 - val_loss: 7887906.0000 - val_mean_squared_error: 7887906.0000\n",
      "Epoch 13/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 7559220.5000 - mean_squared_error: 7559220.5000\n",
      "Epoch 13: val_loss improved from 7887906.00000 to 7484829.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 7537164.0000 - mean_squared_error: 7537164.0000 - val_loss: 7484829.0000 - val_mean_squared_error: 7484829.0000\n",
      "Epoch 14/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 7126592.0000 - mean_squared_error: 7126592.0000\n",
      "Epoch 14: val_loss improved from 7484829.00000 to 7167829.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 7127253.0000 - mean_squared_error: 7127253.0000 - val_loss: 7167829.0000 - val_mean_squared_error: 7167829.0000\n",
      "Epoch 15/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 6652820.5000 - mean_squared_error: 6652820.5000\n",
      "Epoch 15: val_loss improved from 7167829.00000 to 6797414.50000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 6722173.0000 - mean_squared_error: 6722173.0000 - val_loss: 6797414.5000 - val_mean_squared_error: 6797414.5000\n",
      "Epoch 16/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 6346320.0000 - mean_squared_error: 6346320.0000\n",
      "Epoch 16: val_loss improved from 6797414.50000 to 6351082.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 6340569.5000 - mean_squared_error: 6340569.5000 - val_loss: 6351082.0000 - val_mean_squared_error: 6351082.0000\n",
      "Epoch 17/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 5939697.0000 - mean_squared_error: 5939697.0000\n",
      "Epoch 17: val_loss improved from 6351082.00000 to 6021601.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 5920510.0000 - mean_squared_error: 5920510.0000 - val_loss: 6021601.0000 - val_mean_squared_error: 6021601.0000\n",
      "Epoch 18/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 5521875.5000 - mean_squared_error: 5521875.5000\n",
      "Epoch 18: val_loss improved from 6021601.00000 to 5714600.50000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 5519016.5000 - mean_squared_error: 5519016.5000 - val_loss: 5714600.5000 - val_mean_squared_error: 5714600.5000\n",
      "Epoch 19/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 5151039.5000 - mean_squared_error: 5151039.5000\n",
      "Epoch 19: val_loss improved from 5714600.50000 to 5276752.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 5150989.0000 - mean_squared_error: 5150989.0000 - val_loss: 5276752.0000 - val_mean_squared_error: 5276752.0000\n",
      "Epoch 20/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 4756005.5000 - mean_squared_error: 4756005.5000\n",
      "Epoch 20: val_loss improved from 5276752.00000 to 4909656.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 4758543.0000 - mean_squared_error: 4758543.0000 - val_loss: 4909656.0000 - val_mean_squared_error: 4909656.0000\n",
      "Epoch 21/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 4410478.0000 - mean_squared_error: 4410478.0000\n",
      "Epoch 21: val_loss improved from 4909656.00000 to 4678270.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 4404688.0000 - mean_squared_error: 4404688.0000 - val_loss: 4678270.0000 - val_mean_squared_error: 4678270.0000\n",
      "Epoch 22/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 4073833.2500 - mean_squared_error: 4073833.2500\n",
      "Epoch 22: val_loss improved from 4678270.00000 to 4355574.50000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 4075888.0000 - mean_squared_error: 4075888.0000 - val_loss: 4355574.5000 - val_mean_squared_error: 4355574.5000\n",
      "Epoch 23/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 3767074.7500 - mean_squared_error: 3767074.7500\n",
      "Epoch 23: val_loss improved from 4355574.50000 to 4084369.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 3772861.7500 - mean_squared_error: 3772861.7500 - val_loss: 4084369.0000 - val_mean_squared_error: 4084369.0000\n",
      "Epoch 24/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 3467282.5000 - mean_squared_error: 3467282.5000\n",
      "Epoch 24: val_loss improved from 4084369.00000 to 3876357.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 3461190.7500 - mean_squared_error: 3461190.7500 - val_loss: 3876357.0000 - val_mean_squared_error: 3876357.0000\n",
      "Epoch 25/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 3212731.2500 - mean_squared_error: 3212731.2500\n",
      "Epoch 25: val_loss improved from 3876357.00000 to 3511081.75000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 3211997.7500 - mean_squared_error: 3211997.7500 - val_loss: 3511081.7500 - val_mean_squared_error: 3511081.7500\n",
      "Epoch 26/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 2967953.7500 - mean_squared_error: 2967953.7500\n",
      "Epoch 26: val_loss improved from 3511081.75000 to 3337875.75000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 2967953.7500 - mean_squared_error: 2967953.7500 - val_loss: 3337875.7500 - val_mean_squared_error: 3337875.7500\n",
      "Epoch 27/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 2706690.7500 - mean_squared_error: 2706690.7500\n",
      "Epoch 27: val_loss improved from 3337875.75000 to 3159458.25000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 2706529.7500 - mean_squared_error: 2706529.7500 - val_loss: 3159458.2500 - val_mean_squared_error: 3159458.2500\n",
      "Epoch 28/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 2533713.0000 - mean_squared_error: 2533713.0000\n",
      "Epoch 28: val_loss improved from 3159458.25000 to 2944877.75000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 2528774.0000 - mean_squared_error: 2528774.0000 - val_loss: 2944877.7500 - val_mean_squared_error: 2944877.7500\n",
      "Epoch 29/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 2283759.2500 - mean_squared_error: 2283759.2500\n",
      "Epoch 29: val_loss improved from 2944877.75000 to 2801553.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 2282022.0000 - mean_squared_error: 2282022.0000 - val_loss: 2801553.0000 - val_mean_squared_error: 2801553.0000\n",
      "Epoch 30/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 2170905.7500 - mean_squared_error: 2170905.7500\n",
      "Epoch 30: val_loss improved from 2801553.00000 to 2578915.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 2170905.7500 - mean_squared_error: 2170905.7500 - val_loss: 2578915.0000 - val_mean_squared_error: 2578915.0000\n",
      "Epoch 31/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 1972466.2500 - mean_squared_error: 1972466.2500\n",
      "Epoch 31: val_loss improved from 2578915.00000 to 2544849.50000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1969569.6250 - mean_squared_error: 1969569.6250 - val_loss: 2544849.5000 - val_mean_squared_error: 2544849.5000\n",
      "Epoch 32/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 1808961.3750 - mean_squared_error: 1808961.3750\n",
      "Epoch 32: val_loss improved from 2544849.50000 to 2375460.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1808961.3750 - mean_squared_error: 1808961.3750 - val_loss: 2375460.0000 - val_mean_squared_error: 2375460.0000\n",
      "Epoch 33/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 1680678.8750 - mean_squared_error: 1680678.8750\n",
      "Epoch 33: val_loss improved from 2375460.00000 to 2194961.25000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1682094.6250 - mean_squared_error: 1682094.6250 - val_loss: 2194961.2500 - val_mean_squared_error: 2194961.2500\n",
      "Epoch 34/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 1555503.5000 - mean_squared_error: 1555503.5000\n",
      "Epoch 34: val_loss improved from 2194961.25000 to 2150887.50000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1554302.7500 - mean_squared_error: 1554302.7500 - val_loss: 2150887.5000 - val_mean_squared_error: 2150887.5000\n",
      "Epoch 35/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 1431564.5000 - mean_squared_error: 1431564.5000\n",
      "Epoch 35: val_loss improved from 2150887.50000 to 2063931.62500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1430910.5000 - mean_squared_error: 1430910.5000 - val_loss: 2063931.6250 - val_mean_squared_error: 2063931.6250\n",
      "Epoch 36/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 1344265.1250 - mean_squared_error: 1344265.1250\n",
      "Epoch 36: val_loss improved from 2063931.62500 to 1919634.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1344094.8750 - mean_squared_error: 1344094.8750 - val_loss: 1919634.0000 - val_mean_squared_error: 1919634.0000\n",
      "Epoch 37/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 1241074.7500 - mean_squared_error: 1241074.7500\n",
      "Epoch 37: val_loss improved from 1919634.00000 to 1787765.75000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1238617.6250 - mean_squared_error: 1238617.6250 - val_loss: 1787765.7500 - val_mean_squared_error: 1787765.7500\n",
      "Epoch 38/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 1156780.0000 - mean_squared_error: 1156780.0000\n",
      "Epoch 38: val_loss improved from 1787765.75000 to 1746859.12500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1153965.7500 - mean_squared_error: 1153965.7500 - val_loss: 1746859.1250 - val_mean_squared_error: 1746859.1250\n",
      "Epoch 39/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 1073849.7500 - mean_squared_error: 1073849.7500\n",
      "Epoch 39: val_loss improved from 1746859.12500 to 1708774.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1075082.8750 - mean_squared_error: 1075082.8750 - val_loss: 1708774.0000 - val_mean_squared_error: 1708774.0000\n",
      "Epoch 40/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 1021133.0625 - mean_squared_error: 1021133.0625\n",
      "Epoch 40: val_loss did not improve from 1708774.00000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 1020667.6875 - mean_squared_error: 1020667.6875 - val_loss: 1712271.0000 - val_mean_squared_error: 1712271.0000\n",
      "Epoch 41/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 955802.5000 - mean_squared_error: 955802.5000\n",
      "Epoch 41: val_loss improved from 1708774.00000 to 1689599.12500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 956442.0625 - mean_squared_error: 956442.0625 - val_loss: 1689599.1250 - val_mean_squared_error: 1689599.1250\n",
      "Epoch 42/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 902868.5000 - mean_squared_error: 902868.5000\n",
      "Epoch 42: val_loss improved from 1689599.12500 to 1625322.12500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 902699.9375 - mean_squared_error: 902699.9375 - val_loss: 1625322.1250 - val_mean_squared_error: 1625322.1250\n",
      "Epoch 43/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 862494.2500 - mean_squared_error: 862494.2500\n",
      "Epoch 43: val_loss improved from 1625322.12500 to 1568903.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 862339.7500 - mean_squared_error: 862339.7500 - val_loss: 1568903.0000 - val_mean_squared_error: 1568903.0000\n",
      "Epoch 44/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 819029.9375 - mean_squared_error: 819029.9375\n",
      "Epoch 44: val_loss improved from 1568903.00000 to 1564361.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 819553.0000 - mean_squared_error: 819553.0000 - val_loss: 1564361.0000 - val_mean_squared_error: 1564361.0000\n",
      "Epoch 45/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 786635.0625 - mean_squared_error: 786635.0625\n",
      "Epoch 45: val_loss improved from 1564361.00000 to 1482330.62500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 785798.6250 - mean_squared_error: 785798.6250 - val_loss: 1482330.6250 - val_mean_squared_error: 1482330.6250\n",
      "Epoch 46/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 749956.5000 - mean_squared_error: 749956.5000\n",
      "Epoch 46: val_loss improved from 1482330.62500 to 1454673.62500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 750456.8125 - mean_squared_error: 750456.8125 - val_loss: 1454673.6250 - val_mean_squared_error: 1454673.6250\n",
      "Epoch 47/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 729704.5000 - mean_squared_error: 729704.5000\n",
      "Epoch 47: val_loss did not improve from 1454673.62500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 729045.1250 - mean_squared_error: 729045.1250 - val_loss: 1557751.1250 - val_mean_squared_error: 1557751.1250\n",
      "Epoch 48/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 699795.5625 - mean_squared_error: 699795.5625\n",
      "Epoch 48: val_loss improved from 1454673.62500 to 1453166.50000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 699342.8125 - mean_squared_error: 699342.8125 - val_loss: 1453166.5000 - val_mean_squared_error: 1453166.5000\n",
      "Epoch 49/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 673098.0625 - mean_squared_error: 673098.0625\n",
      "Epoch 49: val_loss improved from 1453166.50000 to 1395576.87500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 9s 10ms/step - loss: 672600.0625 - mean_squared_error: 672600.0625 - val_loss: 1395576.8750 - val_mean_squared_error: 1395576.8750\n",
      "Epoch 50/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 661290.8125 - mean_squared_error: 661290.8125\n",
      "Epoch 50: val_loss did not improve from 1395576.87500\n",
      "879/879 [==============================] - 10s 11ms/step - loss: 661153.0000 - mean_squared_error: 661153.0000 - val_loss: 1467062.8750 - val_mean_squared_error: 1467062.8750\n",
      "Epoch 51/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 642945.8750 - mean_squared_error: 642945.8750\n",
      "Epoch 51: val_loss did not improve from 1395576.87500\n",
      "879/879 [==============================] - 10s 11ms/step - loss: 642814.0000 - mean_squared_error: 642814.0000 - val_loss: 1401322.2500 - val_mean_squared_error: 1401322.2500\n",
      "Epoch 52/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 628497.8750 - mean_squared_error: 628497.8750\n",
      "Epoch 52: val_loss did not improve from 1395576.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 628581.1875 - mean_squared_error: 628581.1875 - val_loss: 1412055.2500 - val_mean_squared_error: 1412055.2500\n",
      "Epoch 53/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 621979.4375 - mean_squared_error: 621979.4375\n",
      "Epoch 53: val_loss improved from 1395576.87500 to 1343978.87500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 621886.3125 - mean_squared_error: 621886.3125 - val_loss: 1343978.8750 - val_mean_squared_error: 1343978.8750\n",
      "Epoch 54/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 605352.4375 - mean_squared_error: 605352.4375\n",
      "Epoch 54: val_loss did not improve from 1343978.87500\n",
      "879/879 [==============================] - 10s 11ms/step - loss: 605352.4375 - mean_squared_error: 605352.4375 - val_loss: 1351146.7500 - val_mean_squared_error: 1351146.7500\n",
      "Epoch 55/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 610063.0000 - mean_squared_error: 610063.0000\n",
      "Epoch 55: val_loss did not improve from 1343978.87500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 610375.0000 - mean_squared_error: 610375.0000 - val_loss: 1603427.6250 - val_mean_squared_error: 1603427.6250\n",
      "Epoch 56/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 603641.5625 - mean_squared_error: 603641.5625\n",
      "Epoch 56: val_loss did not improve from 1343978.87500\n",
      "879/879 [==============================] - 9s 10ms/step - loss: 603106.6250 - mean_squared_error: 603106.6250 - val_loss: 1415011.5000 - val_mean_squared_error: 1415011.5000\n",
      "Epoch 57/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 588656.5625 - mean_squared_error: 588656.5625\n",
      "Epoch 57: val_loss did not improve from 1343978.87500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 588252.7500 - mean_squared_error: 588252.7500 - val_loss: 1378223.8750 - val_mean_squared_error: 1378223.8750\n",
      "Epoch 58/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 579893.5000 - mean_squared_error: 579893.5000\n",
      "Epoch 58: val_loss did not improve from 1343978.87500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 578620.3750 - mean_squared_error: 578620.3750 - val_loss: 1471138.2500 - val_mean_squared_error: 1471138.2500\n",
      "Epoch 59/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 574447.0625 - mean_squared_error: 574447.0625\n",
      "Epoch 59: val_loss did not improve from 1343978.87500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 575904.7500 - mean_squared_error: 575904.7500 - val_loss: 1360235.7500 - val_mean_squared_error: 1360235.7500\n",
      "Epoch 60/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 577813.9375 - mean_squared_error: 577813.9375\n",
      "Epoch 60: val_loss improved from 1343978.87500 to 1291591.00000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 577813.9375 - mean_squared_error: 577813.9375 - val_loss: 1291591.0000 - val_mean_squared_error: 1291591.0000\n",
      "Epoch 61/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 575920.6250 - mean_squared_error: 575920.6250\n",
      "Epoch 61: val_loss did not improve from 1291591.00000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 575869.8125 - mean_squared_error: 575869.8125 - val_loss: 1350500.0000 - val_mean_squared_error: 1350500.0000\n",
      "Epoch 62/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 561249.8750 - mean_squared_error: 561249.8750\n",
      "Epoch 62: val_loss did not improve from 1291591.00000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 560985.7500 - mean_squared_error: 560985.7500 - val_loss: 1369915.5000 - val_mean_squared_error: 1369915.5000\n",
      "Epoch 63/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 572656.2500 - mean_squared_error: 572656.2500\n",
      "Epoch 63: val_loss did not improve from 1291591.00000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 572840.1875 - mean_squared_error: 572840.1875 - val_loss: 1356227.3750 - val_mean_squared_error: 1356227.3750\n",
      "Epoch 64/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 553203.5000 - mean_squared_error: 553203.5000\n",
      "Epoch 64: val_loss improved from 1291591.00000 to 1275211.87500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 552300.1250 - mean_squared_error: 552300.1250 - val_loss: 1275211.8750 - val_mean_squared_error: 1275211.8750\n",
      "Epoch 65/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 579553.8125 - mean_squared_error: 579553.8125\n",
      "Epoch 65: val_loss improved from 1275211.87500 to 1255386.87500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 578099.0625 - mean_squared_error: 578099.0625 - val_loss: 1255386.8750 - val_mean_squared_error: 1255386.8750\n",
      "Epoch 66/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 555431.7500 - mean_squared_error: 555431.7500\n",
      "Epoch 66: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 555315.1875 - mean_squared_error: 555315.1875 - val_loss: 1300893.7500 - val_mean_squared_error: 1300893.7500\n",
      "Epoch 67/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 549427.0000 - mean_squared_error: 549427.0000\n",
      "Epoch 67: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 548782.1250 - mean_squared_error: 548782.1250 - val_loss: 1351926.1250 - val_mean_squared_error: 1351926.1250\n",
      "Epoch 68/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 565255.5000 - mean_squared_error: 565255.5000\n",
      "Epoch 68: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 564439.0000 - mean_squared_error: 564439.0000 - val_loss: 1286259.2500 - val_mean_squared_error: 1286259.2500\n",
      "Epoch 69/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 557429.3750 - mean_squared_error: 557429.3750\n",
      "Epoch 69: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 557370.6250 - mean_squared_error: 557370.6250 - val_loss: 1423203.8750 - val_mean_squared_error: 1423203.8750\n",
      "Epoch 70/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 546098.5000 - mean_squared_error: 546098.5000\n",
      "Epoch 70: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 545765.0625 - mean_squared_error: 545765.0625 - val_loss: 1277538.8750 - val_mean_squared_error: 1277538.8750\n",
      "Epoch 71/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 560567.0000 - mean_squared_error: 560567.0000\n",
      "Epoch 71: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 561975.4375 - mean_squared_error: 561975.4375 - val_loss: 1327309.3750 - val_mean_squared_error: 1327309.3750\n",
      "Epoch 72/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 562834.0000 - mean_squared_error: 562834.0000\n",
      "Epoch 72: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 562834.0000 - mean_squared_error: 562834.0000 - val_loss: 1301518.1250 - val_mean_squared_error: 1301518.1250\n",
      "Epoch 73/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 523363.5625 - mean_squared_error: 523363.5625\n",
      "Epoch 73: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 522869.6875 - mean_squared_error: 522869.6875 - val_loss: 1327088.8750 - val_mean_squared_error: 1327088.8750\n",
      "Epoch 74/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 601467.1875 - mean_squared_error: 601467.1875\n",
      "Epoch 74: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 600821.3125 - mean_squared_error: 600821.3125 - val_loss: 1315599.8750 - val_mean_squared_error: 1315599.8750\n",
      "Epoch 75/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 528830.8125 - mean_squared_error: 528830.8125\n",
      "Epoch 75: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 539104.2500 - mean_squared_error: 539104.2500 - val_loss: 1447531.8750 - val_mean_squared_error: 1447531.8750\n",
      "Epoch 76/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 563110.3750 - mean_squared_error: 563110.3750\n",
      "Epoch 76: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 563307.8750 - mean_squared_error: 563307.8750 - val_loss: 1327807.1250 - val_mean_squared_error: 1327807.1250\n",
      "Epoch 77/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 533612.6875 - mean_squared_error: 533612.6875\n",
      "Epoch 77: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 533612.6875 - mean_squared_error: 533612.6875 - val_loss: 1278907.3750 - val_mean_squared_error: 1278907.3750\n",
      "Epoch 78/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 570959.9375 - mean_squared_error: 570959.9375\n",
      "Epoch 78: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 570436.6875 - mean_squared_error: 570436.6875 - val_loss: 1271378.2500 - val_mean_squared_error: 1271378.2500\n",
      "Epoch 79/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 525547.6875 - mean_squared_error: 525547.6875\n",
      "Epoch 79: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 524423.7500 - mean_squared_error: 524423.7500 - val_loss: 1288408.6250 - val_mean_squared_error: 1288408.6250\n",
      "Epoch 80/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 567458.0000 - mean_squared_error: 567458.0000\n",
      "Epoch 80: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 567458.0000 - mean_squared_error: 567458.0000 - val_loss: 1349990.5000 - val_mean_squared_error: 1349990.5000\n",
      "Epoch 81/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 546350.0625 - mean_squared_error: 546350.0625\n",
      "Epoch 81: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 545982.2500 - mean_squared_error: 545982.2500 - val_loss: 1260789.3750 - val_mean_squared_error: 1260789.3750\n",
      "Epoch 82/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 556047.6875 - mean_squared_error: 556047.6875\n",
      "Epoch 82: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 556047.6875 - mean_squared_error: 556047.6875 - val_loss: 1339175.3750 - val_mean_squared_error: 1339175.3750\n",
      "Epoch 83/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 550612.1250 - mean_squared_error: 550612.1250\n",
      "Epoch 83: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 550528.9375 - mean_squared_error: 550528.9375 - val_loss: 1288622.3750 - val_mean_squared_error: 1288622.3750\n",
      "Epoch 84/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 538335.7500 - mean_squared_error: 538335.7500\n",
      "Epoch 84: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 537651.7500 - mean_squared_error: 537651.7500 - val_loss: 1350575.1250 - val_mean_squared_error: 1350575.1250\n",
      "Epoch 85/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 544453.8125 - mean_squared_error: 544453.8125\n",
      "Epoch 85: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 550607.5625 - mean_squared_error: 550607.5625 - val_loss: 1293891.5000 - val_mean_squared_error: 1293891.5000\n",
      "Epoch 86/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 532299.5000 - mean_squared_error: 532299.5000\n",
      "Epoch 86: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 531828.7500 - mean_squared_error: 531828.7500 - val_loss: 1313156.8750 - val_mean_squared_error: 1313156.8750\n",
      "Epoch 87/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 545585.9375 - mean_squared_error: 545585.9375\n",
      "Epoch 87: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 547677.6250 - mean_squared_error: 547677.6250 - val_loss: 1404737.2500 - val_mean_squared_error: 1404737.2500\n",
      "Epoch 88/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 566056.7500 - mean_squared_error: 566056.7500\n",
      "Epoch 88: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 565400.0625 - mean_squared_error: 565400.0625 - val_loss: 1332194.3750 - val_mean_squared_error: 1332194.3750\n",
      "Epoch 89/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 516956.3125 - mean_squared_error: 516956.3125\n",
      "Epoch 89: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 517049.4062 - mean_squared_error: 517049.4062 - val_loss: 1280007.8750 - val_mean_squared_error: 1280007.8750\n",
      "Epoch 90/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 550644.0000 - mean_squared_error: 550644.0000\n",
      "Epoch 90: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 550644.0000 - mean_squared_error: 550644.0000 - val_loss: 1258864.7500 - val_mean_squared_error: 1258864.7500\n",
      "Epoch 91/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 544326.0000 - mean_squared_error: 544326.0000\n",
      "Epoch 91: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 543484.5625 - mean_squared_error: 543484.5625 - val_loss: 1350031.0000 - val_mean_squared_error: 1350031.0000\n",
      "Epoch 92/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 547239.0625 - mean_squared_error: 547239.0625\n",
      "Epoch 92: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 546414.4375 - mean_squared_error: 546414.4375 - val_loss: 1262945.2500 - val_mean_squared_error: 1262945.2500\n",
      "Epoch 93/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 551589.0000 - mean_squared_error: 551589.0000\n",
      "Epoch 93: val_loss did not improve from 1255386.87500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 551520.5000 - mean_squared_error: 551520.5000 - val_loss: 1285113.7500 - val_mean_squared_error: 1285113.7500\n",
      "Epoch 94/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 544484.2500 - mean_squared_error: 544484.2500\n",
      "Epoch 94: val_loss improved from 1255386.87500 to 1242385.25000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 544280.0000 - mean_squared_error: 544280.0000 - val_loss: 1242385.2500 - val_mean_squared_error: 1242385.2500\n",
      "Epoch 95/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 539042.8125 - mean_squared_error: 539042.8125\n",
      "Epoch 95: val_loss improved from 1242385.25000 to 1230483.37500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 539042.8125 - mean_squared_error: 539042.8125 - val_loss: 1230483.3750 - val_mean_squared_error: 1230483.3750\n",
      "Epoch 96/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 544187.3750 - mean_squared_error: 544187.3750\n",
      "Epoch 96: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 544187.3750 - mean_squared_error: 544187.3750 - val_loss: 1293700.1250 - val_mean_squared_error: 1293700.1250\n",
      "Epoch 97/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 538684.7500 - mean_squared_error: 538684.7500\n",
      "Epoch 97: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 538684.7500 - mean_squared_error: 538684.7500 - val_loss: 1318825.7500 - val_mean_squared_error: 1318825.7500\n",
      "Epoch 98/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 560427.9375 - mean_squared_error: 560427.9375\n",
      "Epoch 98: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 560542.0625 - mean_squared_error: 560542.0625 - val_loss: 1364962.0000 - val_mean_squared_error: 1364962.0000\n",
      "Epoch 99/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 526425.9375 - mean_squared_error: 526425.9375\n",
      "Epoch 99: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 526984.9375 - mean_squared_error: 526984.9375 - val_loss: 1265889.1250 - val_mean_squared_error: 1265889.1250\n",
      "Epoch 100/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 543710.4375 - mean_squared_error: 543710.4375\n",
      "Epoch 100: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 545666.3125 - mean_squared_error: 545666.3125 - val_loss: 1272851.8750 - val_mean_squared_error: 1272851.8750\n",
      "Epoch 101/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 526926.6875 - mean_squared_error: 526926.6875\n",
      "Epoch 101: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 526491.8125 - mean_squared_error: 526491.8125 - val_loss: 1263340.3750 - val_mean_squared_error: 1263340.3750\n",
      "Epoch 102/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 545832.6875 - mean_squared_error: 545832.6875\n",
      "Epoch 102: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 545832.6875 - mean_squared_error: 545832.6875 - val_loss: 1302029.5000 - val_mean_squared_error: 1302029.5000\n",
      "Epoch 103/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 532726.6875 - mean_squared_error: 532726.6875\n",
      "Epoch 103: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 533684.7500 - mean_squared_error: 533684.7500 - val_loss: 1317058.7500 - val_mean_squared_error: 1317058.7500\n",
      "Epoch 104/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 528066.0625 - mean_squared_error: 528066.0625\n",
      "Epoch 104: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 534869.2500 - mean_squared_error: 534869.2500 - val_loss: 1251905.6250 - val_mean_squared_error: 1251905.6250\n",
      "Epoch 105/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 552510.8125 - mean_squared_error: 552510.8125\n",
      "Epoch 105: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 552596.6875 - mean_squared_error: 552596.6875 - val_loss: 1256947.6250 - val_mean_squared_error: 1256947.6250\n",
      "Epoch 106/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 525762.5000 - mean_squared_error: 525762.5000\n",
      "Epoch 106: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 524967.8125 - mean_squared_error: 524967.8125 - val_loss: 1275456.2500 - val_mean_squared_error: 1275456.2500\n",
      "Epoch 107/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 542663.1250 - mean_squared_error: 542663.1250\n",
      "Epoch 107: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 542412.9375 - mean_squared_error: 542412.9375 - val_loss: 1303734.0000 - val_mean_squared_error: 1303734.0000\n",
      "Epoch 108/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 549310.0625 - mean_squared_error: 549310.0625\n",
      "Epoch 108: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 549180.0625 - mean_squared_error: 549180.0625 - val_loss: 1365520.5000 - val_mean_squared_error: 1365520.5000\n",
      "Epoch 109/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 517559.3750 - mean_squared_error: 517559.3750\n",
      "Epoch 109: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 517363.5625 - mean_squared_error: 517363.5625 - val_loss: 1332315.1250 - val_mean_squared_error: 1332315.1250\n",
      "Epoch 110/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 529977.0000 - mean_squared_error: 529977.0000\n",
      "Epoch 110: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 528388.5625 - mean_squared_error: 528388.5625 - val_loss: 1378354.1250 - val_mean_squared_error: 1378354.1250\n",
      "Epoch 111/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 550411.9375 - mean_squared_error: 550411.9375\n",
      "Epoch 111: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 550324.3750 - mean_squared_error: 550324.3750 - val_loss: 1271745.5000 - val_mean_squared_error: 1271745.5000\n",
      "Epoch 112/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 530141.7500 - mean_squared_error: 530141.7500\n",
      "Epoch 112: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 530053.7500 - mean_squared_error: 530053.7500 - val_loss: 1319380.8750 - val_mean_squared_error: 1319380.8750\n",
      "Epoch 113/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 533978.2500 - mean_squared_error: 533978.2500\n",
      "Epoch 113: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 533978.2500 - mean_squared_error: 533978.2500 - val_loss: 1289674.5000 - val_mean_squared_error: 1289674.5000\n",
      "Epoch 114/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 554626.0000 - mean_squared_error: 554626.0000\n",
      "Epoch 114: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 554107.8125 - mean_squared_error: 554107.8125 - val_loss: 1312724.3750 - val_mean_squared_error: 1312724.3750\n",
      "Epoch 115/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 536138.5625 - mean_squared_error: 536138.5625\n",
      "Epoch 115: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 534470.4375 - mean_squared_error: 534470.4375 - val_loss: 1306552.2500 - val_mean_squared_error: 1306552.2500\n",
      "Epoch 116/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 532291.1250 - mean_squared_error: 532291.1250\n",
      "Epoch 116: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 534056.0000 - mean_squared_error: 534056.0000 - val_loss: 1323883.5000 - val_mean_squared_error: 1323883.5000\n",
      "Epoch 117/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 523796.0000 - mean_squared_error: 523796.0000\n",
      "Epoch 117: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 523760.7188 - mean_squared_error: 523760.7188 - val_loss: 1250382.3750 - val_mean_squared_error: 1250382.3750\n",
      "Epoch 118/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 536840.4375 - mean_squared_error: 536840.4375\n",
      "Epoch 118: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 536762.5625 - mean_squared_error: 536762.5625 - val_loss: 1316896.2500 - val_mean_squared_error: 1316896.2500\n",
      "Epoch 119/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 525326.0000 - mean_squared_error: 525326.0000\n",
      "Epoch 119: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 524756.1250 - mean_squared_error: 524756.1250 - val_loss: 1257585.5000 - val_mean_squared_error: 1257585.5000\n",
      "Epoch 120/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 531335.6250 - mean_squared_error: 531335.6250\n",
      "Epoch 120: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 536328.6250 - mean_squared_error: 536328.6250 - val_loss: 1270957.3750 - val_mean_squared_error: 1270957.3750\n",
      "Epoch 121/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 536414.6250 - mean_squared_error: 536414.6250\n",
      "Epoch 121: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 536134.3750 - mean_squared_error: 536134.3750 - val_loss: 1267117.5000 - val_mean_squared_error: 1267117.5000\n",
      "Epoch 122/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 528786.0625 - mean_squared_error: 528786.0625\n",
      "Epoch 122: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 528839.4375 - mean_squared_error: 528839.4375 - val_loss: 1264990.5000 - val_mean_squared_error: 1264990.5000\n",
      "Epoch 123/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 541445.0000 - mean_squared_error: 541445.0000\n",
      "Epoch 123: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 541445.0000 - mean_squared_error: 541445.0000 - val_loss: 1292395.0000 - val_mean_squared_error: 1292395.0000\n",
      "Epoch 124/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 516801.5938 - mean_squared_error: 516801.5938\n",
      "Epoch 124: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 516861.9688 - mean_squared_error: 516861.9688 - val_loss: 1295372.3750 - val_mean_squared_error: 1295372.3750\n",
      "Epoch 125/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 542400.5625 - mean_squared_error: 542400.5625\n",
      "Epoch 125: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 542172.7500 - mean_squared_error: 542172.7500 - val_loss: 1295219.1250 - val_mean_squared_error: 1295219.1250\n",
      "Epoch 126/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 517495.2812 - mean_squared_error: 517495.2812\n",
      "Epoch 126: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 520719.8125 - mean_squared_error: 520719.8125 - val_loss: 1355271.1250 - val_mean_squared_error: 1355271.1250\n",
      "Epoch 127/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 539082.5000 - mean_squared_error: 539082.5000\n",
      "Epoch 127: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 538995.9375 - mean_squared_error: 538995.9375 - val_loss: 1289724.1250 - val_mean_squared_error: 1289724.1250\n",
      "Epoch 128/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 519224.1562 - mean_squared_error: 519224.1562\n",
      "Epoch 128: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 519132.4375 - mean_squared_error: 519132.4375 - val_loss: 1427764.1250 - val_mean_squared_error: 1427764.1250\n",
      "Epoch 129/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 534653.1250 - mean_squared_error: 534653.1250\n",
      "Epoch 129: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 534653.1250 - mean_squared_error: 534653.1250 - val_loss: 1299237.1250 - val_mean_squared_error: 1299237.1250\n",
      "Epoch 130/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 533898.3750 - mean_squared_error: 533898.3750\n",
      "Epoch 130: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 533825.4375 - mean_squared_error: 533825.4375 - val_loss: 1233511.7500 - val_mean_squared_error: 1233511.7500\n",
      "Epoch 131/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 539397.3750 - mean_squared_error: 539397.3750\n",
      "Epoch 131: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 539350.5000 - mean_squared_error: 539350.5000 - val_loss: 1237555.5000 - val_mean_squared_error: 1237555.5000\n",
      "Epoch 132/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 521391.5312 - mean_squared_error: 521391.5312\n",
      "Epoch 132: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 521785.4375 - mean_squared_error: 521785.4375 - val_loss: 1370683.0000 - val_mean_squared_error: 1370683.0000\n",
      "Epoch 133/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 546303.3750 - mean_squared_error: 546303.3750\n",
      "Epoch 133: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 545779.6875 - mean_squared_error: 545779.6875 - val_loss: 1249086.6250 - val_mean_squared_error: 1249086.6250\n",
      "Epoch 134/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 524573.1875 - mean_squared_error: 524573.1875\n",
      "Epoch 134: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 524573.1875 - mean_squared_error: 524573.1875 - val_loss: 1275739.2500 - val_mean_squared_error: 1275739.2500\n",
      "Epoch 135/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 514199.0625 - mean_squared_error: 514199.0625\n",
      "Epoch 135: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 514199.0625 - mean_squared_error: 514199.0625 - val_loss: 1276113.3750 - val_mean_squared_error: 1276113.3750\n",
      "Epoch 136/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 541840.1875 - mean_squared_error: 541840.1875\n",
      "Epoch 136: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 541725.0000 - mean_squared_error: 541725.0000 - val_loss: 1285922.5000 - val_mean_squared_error: 1285922.5000\n",
      "Epoch 137/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 514247.6250 - mean_squared_error: 514247.6250\n",
      "Epoch 137: val_loss did not improve from 1230483.37500\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 513750.5625 - mean_squared_error: 513750.5625 - val_loss: 1304511.2500 - val_mean_squared_error: 1304511.2500\n",
      "Epoch 138/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 525585.8125 - mean_squared_error: 525585.8125\n",
      "Epoch 138: val_loss improved from 1230483.37500 to 1221613.50000, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 525483.2500 - mean_squared_error: 525483.2500 - val_loss: 1221613.5000 - val_mean_squared_error: 1221613.5000\n",
      "Epoch 139/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 517486.4062 - mean_squared_error: 517486.4062\n",
      "Epoch 139: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 517248.0312 - mean_squared_error: 517248.0312 - val_loss: 1346458.5000 - val_mean_squared_error: 1346458.5000\n",
      "Epoch 140/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 538788.6875 - mean_squared_error: 538788.6875\n",
      "Epoch 140: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 538199.0625 - mean_squared_error: 538199.0625 - val_loss: 1313814.0000 - val_mean_squared_error: 1313814.0000\n",
      "Epoch 141/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 522622.7812 - mean_squared_error: 522622.7812\n",
      "Epoch 141: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 10s 11ms/step - loss: 523362.1562 - mean_squared_error: 523362.1562 - val_loss: 1337947.0000 - val_mean_squared_error: 1337947.0000\n",
      "Epoch 142/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 526502.5000 - mean_squared_error: 526502.5000\n",
      "Epoch 142: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 526502.5000 - mean_squared_error: 526502.5000 - val_loss: 1306435.5000 - val_mean_squared_error: 1306435.5000\n",
      "Epoch 143/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 527337.8125 - mean_squared_error: 527337.8125\n",
      "Epoch 143: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 530137.6250 - mean_squared_error: 530137.6250 - val_loss: 1242338.7500 - val_mean_squared_error: 1242338.7500\n",
      "Epoch 144/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 511333.3750 - mean_squared_error: 511333.3750\n",
      "Epoch 144: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 511714.6562 - mean_squared_error: 511714.6562 - val_loss: 1318611.0000 - val_mean_squared_error: 1318611.0000\n",
      "Epoch 145/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 533522.1250 - mean_squared_error: 533522.1250\n",
      "Epoch 145: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 532857.3750 - mean_squared_error: 532857.3750 - val_loss: 1250304.0000 - val_mean_squared_error: 1250304.0000\n",
      "Epoch 146/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 518922.5000 - mean_squared_error: 518922.5000\n",
      "Epoch 146: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 519093.3125 - mean_squared_error: 519093.3125 - val_loss: 1302556.2500 - val_mean_squared_error: 1302556.2500\n",
      "Epoch 147/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 523347.5938 - mean_squared_error: 523347.5938\n",
      "Epoch 147: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 11s 12ms/step - loss: 523299.5625 - mean_squared_error: 523299.5625 - val_loss: 1249177.8750 - val_mean_squared_error: 1249177.8750\n",
      "Epoch 148/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 530222.5000 - mean_squared_error: 530222.5000\n",
      "Epoch 148: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 10s 12ms/step - loss: 530153.0000 - mean_squared_error: 530153.0000 - val_loss: 1263717.0000 - val_mean_squared_error: 1263717.0000\n",
      "Epoch 149/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 528919.5625 - mean_squared_error: 528919.5625\n",
      "Epoch 149: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 10s 11ms/step - loss: 528792.2500 - mean_squared_error: 528792.2500 - val_loss: 1400395.8750 - val_mean_squared_error: 1400395.8750\n",
      "Epoch 150/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 526883.3750 - mean_squared_error: 526883.3750\n",
      "Epoch 150: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 526883.3750 - mean_squared_error: 526883.3750 - val_loss: 1248578.0000 - val_mean_squared_error: 1248578.0000\n",
      "Epoch 151/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 531247.8125 - mean_squared_error: 531247.8125\n",
      "Epoch 151: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 531153.4375 - mean_squared_error: 531153.4375 - val_loss: 1268322.1250 - val_mean_squared_error: 1268322.1250\n",
      "Epoch 152/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 512269.6875 - mean_squared_error: 512269.6875\n",
      "Epoch 152: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 512866.5625 - mean_squared_error: 512866.5625 - val_loss: 1259782.8750 - val_mean_squared_error: 1259782.8750\n",
      "Epoch 153/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 534734.9375 - mean_squared_error: 534734.9375\n",
      "Epoch 153: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 535400.7500 - mean_squared_error: 535400.7500 - val_loss: 1328239.3750 - val_mean_squared_error: 1328239.3750\n",
      "Epoch 154/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 511142.3750 - mean_squared_error: 511142.3750\n",
      "Epoch 154: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 512823.6875 - mean_squared_error: 512823.6875 - val_loss: 1323980.1250 - val_mean_squared_error: 1323980.1250\n",
      "Epoch 155/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 529453.6875 - mean_squared_error: 529453.6875\n",
      "Epoch 155: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 529453.6875 - mean_squared_error: 529453.6875 - val_loss: 1369816.0000 - val_mean_squared_error: 1369816.0000\n",
      "Epoch 156/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 529549.5625 - mean_squared_error: 529549.5625\n",
      "Epoch 156: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 528854.0000 - mean_squared_error: 528854.0000 - val_loss: 1254775.6250 - val_mean_squared_error: 1254775.6250\n",
      "Epoch 157/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 527857.6875 - mean_squared_error: 527857.6875\n",
      "Epoch 157: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 527315.3125 - mean_squared_error: 527315.3125 - val_loss: 1298624.5000 - val_mean_squared_error: 1298624.5000\n",
      "Epoch 158/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 517717.5000 - mean_squared_error: 517717.5000\n",
      "Epoch 158: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 517366.0312 - mean_squared_error: 517366.0312 - val_loss: 1267627.6250 - val_mean_squared_error: 1267627.6250\n",
      "Epoch 159/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 521642.0000 - mean_squared_error: 521642.0000\n",
      "Epoch 159: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 521642.0000 - mean_squared_error: 521642.0000 - val_loss: 1245536.5000 - val_mean_squared_error: 1245536.5000\n",
      "Epoch 160/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 523861.3125 - mean_squared_error: 523861.3125\n",
      "Epoch 160: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 523914.6250 - mean_squared_error: 523914.6250 - val_loss: 1349645.0000 - val_mean_squared_error: 1349645.0000\n",
      "Epoch 161/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 522106.6562 - mean_squared_error: 522106.6562\n",
      "Epoch 161: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 521701.1875 - mean_squared_error: 521701.1875 - val_loss: 1238949.2500 - val_mean_squared_error: 1238949.2500\n",
      "Epoch 162/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 532897.0000 - mean_squared_error: 532897.0000\n",
      "Epoch 162: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 533545.5000 - mean_squared_error: 533545.5000 - val_loss: 1296809.8750 - val_mean_squared_error: 1296809.8750\n",
      "Epoch 163/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 520753.4375 - mean_squared_error: 520753.4375\n",
      "Epoch 163: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 520506.0312 - mean_squared_error: 520506.0312 - val_loss: 1237018.3750 - val_mean_squared_error: 1237018.3750\n",
      "Epoch 164/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 520807.2188 - mean_squared_error: 520807.2188\n",
      "Epoch 164: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 520748.2812 - mean_squared_error: 520748.2812 - val_loss: 1263018.6250 - val_mean_squared_error: 1263018.6250\n",
      "Epoch 165/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 545940.5000 - mean_squared_error: 545940.5000\n",
      "Epoch 165: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 545908.9375 - mean_squared_error: 545908.9375 - val_loss: 1288024.8750 - val_mean_squared_error: 1288024.8750\n",
      "Epoch 166/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 507327.1562 - mean_squared_error: 507327.1562\n",
      "Epoch 166: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 507816.1875 - mean_squared_error: 507816.1875 - val_loss: 1265964.3750 - val_mean_squared_error: 1265964.3750\n",
      "Epoch 167/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 535867.1875 - mean_squared_error: 535867.1875\n",
      "Epoch 167: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 535433.4375 - mean_squared_error: 535433.4375 - val_loss: 1340219.0000 - val_mean_squared_error: 1340219.0000\n",
      "Epoch 168/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 504706.5625 - mean_squared_error: 504706.5625\n",
      "Epoch 168: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 505716.1250 - mean_squared_error: 505716.1250 - val_loss: 1241514.7500 - val_mean_squared_error: 1241514.7500\n",
      "Epoch 169/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 527778.4375 - mean_squared_error: 527778.4375\n",
      "Epoch 169: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 528089.6250 - mean_squared_error: 528089.6250 - val_loss: 1368302.2500 - val_mean_squared_error: 1368302.2500\n",
      "Epoch 170/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 522940.7500 - mean_squared_error: 522940.7500\n",
      "Epoch 170: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 522986.3125 - mean_squared_error: 522986.3125 - val_loss: 1319901.3750 - val_mean_squared_error: 1319901.3750\n",
      "Epoch 171/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 514041.7188 - mean_squared_error: 514041.7188\n",
      "Epoch 171: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 513734.4688 - mean_squared_error: 513734.4688 - val_loss: 1255839.2500 - val_mean_squared_error: 1255839.2500\n",
      "Epoch 172/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 538073.3750 - mean_squared_error: 538073.3750\n",
      "Epoch 172: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 536257.0000 - mean_squared_error: 536257.0000 - val_loss: 1318023.8750 - val_mean_squared_error: 1318023.8750\n",
      "Epoch 173/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 505870.8750 - mean_squared_error: 505870.8750\n",
      "Epoch 173: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 506236.8438 - mean_squared_error: 506236.8438 - val_loss: 1276936.1250 - val_mean_squared_error: 1276936.1250\n",
      "Epoch 174/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 535600.5000 - mean_squared_error: 535600.5000\n",
      "Epoch 174: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 536774.3750 - mean_squared_error: 536774.3750 - val_loss: 1281401.1250 - val_mean_squared_error: 1281401.1250\n",
      "Epoch 175/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 511324.0312 - mean_squared_error: 511324.0312\n",
      "Epoch 175: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 510691.5000 - mean_squared_error: 510691.5000 - val_loss: 1353965.6250 - val_mean_squared_error: 1353965.6250\n",
      "Epoch 176/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 548232.3750 - mean_squared_error: 548232.3750\n",
      "Epoch 176: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 547844.7500 - mean_squared_error: 547844.7500 - val_loss: 1233152.6250 - val_mean_squared_error: 1233152.6250\n",
      "Epoch 177/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 495006.8438 - mean_squared_error: 495006.8438\n",
      "Epoch 177: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 494963.2500 - mean_squared_error: 494963.2500 - val_loss: 1258282.3750 - val_mean_squared_error: 1258282.3750\n",
      "Epoch 178/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 533219.8125 - mean_squared_error: 533219.8125\n",
      "Epoch 178: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 533219.8125 - mean_squared_error: 533219.8125 - val_loss: 1297729.1250 - val_mean_squared_error: 1297729.1250\n",
      "Epoch 179/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 511277.5000 - mean_squared_error: 511277.5000\n",
      "Epoch 179: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 511208.1562 - mean_squared_error: 511208.1562 - val_loss: 1254074.0000 - val_mean_squared_error: 1254074.0000\n",
      "Epoch 180/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 529453.3125 - mean_squared_error: 529453.3125\n",
      "Epoch 180: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 528939.0000 - mean_squared_error: 528939.0000 - val_loss: 1274424.3750 - val_mean_squared_error: 1274424.3750\n",
      "Epoch 181/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 534750.1250 - mean_squared_error: 534750.1250\n",
      "Epoch 181: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 535411.5000 - mean_squared_error: 535411.5000 - val_loss: 1319296.3750 - val_mean_squared_error: 1319296.3750\n",
      "Epoch 182/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 504298.4375 - mean_squared_error: 504298.4375\n",
      "Epoch 182: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 504315.8125 - mean_squared_error: 504315.8125 - val_loss: 1273703.7500 - val_mean_squared_error: 1273703.7500\n",
      "Epoch 183/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 536455.3750 - mean_squared_error: 536455.3750\n",
      "Epoch 183: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 536455.3750 - mean_squared_error: 536455.3750 - val_loss: 1234167.5000 - val_mean_squared_error: 1234167.5000\n",
      "Epoch 184/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 510263.3125 - mean_squared_error: 510263.3125\n",
      "Epoch 184: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 510940.1562 - mean_squared_error: 510940.1562 - val_loss: 1317191.2500 - val_mean_squared_error: 1317191.2500\n",
      "Epoch 185/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 536237.0625 - mean_squared_error: 536237.0625\n",
      "Epoch 185: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 534683.8750 - mean_squared_error: 534683.8750 - val_loss: 1247741.6250 - val_mean_squared_error: 1247741.6250\n",
      "Epoch 186/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 517171.5000 - mean_squared_error: 517171.5000\n",
      "Epoch 186: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 516515.5312 - mean_squared_error: 516515.5312 - val_loss: 1300051.3750 - val_mean_squared_error: 1300051.3750\n",
      "Epoch 187/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 519823.3125 - mean_squared_error: 519823.3125\n",
      "Epoch 187: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 519049.1250 - mean_squared_error: 519049.1250 - val_loss: 1267596.7500 - val_mean_squared_error: 1267596.7500\n",
      "Epoch 188/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 524930.3125 - mean_squared_error: 524930.3125\n",
      "Epoch 188: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 524864.2500 - mean_squared_error: 524864.2500 - val_loss: 1355859.7500 - val_mean_squared_error: 1355859.7500\n",
      "Epoch 189/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 511677.6562 - mean_squared_error: 511677.6562\n",
      "Epoch 189: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 510252.0000 - mean_squared_error: 510252.0000 - val_loss: 1349158.5000 - val_mean_squared_error: 1349158.5000\n",
      "Epoch 190/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 520433.7500 - mean_squared_error: 520433.7500\n",
      "Epoch 190: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 520169.1562 - mean_squared_error: 520169.1562 - val_loss: 1347905.6250 - val_mean_squared_error: 1347905.6250\n",
      "Epoch 191/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 529243.3125 - mean_squared_error: 529243.3125\n",
      "Epoch 191: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 530127.2500 - mean_squared_error: 530127.2500 - val_loss: 1312757.6250 - val_mean_squared_error: 1312757.6250\n",
      "Epoch 192/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 524805.0000 - mean_squared_error: 524805.0000\n",
      "Epoch 192: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 525173.3750 - mean_squared_error: 525173.3750 - val_loss: 1309387.7500 - val_mean_squared_error: 1309387.7500\n",
      "Epoch 193/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 504457.7500 - mean_squared_error: 504457.7500\n",
      "Epoch 193: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 504030.9688 - mean_squared_error: 504030.9688 - val_loss: 1254722.0000 - val_mean_squared_error: 1254722.0000\n",
      "Epoch 194/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 536589.8125 - mean_squared_error: 536589.8125\n",
      "Epoch 194: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 536574.4375 - mean_squared_error: 536574.4375 - val_loss: 1334701.7500 - val_mean_squared_error: 1334701.7500\n",
      "Epoch 195/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 521126.2812 - mean_squared_error: 521126.2812\n",
      "Epoch 195: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 520063.2188 - mean_squared_error: 520063.2188 - val_loss: 1281309.7500 - val_mean_squared_error: 1281309.7500\n",
      "Epoch 196/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 518393.3125 - mean_squared_error: 518393.3125\n",
      "Epoch 196: val_loss did not improve from 1221613.50000\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 516642.8438 - mean_squared_error: 516642.8438 - val_loss: 1283516.7500 - val_mean_squared_error: 1283516.7500\n",
      "Epoch 197/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 515828.7812 - mean_squared_error: 515828.7812\n",
      "Epoch 197: val_loss improved from 1221613.50000 to 1212212.12500, saving model to ./models/dnn_cost\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost/assets\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 515817.9688 - mean_squared_error: 515817.9688 - val_loss: 1212212.1250 - val_mean_squared_error: 1212212.1250\n",
      "Epoch 198/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 526928.0625 - mean_squared_error: 526928.0625\n",
      "Epoch 198: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 528521.0625 - mean_squared_error: 528521.0625 - val_loss: 1279922.1250 - val_mean_squared_error: 1279922.1250\n",
      "Epoch 199/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 509626.3750 - mean_squared_error: 509626.3750\n",
      "Epoch 199: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 510426.7812 - mean_squared_error: 510426.7812 - val_loss: 1265489.1250 - val_mean_squared_error: 1265489.1250\n",
      "Epoch 200/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 523095.8750 - mean_squared_error: 523095.8750\n",
      "Epoch 200: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 523303.5938 - mean_squared_error: 523303.5938 - val_loss: 1257142.1250 - val_mean_squared_error: 1257142.1250\n",
      "Epoch 201/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 516565.7188 - mean_squared_error: 516565.7188\n",
      "Epoch 201: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 516467.2500 - mean_squared_error: 516467.2500 - val_loss: 1265081.6250 - val_mean_squared_error: 1265081.6250\n",
      "Epoch 202/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 514374.0625 - mean_squared_error: 514374.0625\n",
      "Epoch 202: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 514374.0625 - mean_squared_error: 514374.0625 - val_loss: 1225097.1250 - val_mean_squared_error: 1225097.1250\n",
      "Epoch 203/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 534795.8125 - mean_squared_error: 534795.8125\n",
      "Epoch 203: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 534414.0625 - mean_squared_error: 534414.0625 - val_loss: 1358033.1250 - val_mean_squared_error: 1358033.1250\n",
      "Epoch 204/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 495254.3750 - mean_squared_error: 495254.3750\n",
      "Epoch 204: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 495143.5625 - mean_squared_error: 495143.5625 - val_loss: 1231461.8750 - val_mean_squared_error: 1231461.8750\n",
      "Epoch 205/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 536451.8125 - mean_squared_error: 536451.8125\n",
      "Epoch 205: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 536159.6250 - mean_squared_error: 536159.6250 - val_loss: 1261785.1250 - val_mean_squared_error: 1261785.1250\n",
      "Epoch 206/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 500922.5938 - mean_squared_error: 500922.5938\n",
      "Epoch 206: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 500922.5938 - mean_squared_error: 500922.5938 - val_loss: 1240046.2500 - val_mean_squared_error: 1240046.2500\n",
      "Epoch 207/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 526028.2500 - mean_squared_error: 526028.2500\n",
      "Epoch 207: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 524250.9375 - mean_squared_error: 524250.9375 - val_loss: 1260136.7500 - val_mean_squared_error: 1260136.7500\n",
      "Epoch 208/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 506992.4062 - mean_squared_error: 506992.4062\n",
      "Epoch 208: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 506236.9688 - mean_squared_error: 506236.9688 - val_loss: 1320505.0000 - val_mean_squared_error: 1320505.0000\n",
      "Epoch 209/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 532378.3125 - mean_squared_error: 532378.3125\n",
      "Epoch 209: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 532761.6250 - mean_squared_error: 532761.6250 - val_loss: 1261630.3750 - val_mean_squared_error: 1261630.3750\n",
      "Epoch 210/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 492373.3125 - mean_squared_error: 492373.3125\n",
      "Epoch 210: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 493757.4688 - mean_squared_error: 493757.4688 - val_loss: 1251092.3750 - val_mean_squared_error: 1251092.3750\n",
      "Epoch 211/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 528235.6875 - mean_squared_error: 528235.6875\n",
      "Epoch 211: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 527661.3750 - mean_squared_error: 527661.3750 - val_loss: 1301300.8750 - val_mean_squared_error: 1301300.8750\n",
      "Epoch 212/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 516288.3750 - mean_squared_error: 516288.3750\n",
      "Epoch 212: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 515579.0000 - mean_squared_error: 515579.0000 - val_loss: 1231866.5000 - val_mean_squared_error: 1231866.5000\n",
      "Epoch 213/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 519602.2500 - mean_squared_error: 519602.2500\n",
      "Epoch 213: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 519602.2500 - mean_squared_error: 519602.2500 - val_loss: 1281503.7500 - val_mean_squared_error: 1281503.7500\n",
      "Epoch 214/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 513139.8750 - mean_squared_error: 513139.8750\n",
      "Epoch 214: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 512692.3438 - mean_squared_error: 512692.3438 - val_loss: 1217072.0000 - val_mean_squared_error: 1217072.0000\n",
      "Epoch 215/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 518114.5938 - mean_squared_error: 518114.5938\n",
      "Epoch 215: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 518137.8750 - mean_squared_error: 518137.8750 - val_loss: 1374485.7500 - val_mean_squared_error: 1374485.7500\n",
      "Epoch 216/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 512555.3125 - mean_squared_error: 512555.3125\n",
      "Epoch 216: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 513099.5000 - mean_squared_error: 513099.5000 - val_loss: 1234165.2500 - val_mean_squared_error: 1234165.2500\n",
      "Epoch 217/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 515617.2812 - mean_squared_error: 515617.2812\n",
      "Epoch 217: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 515654.3750 - mean_squared_error: 515654.3750 - val_loss: 1362425.2500 - val_mean_squared_error: 1362425.2500\n",
      "Epoch 218/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 525624.0000 - mean_squared_error: 525624.0000\n",
      "Epoch 218: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 525209.6875 - mean_squared_error: 525209.6875 - val_loss: 1294430.7500 - val_mean_squared_error: 1294430.7500\n",
      "Epoch 219/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 508785.3750 - mean_squared_error: 508785.3750\n",
      "Epoch 219: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 508727.0312 - mean_squared_error: 508727.0312 - val_loss: 1250477.3750 - val_mean_squared_error: 1250477.3750\n",
      "Epoch 220/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 518510.7188 - mean_squared_error: 518510.7188\n",
      "Epoch 220: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 518307.9062 - mean_squared_error: 518307.9062 - val_loss: 1344745.0000 - val_mean_squared_error: 1344745.0000\n",
      "Epoch 221/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 512364.9688 - mean_squared_error: 512364.9688\n",
      "Epoch 221: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 512364.9688 - mean_squared_error: 512364.9688 - val_loss: 1259954.6250 - val_mean_squared_error: 1259954.6250\n",
      "Epoch 222/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 528421.8125 - mean_squared_error: 528421.8125\n",
      "Epoch 222: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 527753.8125 - mean_squared_error: 527753.8125 - val_loss: 1295169.3750 - val_mean_squared_error: 1295169.3750\n",
      "Epoch 223/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 514144.4375 - mean_squared_error: 514144.4375\n",
      "Epoch 223: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 513590.6875 - mean_squared_error: 513590.6875 - val_loss: 1241309.8750 - val_mean_squared_error: 1241309.8750\n",
      "Epoch 224/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 503010.0312 - mean_squared_error: 503010.0312\n",
      "Epoch 224: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 503010.0312 - mean_squared_error: 503010.0312 - val_loss: 1277758.7500 - val_mean_squared_error: 1277758.7500\n",
      "Epoch 225/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 526259.9375 - mean_squared_error: 526259.9375\n",
      "Epoch 225: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 524398.1250 - mean_squared_error: 524398.1250 - val_loss: 1225436.1250 - val_mean_squared_error: 1225436.1250\n",
      "Epoch 226/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 502670.0000 - mean_squared_error: 502670.0000\n",
      "Epoch 226: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 510803.3750 - mean_squared_error: 510803.3750 - val_loss: 1237273.2500 - val_mean_squared_error: 1237273.2500\n",
      "Epoch 227/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 508671.8750 - mean_squared_error: 508671.8750\n",
      "Epoch 227: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 508729.7812 - mean_squared_error: 508729.7812 - val_loss: 1272663.3750 - val_mean_squared_error: 1272663.3750\n",
      "Epoch 228/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 527025.4375 - mean_squared_error: 527025.4375\n",
      "Epoch 228: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 525909.7500 - mean_squared_error: 525909.7500 - val_loss: 1243044.8750 - val_mean_squared_error: 1243044.8750\n",
      "Epoch 229/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 495436.3125 - mean_squared_error: 495436.3125\n",
      "Epoch 229: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 494645.6250 - mean_squared_error: 494645.6250 - val_loss: 1240431.3750 - val_mean_squared_error: 1240431.3750\n",
      "Epoch 230/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 517623.0938 - mean_squared_error: 517623.0938\n",
      "Epoch 230: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 519299.9375 - mean_squared_error: 519299.9375 - val_loss: 1248037.3750 - val_mean_squared_error: 1248037.3750\n",
      "Epoch 231/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 504126.2188 - mean_squared_error: 504126.2188\n",
      "Epoch 231: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 503479.0625 - mean_squared_error: 503479.0625 - val_loss: 1311387.1250 - val_mean_squared_error: 1311387.1250\n",
      "Epoch 232/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 537488.1875 - mean_squared_error: 537488.1875\n",
      "Epoch 232: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 537488.1875 - mean_squared_error: 537488.1875 - val_loss: 1361503.0000 - val_mean_squared_error: 1361503.0000\n",
      "Epoch 233/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 497253.1250 - mean_squared_error: 497253.1250\n",
      "Epoch 233: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 496824.0625 - mean_squared_error: 496824.0625 - val_loss: 1280341.0000 - val_mean_squared_error: 1280341.0000\n",
      "Epoch 234/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 521278.6875 - mean_squared_error: 521278.6875\n",
      "Epoch 234: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 521217.8438 - mean_squared_error: 521217.8438 - val_loss: 1331131.7500 - val_mean_squared_error: 1331131.7500\n",
      "Epoch 235/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 516153.5000 - mean_squared_error: 516153.5000\n",
      "Epoch 235: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 515767.0312 - mean_squared_error: 515767.0312 - val_loss: 1222900.2500 - val_mean_squared_error: 1222900.2500\n",
      "Epoch 236/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 504586.5312 - mean_squared_error: 504586.5312\n",
      "Epoch 236: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 503979.3438 - mean_squared_error: 503979.3438 - val_loss: 1325502.8750 - val_mean_squared_error: 1325502.8750\n",
      "Epoch 237/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 515843.4688 - mean_squared_error: 515843.4688\n",
      "Epoch 237: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 516360.6562 - mean_squared_error: 516360.6562 - val_loss: 1240279.0000 - val_mean_squared_error: 1240279.0000\n",
      "Epoch 238/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 505214.5312 - mean_squared_error: 505214.5312\n",
      "Epoch 238: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 504498.5938 - mean_squared_error: 504498.5938 - val_loss: 1272827.0000 - val_mean_squared_error: 1272827.0000\n",
      "Epoch 239/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 517689.6875 - mean_squared_error: 517689.6875\n",
      "Epoch 239: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 517072.2500 - mean_squared_error: 517072.2500 - val_loss: 1318512.6250 - val_mean_squared_error: 1318512.6250\n",
      "Epoch 240/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 503769.5625 - mean_squared_error: 503769.5625\n",
      "Epoch 240: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 502834.5625 - mean_squared_error: 502834.5625 - val_loss: 1250212.3750 - val_mean_squared_error: 1250212.3750\n",
      "Epoch 241/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 503835.5938 - mean_squared_error: 503835.5938\n",
      "Epoch 241: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 505277.9062 - mean_squared_error: 505277.9062 - val_loss: 1225314.8750 - val_mean_squared_error: 1225314.8750\n",
      "Epoch 242/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 523663.2812 - mean_squared_error: 523663.2812\n",
      "Epoch 242: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 523403.4375 - mean_squared_error: 523403.4375 - val_loss: 1243755.8750 - val_mean_squared_error: 1243755.8750\n",
      "Epoch 243/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 507331.8125 - mean_squared_error: 507331.8125\n",
      "Epoch 243: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 507241.0938 - mean_squared_error: 507241.0938 - val_loss: 1225687.2500 - val_mean_squared_error: 1225687.2500\n",
      "Epoch 244/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 514677.9688 - mean_squared_error: 514677.9688\n",
      "Epoch 244: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 514473.5938 - mean_squared_error: 514473.5938 - val_loss: 1246691.8750 - val_mean_squared_error: 1246691.8750\n",
      "Epoch 245/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 512473.4688 - mean_squared_error: 512473.4688\n",
      "Epoch 245: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 511575.8438 - mean_squared_error: 511575.8438 - val_loss: 1269955.7500 - val_mean_squared_error: 1269955.7500\n",
      "Epoch 246/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 506757.6875 - mean_squared_error: 506757.6875\n",
      "Epoch 246: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 506784.1562 - mean_squared_error: 506784.1562 - val_loss: 1342709.7500 - val_mean_squared_error: 1342709.7500\n",
      "Epoch 247/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 528347.0625 - mean_squared_error: 528347.0625\n",
      "Epoch 247: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 528238.7500 - mean_squared_error: 528238.7500 - val_loss: 1233673.0000 - val_mean_squared_error: 1233673.0000\n",
      "Epoch 248/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 491121.3438 - mean_squared_error: 491121.3438\n",
      "Epoch 248: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 491698.5000 - mean_squared_error: 491698.5000 - val_loss: 1249042.3750 - val_mean_squared_error: 1249042.3750\n",
      "Epoch 249/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 530042.6250 - mean_squared_error: 530042.6250\n",
      "Epoch 249: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 530442.6250 - mean_squared_error: 530442.6250 - val_loss: 1276912.2500 - val_mean_squared_error: 1276912.2500\n",
      "Epoch 250/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 492941.9688 - mean_squared_error: 492941.9688\n",
      "Epoch 250: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 492122.4375 - mean_squared_error: 492122.4375 - val_loss: 1278631.6250 - val_mean_squared_error: 1278631.6250\n",
      "Epoch 251/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 516164.2812 - mean_squared_error: 516164.2812\n",
      "Epoch 251: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 518516.5625 - mean_squared_error: 518516.5625 - val_loss: 1301788.7500 - val_mean_squared_error: 1301788.7500\n",
      "Epoch 252/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 506332.8750 - mean_squared_error: 506332.8750\n",
      "Epoch 252: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 506628.0938 - mean_squared_error: 506628.0938 - val_loss: 1229822.0000 - val_mean_squared_error: 1229822.0000\n",
      "Epoch 253/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 509482.8125 - mean_squared_error: 509482.8125\n",
      "Epoch 253: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 509482.8125 - mean_squared_error: 509482.8125 - val_loss: 1261859.3750 - val_mean_squared_error: 1261859.3750\n",
      "Epoch 254/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 503821.1250 - mean_squared_error: 503821.1250\n",
      "Epoch 254: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 503821.1250 - mean_squared_error: 503821.1250 - val_loss: 1236114.8750 - val_mean_squared_error: 1236114.8750\n",
      "Epoch 255/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 513459.1562 - mean_squared_error: 513459.1562\n",
      "Epoch 255: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 513438.2500 - mean_squared_error: 513438.2500 - val_loss: 1255647.3750 - val_mean_squared_error: 1255647.3750\n",
      "Epoch 256/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 515910.0312 - mean_squared_error: 515910.0312\n",
      "Epoch 256: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 515910.0312 - mean_squared_error: 515910.0312 - val_loss: 1263140.3750 - val_mean_squared_error: 1263140.3750\n",
      "Epoch 257/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 513548.8125 - mean_squared_error: 513548.8125\n",
      "Epoch 257: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 513017.4688 - mean_squared_error: 513017.4688 - val_loss: 1246224.5000 - val_mean_squared_error: 1246224.5000\n",
      "Epoch 258/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 500880.7500 - mean_squared_error: 500880.7500\n",
      "Epoch 258: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 500774.6875 - mean_squared_error: 500774.6875 - val_loss: 1322332.1250 - val_mean_squared_error: 1322332.1250\n",
      "Epoch 259/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 519175.4375 - mean_squared_error: 519175.4375\n",
      "Epoch 259: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 519631.9062 - mean_squared_error: 519631.9062 - val_loss: 1214924.5000 - val_mean_squared_error: 1214924.5000\n",
      "Epoch 260/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 497073.5000 - mean_squared_error: 497073.5000\n",
      "Epoch 260: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 496238.8125 - mean_squared_error: 496238.8125 - val_loss: 1298955.5000 - val_mean_squared_error: 1298955.5000\n",
      "Epoch 261/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 507879.3125 - mean_squared_error: 507879.3125\n",
      "Epoch 261: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 507746.0312 - mean_squared_error: 507746.0312 - val_loss: 1358842.3750 - val_mean_squared_error: 1358842.3750\n",
      "Epoch 262/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 511546.2188 - mean_squared_error: 511546.2188\n",
      "Epoch 262: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 511489.1875 - mean_squared_error: 511489.1875 - val_loss: 1258286.3750 - val_mean_squared_error: 1258286.3750\n",
      "Epoch 263/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 516628.8750 - mean_squared_error: 516628.8750\n",
      "Epoch 263: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 516100.3438 - mean_squared_error: 516100.3438 - val_loss: 1232307.7500 - val_mean_squared_error: 1232307.7500\n",
      "Epoch 264/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 507233.9375 - mean_squared_error: 507233.9375\n",
      "Epoch 264: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 507233.9375 - mean_squared_error: 507233.9375 - val_loss: 1286637.7500 - val_mean_squared_error: 1286637.7500\n",
      "Epoch 265/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 505824.1562 - mean_squared_error: 505824.1562\n",
      "Epoch 265: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 505824.1562 - mean_squared_error: 505824.1562 - val_loss: 1265596.2500 - val_mean_squared_error: 1265596.2500\n",
      "Epoch 266/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 507534.5312 - mean_squared_error: 507534.5312\n",
      "Epoch 266: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 508064.2188 - mean_squared_error: 508064.2188 - val_loss: 1278820.6250 - val_mean_squared_error: 1278820.6250\n",
      "Epoch 267/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 515775.9375 - mean_squared_error: 515775.9375\n",
      "Epoch 267: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 515775.9375 - mean_squared_error: 515775.9375 - val_loss: 1242128.7500 - val_mean_squared_error: 1242128.7500\n",
      "Epoch 268/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 498582.4375 - mean_squared_error: 498582.4375\n",
      "Epoch 268: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 498131.2188 - mean_squared_error: 498131.2188 - val_loss: 1265667.6250 - val_mean_squared_error: 1265667.6250\n",
      "Epoch 269/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 508460.7812 - mean_squared_error: 508460.7812\n",
      "Epoch 269: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 509240.6875 - mean_squared_error: 509240.6875 - val_loss: 1250850.2500 - val_mean_squared_error: 1250850.2500\n",
      "Epoch 270/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 506065.7188 - mean_squared_error: 506065.7188\n",
      "Epoch 270: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 506172.0000 - mean_squared_error: 506172.0000 - val_loss: 1241917.8750 - val_mean_squared_error: 1241917.8750\n",
      "Epoch 271/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 516932.7188 - mean_squared_error: 516932.7188\n",
      "Epoch 271: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 516233.1250 - mean_squared_error: 516233.1250 - val_loss: 1277316.1250 - val_mean_squared_error: 1277316.1250\n",
      "Epoch 272/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 497076.3438 - mean_squared_error: 497076.3438\n",
      "Epoch 272: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 497005.4062 - mean_squared_error: 497005.4062 - val_loss: 1309108.3750 - val_mean_squared_error: 1309108.3750\n",
      "Epoch 273/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 513816.4062 - mean_squared_error: 513816.4062\n",
      "Epoch 273: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 515814.2188 - mean_squared_error: 515814.2188 - val_loss: 1252143.6250 - val_mean_squared_error: 1252143.6250\n",
      "Epoch 274/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 508221.3750 - mean_squared_error: 508221.3750\n",
      "Epoch 274: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 508221.3750 - mean_squared_error: 508221.3750 - val_loss: 1239001.1250 - val_mean_squared_error: 1239001.1250\n",
      "Epoch 275/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 509781.5312 - mean_squared_error: 509781.5312\n",
      "Epoch 275: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 510634.0312 - mean_squared_error: 510634.0312 - val_loss: 1236784.5000 - val_mean_squared_error: 1236784.5000\n",
      "Epoch 276/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 503809.4688 - mean_squared_error: 503809.4688\n",
      "Epoch 276: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 503643.5625 - mean_squared_error: 503643.5625 - val_loss: 1258537.5000 - val_mean_squared_error: 1258537.5000\n",
      "Epoch 277/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 517025.5938 - mean_squared_error: 517025.5938\n",
      "Epoch 277: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 516957.3125 - mean_squared_error: 516957.3125 - val_loss: 1251051.2500 - val_mean_squared_error: 1251051.2500\n",
      "Epoch 278/1000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 495247.7812 - mean_squared_error: 495247.7812\n",
      "Epoch 278: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 495144.0000 - mean_squared_error: 495144.0000 - val_loss: 1273308.5000 - val_mean_squared_error: 1273308.5000\n",
      "Epoch 279/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 511545.5938 - mean_squared_error: 511545.5938\n",
      "Epoch 279: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 511109.9062 - mean_squared_error: 511109.9062 - val_loss: 1260837.2500 - val_mean_squared_error: 1260837.2500\n",
      "Epoch 280/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 500588.5938 - mean_squared_error: 500588.5938\n",
      "Epoch 280: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 502126.3438 - mean_squared_error: 502126.3438 - val_loss: 1240698.3750 - val_mean_squared_error: 1240698.3750\n",
      "Epoch 281/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 506051.6250 - mean_squared_error: 506051.6250\n",
      "Epoch 281: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 506589.1250 - mean_squared_error: 506589.1250 - val_loss: 1259894.1250 - val_mean_squared_error: 1259894.1250\n",
      "Epoch 282/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 515756.7500 - mean_squared_error: 515756.7500\n",
      "Epoch 282: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 515756.7500 - mean_squared_error: 515756.7500 - val_loss: 1241157.3750 - val_mean_squared_error: 1241157.3750\n",
      "Epoch 283/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 497487.9688 - mean_squared_error: 497487.9688\n",
      "Epoch 283: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 504812.0625 - mean_squared_error: 504812.0625 - val_loss: 1227675.6250 - val_mean_squared_error: 1227675.6250\n",
      "Epoch 284/1000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 504528.0938 - mean_squared_error: 504528.0938\n",
      "Epoch 284: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 504769.7188 - mean_squared_error: 504769.7188 - val_loss: 1268736.8750 - val_mean_squared_error: 1268736.8750\n",
      "Epoch 285/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 508445.6875 - mean_squared_error: 508445.6875\n",
      "Epoch 285: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 507021.7188 - mean_squared_error: 507021.7188 - val_loss: 1244497.5000 - val_mean_squared_error: 1244497.5000\n",
      "Epoch 286/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 525548.8750 - mean_squared_error: 525548.8750\n",
      "Epoch 286: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 527714.6875 - mean_squared_error: 527714.6875 - val_loss: 1221809.5000 - val_mean_squared_error: 1221809.5000\n",
      "Epoch 287/1000\n",
      "871/879 [============================>.] - ETA: 0s - loss: 498792.6250 - mean_squared_error: 498792.6250\n",
      "Epoch 287: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 498870.0938 - mean_squared_error: 498870.0938 - val_loss: 1268191.6250 - val_mean_squared_error: 1268191.6250\n",
      "Epoch 288/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 510755.3750 - mean_squared_error: 510755.3750\n",
      "Epoch 288: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 510014.3438 - mean_squared_error: 510014.3438 - val_loss: 1309426.2500 - val_mean_squared_error: 1309426.2500\n",
      "Epoch 289/1000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 509742.0625 - mean_squared_error: 509742.0625\n",
      "Epoch 289: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 509653.7500 - mean_squared_error: 509653.7500 - val_loss: 1241287.8750 - val_mean_squared_error: 1241287.8750\n",
      "Epoch 290/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 497616.4688 - mean_squared_error: 497616.4688\n",
      "Epoch 290: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 497616.4688 - mean_squared_error: 497616.4688 - val_loss: 1249564.8750 - val_mean_squared_error: 1249564.8750\n",
      "Epoch 291/1000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 514512.3438 - mean_squared_error: 514512.3438\n",
      "Epoch 291: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 514234.3438 - mean_squared_error: 514234.3438 - val_loss: 1214670.5000 - val_mean_squared_error: 1214670.5000\n",
      "Epoch 292/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 510202.5625 - mean_squared_error: 510202.5625\n",
      "Epoch 292: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 509848.4375 - mean_squared_error: 509848.4375 - val_loss: 1283408.0000 - val_mean_squared_error: 1283408.0000\n",
      "Epoch 293/1000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 508821.2500 - mean_squared_error: 508821.2500\n",
      "Epoch 293: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 508631.8125 - mean_squared_error: 508631.8125 - val_loss: 1260749.8750 - val_mean_squared_error: 1260749.8750\n",
      "Epoch 294/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 504289.4375 - mean_squared_error: 504289.4375\n",
      "Epoch 294: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 504289.4375 - mean_squared_error: 504289.4375 - val_loss: 1244354.8750 - val_mean_squared_error: 1244354.8750\n",
      "Epoch 295/1000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 511063.8750 - mean_squared_error: 511063.8750\n",
      "Epoch 295: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 511947.3125 - mean_squared_error: 511947.3125 - val_loss: 1263114.3750 - val_mean_squared_error: 1263114.3750\n",
      "Epoch 296/1000\n",
      "879/879 [==============================] - ETA: 0s - loss: 512070.9062 - mean_squared_error: 512070.9062\n",
      "Epoch 296: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 512070.9062 - mean_squared_error: 512070.9062 - val_loss: 1255620.1250 - val_mean_squared_error: 1255620.1250\n",
      "Epoch 297/1000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 491763.7188 - mean_squared_error: 491763.7188\n",
      "Epoch 297: val_loss did not improve from 1212212.12500\n",
      "879/879 [==============================] - 7s 8ms/step - loss: 492484.5625 - mean_squared_error: 492484.5625 - val_loss: 1263383.6250 - val_mean_squared_error: 1263383.6250\n",
      "Epoch 297: early stopping\n"
     ]
    }
   ],
   "source": [
    "cost_model = keras.Sequential([\n",
    "    layers.Input(shape=(cost_x_train.shape[-1],)),  # Input layer with input_dim features\n",
    "    layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n",
    "    layers.Dense(64, activation='relu'),   # Additional hidden layer\n",
    "    layers.Dense(1, activation='relu')\n",
    "])\n",
    "\n",
    "# cost_model = keras.Sequential([\n",
    "#     layers.Input(shape=(clm_x_train.shape[-1],)),\n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dropout(0.25),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dropout(0.25),\n",
    "#     layers.Dense(32, activation='relu'),\n",
    "#     layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 32\n",
    "early_stop = EarlyStopping(monitor='val_loss', verbose=1, patience=int(n_epochs * 0.1))\n",
    "model_checkpoint = ModelCheckpoint('./models/dnn_cost', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "cost_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "history = cost_model.fit(\n",
    "    cost_x_train, cost_y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=(cost_x_test, cost_y_test),\n",
    "    callbacks=[early_stop, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 20:12:11.698900: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879/879 [==============================] - ETA: 0s - loss: 272.8740 - mean_squared_error: 12081201.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 20:12:24.987289: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 129.04962, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 17s 17ms/step - loss: 272.8740 - mean_squared_error: 12081201.0000 - val_loss: 129.0496 - val_mean_squared_error: 11822613.0000\n",
      "Epoch 2/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 125.5422 - mean_squared_error: 12081873.0000\n",
      "Epoch 2: val_loss improved from 129.04962 to 121.63493, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 125.5363 - mean_squared_error: 12079937.0000 - val_loss: 121.6349 - val_mean_squared_error: 11821944.0000\n",
      "Epoch 3/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 118.2397 - mean_squared_error: 12081945.0000\n",
      "Epoch 3: val_loss improved from 121.63493 to 115.28067, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 118.2307 - mean_squared_error: 12079425.0000 - val_loss: 115.2807 - val_mean_squared_error: 11821255.0000\n",
      "Epoch 4/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 112.0035 - mean_squared_error: 12086321.0000\n",
      "Epoch 4: val_loss improved from 115.28067 to 110.30383, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 111.9961 - mean_squared_error: 12078924.0000 - val_loss: 110.3038 - val_mean_squared_error: 11821048.0000\n",
      "Epoch 5/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 107.0282 - mean_squared_error: 12078459.0000\n",
      "Epoch 5: val_loss improved from 110.30383 to 105.76453, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 107.0654 - mean_squared_error: 12078557.0000 - val_loss: 105.7645 - val_mean_squared_error: 11820644.0000\n",
      "Epoch 6/5000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 102.8965 - mean_squared_error: 12068763.0000\n",
      "Epoch 6: val_loss improved from 105.76453 to 103.08184, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 102.9058 - mean_squared_error: 12078262.0000 - val_loss: 103.0818 - val_mean_squared_error: 11820084.0000\n",
      "Epoch 7/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 100.0685 - mean_squared_error: 12083193.0000\n",
      "Epoch 7: val_loss improved from 103.08184 to 102.19405, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 100.0293 - mean_squared_error: 12078042.0000 - val_loss: 102.1940 - val_mean_squared_error: 11820555.0000\n",
      "Epoch 8/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 99.8360 - mean_squared_error: 12080491.0000\n",
      "Epoch 8: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 99.8269 - mean_squared_error: 12077928.0000 - val_loss: 120.7840 - val_mean_squared_error: 11820468.0000\n",
      "Epoch 9/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 104.6168 - mean_squared_error: 12077951.0000\n",
      "Epoch 9: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 104.6168 - mean_squared_error: 12077951.0000 - val_loss: 107.6556 - val_mean_squared_error: 11820500.0000\n",
      "Epoch 10/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 101.9603 - mean_squared_error: 12068779.0000\n",
      "Epoch 10: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 101.9843 - mean_squared_error: 12077784.0000 - val_loss: 106.6636 - val_mean_squared_error: 11819995.0000\n",
      "Epoch 11/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 105.2162 - mean_squared_error: 11915388.0000\n",
      "Epoch 11: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 105.3326 - mean_squared_error: 12077823.0000 - val_loss: 110.8568 - val_mean_squared_error: 11819831.0000\n",
      "Epoch 12/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 101.2703 - mean_squared_error: 12077730.0000\n",
      "Epoch 12: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 101.2703 - mean_squared_error: 12077730.0000 - val_loss: 105.9371 - val_mean_squared_error: 11820602.0000\n",
      "Epoch 13/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 99.4843 - mean_squared_error: 12077635.0000\n",
      "Epoch 13: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 99.4843 - mean_squared_error: 12077635.0000 - val_loss: 104.6347 - val_mean_squared_error: 11819974.0000\n",
      "Epoch 14/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 101.9151 - mean_squared_error: 12089124.0000\n",
      "Epoch 14: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 101.8746 - mean_squared_error: 12077669.0000 - val_loss: 103.2807 - val_mean_squared_error: 11819917.0000\n",
      "Epoch 15/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 98.5103 - mean_squared_error: 12078548.0000\n",
      "Epoch 15: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 98.5222 - mean_squared_error: 12077581.0000 - val_loss: 104.2764 - val_mean_squared_error: 11820403.0000\n",
      "Epoch 16/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 99.0941 - mean_squared_error: 12065244.0000\n",
      "Epoch 16: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 99.1225 - mean_squared_error: 12077590.0000 - val_loss: 102.6914 - val_mean_squared_error: 11819892.0000\n",
      "Epoch 17/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 100.4579 - mean_squared_error: 12088437.0000\n",
      "Epoch 17: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 100.4243 - mean_squared_error: 12077618.0000 - val_loss: 106.4953 - val_mean_squared_error: 11820067.0000\n",
      "Epoch 18/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 98.8530 - mean_squared_error: 12077531.0000\n",
      "Epoch 18: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 98.8530 - mean_squared_error: 12077531.0000 - val_loss: 104.4992 - val_mean_squared_error: 11820430.0000\n",
      "Epoch 19/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 114.0633 - mean_squared_error: 12077775.0000\n",
      "Epoch 19: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 114.0633 - mean_squared_error: 12077775.0000 - val_loss: 104.7532 - val_mean_squared_error: 11820059.0000\n",
      "Epoch 20/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 96.8342 - mean_squared_error: 12077502.0000\n",
      "Epoch 20: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 96.8342 - mean_squared_error: 12077502.0000 - val_loss: 102.7509 - val_mean_squared_error: 11820009.0000\n",
      "Epoch 21/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 96.5562 - mean_squared_error: 12076479.0000\n",
      "Epoch 21: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 96.5412 - mean_squared_error: 12077481.0000 - val_loss: 103.5251 - val_mean_squared_error: 11819998.0000\n",
      "Epoch 22/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 96.6785 - mean_squared_error: 11945468.0000\n",
      "Epoch 22: val_loss did not improve from 102.19405\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 96.6451 - mean_squared_error: 12077472.0000 - val_loss: 106.5726 - val_mean_squared_error: 11820875.0000\n",
      "Epoch 23/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 95.8379 - mean_squared_error: 12076092.0000\n",
      "Epoch 23: val_loss improved from 102.19405 to 102.12104, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 95.8227 - mean_squared_error: 12077447.0000 - val_loss: 102.1210 - val_mean_squared_error: 11820199.0000\n",
      "Epoch 24/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 95.9008 - mean_squared_error: 12078277.0000\n",
      "Epoch 24: val_loss did not improve from 102.12104\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 95.9249 - mean_squared_error: 12077444.0000 - val_loss: 103.2522 - val_mean_squared_error: 11819811.0000\n",
      "Epoch 25/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 97.2287 - mean_squared_error: 12073610.0000\n",
      "Epoch 25: val_loss did not improve from 102.12104\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 97.2306 - mean_squared_error: 12077471.0000 - val_loss: 103.1626 - val_mean_squared_error: 11820185.0000\n",
      "Epoch 26/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 96.6871 - mean_squared_error: 12074334.0000\n",
      "Epoch 26: val_loss did not improve from 102.12104\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 96.6557 - mean_squared_error: 12077457.0000 - val_loss: 102.8998 - val_mean_squared_error: 11819778.0000\n",
      "Epoch 27/5000\n",
      "872/879 [============================>.] - ETA: 0s - loss: 96.4659 - mean_squared_error: 12137028.0000\n",
      "Epoch 27: val_loss did not improve from 102.12104\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 96.3716 - mean_squared_error: 12077435.0000 - val_loss: 104.1959 - val_mean_squared_error: 11819553.0000\n",
      "Epoch 28/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 94.1820 - mean_squared_error: 12087997.0000\n",
      "Epoch 28: val_loss improved from 102.12104 to 99.58022, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 94.1356 - mean_squared_error: 12077424.0000 - val_loss: 99.5802 - val_mean_squared_error: 11819485.0000\n",
      "Epoch 29/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 93.8027 - mean_squared_error: 12085299.0000\n",
      "Epoch 29: val_loss improved from 99.58022 to 99.48922, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 93.7935 - mean_squared_error: 12077408.0000 - val_loss: 99.4892 - val_mean_squared_error: 11819849.0000\n",
      "Epoch 30/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 94.0437 - mean_squared_error: 12077428.0000\n",
      "Epoch 30: val_loss did not improve from 99.48922\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 94.0437 - mean_squared_error: 12077428.0000 - val_loss: 99.6179 - val_mean_squared_error: 11819726.0000\n",
      "Epoch 31/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 93.4533 - mean_squared_error: 12079147.0000\n",
      "Epoch 31: val_loss did not improve from 99.48922\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 93.4531 - mean_squared_error: 12077400.0000 - val_loss: 101.6527 - val_mean_squared_error: 11819782.0000\n",
      "Epoch 32/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 93.9803 - mean_squared_error: 12090533.0000\n",
      "Epoch 32: val_loss did not improve from 99.48922\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 94.0061 - mean_squared_error: 12077401.0000 - val_loss: 100.0695 - val_mean_squared_error: 11819432.0000\n",
      "Epoch 33/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 94.1477 - mean_squared_error: 12090258.0000\n",
      "Epoch 33: val_loss did not improve from 99.48922\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 94.1095 - mean_squared_error: 12077414.0000 - val_loss: 100.1324 - val_mean_squared_error: 11819469.0000\n",
      "Epoch 34/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 92.5974 - mean_squared_error: 12044669.0000\n",
      "Epoch 34: val_loss improved from 99.48922 to 99.18927, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 92.5994 - mean_squared_error: 12077349.0000 - val_loss: 99.1893 - val_mean_squared_error: 11819487.0000\n",
      "Epoch 35/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 93.6030 - mean_squared_error: 12091986.0000\n",
      "Epoch 35: val_loss improved from 99.18927 to 99.04529, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 93.5715 - mean_squared_error: 12077401.0000 - val_loss: 99.0453 - val_mean_squared_error: 11819467.0000\n",
      "Epoch 36/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 94.0170 - mean_squared_error: 12077387.0000\n",
      "Epoch 36: val_loss did not improve from 99.04529\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 94.0170 - mean_squared_error: 12077387.0000 - val_loss: 100.0269 - val_mean_squared_error: 11819749.0000\n",
      "Epoch 37/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 93.9023 - mean_squared_error: 12072859.0000\n",
      "Epoch 37: val_loss did not improve from 99.04529\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 93.9059 - mean_squared_error: 12077391.0000 - val_loss: 99.4634 - val_mean_squared_error: 11819754.0000\n",
      "Epoch 38/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 93.4359 - mean_squared_error: 12079953.0000\n",
      "Epoch 38: val_loss improved from 99.04529 to 98.60407, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 93.4173 - mean_squared_error: 12077374.0000 - val_loss: 98.6041 - val_mean_squared_error: 11819933.0000\n",
      "Epoch 39/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 94.1521 - mean_squared_error: 12077386.0000\n",
      "Epoch 39: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 94.1521 - mean_squared_error: 12077386.0000 - val_loss: 102.3129 - val_mean_squared_error: 11819267.0000\n",
      "Epoch 40/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 94.0293 - mean_squared_error: 12098458.0000\n",
      "Epoch 40: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 93.9764 - mean_squared_error: 12077368.0000 - val_loss: 99.3469 - val_mean_squared_error: 11820011.0000\n",
      "Epoch 41/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 93.9933 - mean_squared_error: 12090311.0000\n",
      "Epoch 41: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 93.9847 - mean_squared_error: 12077369.0000 - val_loss: 99.5601 - val_mean_squared_error: 11819814.0000\n",
      "Epoch 42/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 94.1029 - mean_squared_error: 12079885.0000\n",
      "Epoch 42: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 94.0945 - mean_squared_error: 12077379.0000 - val_loss: 100.6631 - val_mean_squared_error: 11820170.0000\n",
      "Epoch 43/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 93.9365 - mean_squared_error: 12076836.0000\n",
      "Epoch 43: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 93.8662 - mean_squared_error: 12077372.0000 - val_loss: 99.6228 - val_mean_squared_error: 11820321.0000\n",
      "Epoch 44/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 93.9952 - mean_squared_error: 12079785.0000\n",
      "Epoch 44: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 93.9868 - mean_squared_error: 12077376.0000 - val_loss: 99.3088 - val_mean_squared_error: 11819524.0000\n",
      "Epoch 45/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 93.7677 - mean_squared_error: 12087579.0000\n",
      "Epoch 45: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 93.6813 - mean_squared_error: 12077355.0000 - val_loss: 100.3383 - val_mean_squared_error: 11819736.0000\n",
      "Epoch 46/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 93.5814 - mean_squared_error: 12050060.0000\n",
      "Epoch 46: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 93.5900 - mean_squared_error: 12077360.0000 - val_loss: 99.0836 - val_mean_squared_error: 11819592.0000\n",
      "Epoch 47/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 93.4571 - mean_squared_error: 12092455.0000\n",
      "Epoch 47: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 93.4213 - mean_squared_error: 12077363.0000 - val_loss: 99.3285 - val_mean_squared_error: 11820152.0000\n",
      "Epoch 48/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 93.4364 - mean_squared_error: 12081832.0000\n",
      "Epoch 48: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 93.4666 - mean_squared_error: 12077358.0000 - val_loss: 99.9212 - val_mean_squared_error: 11819655.0000\n",
      "Epoch 49/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 93.0450 - mean_squared_error: 12079850.0000\n",
      "Epoch 49: val_loss did not improve from 98.60407\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 93.0332 - mean_squared_error: 12077335.0000 - val_loss: 101.3706 - val_mean_squared_error: 11819878.0000\n",
      "Epoch 50/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 93.4248 - mean_squared_error: 12086642.0000\n",
      "Epoch 50: val_loss improved from 98.60407 to 88.57833, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 93.3921 - mean_squared_error: 12077423.0000 - val_loss: 88.5783 - val_mean_squared_error: 11819334.0000\n",
      "Epoch 51/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 84.6920 - mean_squared_error: 12076782.0000\n",
      "Epoch 51: val_loss improved from 88.57833 to 87.14508, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.6859 - mean_squared_error: 12077260.0000 - val_loss: 87.1451 - val_mean_squared_error: 11819859.0000\n",
      "Epoch 52/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 84.8208 - mean_squared_error: 12063027.0000\n",
      "Epoch 52: val_loss did not improve from 87.14508\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.8508 - mean_squared_error: 12077268.0000 - val_loss: 87.7254 - val_mean_squared_error: 11819613.0000\n",
      "Epoch 53/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 85.1432 - mean_squared_error: 12077270.0000\n",
      "Epoch 53: val_loss did not improve from 87.14508\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 85.1432 - mean_squared_error: 12077270.0000 - val_loss: 87.5776 - val_mean_squared_error: 11819515.0000\n",
      "Epoch 54/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.9706 - mean_squared_error: 12077269.0000\n",
      "Epoch 54: val_loss did not improve from 87.14508\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 84.9706 - mean_squared_error: 12077269.0000 - val_loss: 89.0137 - val_mean_squared_error: 11819960.0000\n",
      "Epoch 55/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 85.3042 - mean_squared_error: 12080900.0000\n",
      "Epoch 55: val_loss did not improve from 87.14508\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 85.3002 - mean_squared_error: 12077272.0000 - val_loss: 87.1613 - val_mean_squared_error: 11819618.0000\n",
      "Epoch 56/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 85.1258 - mean_squared_error: 12079722.0000\n",
      "Epoch 56: val_loss improved from 87.14508 to 87.07457, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 85.1204 - mean_squared_error: 12077279.0000 - val_loss: 87.0746 - val_mean_squared_error: 11819505.0000\n",
      "Epoch 57/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 85.3837 - mean_squared_error: 12075682.0000\n",
      "Epoch 57: val_loss did not improve from 87.07457\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 85.4003 - mean_squared_error: 12077285.0000 - val_loss: 89.0052 - val_mean_squared_error: 11819687.0000\n",
      "Epoch 58/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.9340 - mean_squared_error: 12077263.0000\n",
      "Epoch 58: val_loss improved from 87.07457 to 86.79067, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.9340 - mean_squared_error: 12077263.0000 - val_loss: 86.7907 - val_mean_squared_error: 11819702.0000\n",
      "Epoch 59/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 84.7194 - mean_squared_error: 12087292.0000\n",
      "Epoch 59: val_loss did not improve from 86.79067\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.7049 - mean_squared_error: 12077265.0000 - val_loss: 87.0193 - val_mean_squared_error: 11819770.0000\n",
      "Epoch 60/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 84.6431 - mean_squared_error: 11979600.0000\n",
      "Epoch 60: val_loss did not improve from 86.79067\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 84.6975 - mean_squared_error: 12077256.0000 - val_loss: 88.4826 - val_mean_squared_error: 11819211.0000\n",
      "Epoch 61/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 86.0046 - mean_squared_error: 12073845.0000\n",
      "Epoch 61: val_loss did not improve from 86.79067\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 86.0298 - mean_squared_error: 12077283.0000 - val_loss: 86.8272 - val_mean_squared_error: 11819874.0000\n",
      "Epoch 62/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 84.3659 - mean_squared_error: 12079355.0000\n",
      "Epoch 62: val_loss did not improve from 86.79067\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.3596 - mean_squared_error: 12077254.0000 - val_loss: 87.3079 - val_mean_squared_error: 11819471.0000\n",
      "Epoch 63/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 84.8738 - mean_squared_error: 12077283.0000\n",
      "Epoch 63: val_loss did not improve from 86.79067\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 84.8763 - mean_squared_error: 12077268.0000 - val_loss: 87.2140 - val_mean_squared_error: 11819670.0000\n",
      "Epoch 64/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 84.5669 - mean_squared_error: 12088793.0000\n",
      "Epoch 64: val_loss did not improve from 86.79067\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.5586 - mean_squared_error: 12077254.0000 - val_loss: 87.8219 - val_mean_squared_error: 11819943.0000\n",
      "Epoch 65/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 85.3029 - mean_squared_error: 12077299.0000\n",
      "Epoch 65: val_loss improved from 86.79067 to 86.47850, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 85.3029 - mean_squared_error: 12077299.0000 - val_loss: 86.4785 - val_mean_squared_error: 11819659.0000\n",
      "Epoch 66/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 84.1129 - mean_squared_error: 12079352.0000\n",
      "Epoch 66: val_loss did not improve from 86.47850\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.1182 - mean_squared_error: 12077248.0000 - val_loss: 87.2236 - val_mean_squared_error: 11819295.0000\n",
      "Epoch 67/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.8462 - mean_squared_error: 12077265.0000\n",
      "Epoch 67: val_loss did not improve from 86.47850\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.8462 - mean_squared_error: 12077265.0000 - val_loss: 87.3501 - val_mean_squared_error: 11819738.0000\n",
      "Epoch 68/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 85.2919 - mean_squared_error: 12084124.0000\n",
      "Epoch 68: val_loss did not improve from 86.47850\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 85.4155 - mean_squared_error: 12077270.0000 - val_loss: 88.0997 - val_mean_squared_error: 11820002.0000\n",
      "Epoch 69/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 84.0408 - mean_squared_error: 12079762.0000\n",
      "Epoch 69: val_loss did not improve from 86.47850\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 84.0346 - mean_squared_error: 12077246.0000 - val_loss: 86.5558 - val_mean_squared_error: 11819536.0000\n",
      "Epoch 70/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 84.3299 - mean_squared_error: 12079836.0000\n",
      "Epoch 70: val_loss did not improve from 86.47850\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.3124 - mean_squared_error: 12077257.0000 - val_loss: 87.4520 - val_mean_squared_error: 11819185.0000\n",
      "Epoch 71/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 444.0436 - mean_squared_error: 12079979.0000\n",
      "Epoch 71: val_loss did not improve from 86.47850\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 443.9544 - mean_squared_error: 12077408.0000 - val_loss: 87.7715 - val_mean_squared_error: 11819594.0000\n",
      "Epoch 72/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.0026 - mean_squared_error: 12077223.0000\n",
      "Epoch 72: val_loss improved from 86.47850 to 86.33880, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 14s 15ms/step - loss: 84.0026 - mean_squared_error: 12077223.0000 - val_loss: 86.3388 - val_mean_squared_error: 11819716.0000\n",
      "Epoch 73/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.5391 - mean_squared_error: 12056987.0000\n",
      "Epoch 73: val_loss did not improve from 86.33880\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.5637 - mean_squared_error: 12077214.0000 - val_loss: 86.4803 - val_mean_squared_error: 11819667.0000\n",
      "Epoch 74/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 83.6448 - mean_squared_error: 12095866.0000\n",
      "Epoch 74: val_loss improved from 86.33880 to 86.04909, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.6108 - mean_squared_error: 12077226.0000 - val_loss: 86.0491 - val_mean_squared_error: 11819596.0000\n",
      "Epoch 75/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.5574 - mean_squared_error: 12086924.0000\n",
      "Epoch 75: val_loss did not improve from 86.04909\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 83.6217 - mean_squared_error: 12077229.0000 - val_loss: 86.8279 - val_mean_squared_error: 11819519.0000\n",
      "Epoch 76/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.4969 - mean_squared_error: 12087095.0000\n",
      "Epoch 76: val_loss improved from 86.04909 to 85.83994, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.5003 - mean_squared_error: 12077226.0000 - val_loss: 85.8399 - val_mean_squared_error: 11819653.0000\n",
      "Epoch 77/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.6541 - mean_squared_error: 12077220.0000\n",
      "Epoch 77: val_loss did not improve from 85.83994\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.6541 - mean_squared_error: 12077220.0000 - val_loss: 86.2569 - val_mean_squared_error: 11819617.0000\n",
      "Epoch 78/5000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 83.3408 - mean_squared_error: 12109670.0000\n",
      "Epoch 78: val_loss did not improve from 85.83994\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.3955 - mean_squared_error: 12077214.0000 - val_loss: 86.0749 - val_mean_squared_error: 11819579.0000\n",
      "Epoch 79/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 83.6528 - mean_squared_error: 12068518.0000\n",
      "Epoch 79: val_loss improved from 85.83994 to 85.73042, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.6236 - mean_squared_error: 12077235.0000 - val_loss: 85.7304 - val_mean_squared_error: 11819727.0000\n",
      "Epoch 80/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.4463 - mean_squared_error: 12064202.0000\n",
      "Epoch 80: val_loss did not improve from 85.73042\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 83.4666 - mean_squared_error: 12077227.0000 - val_loss: 85.9055 - val_mean_squared_error: 11819433.0000\n",
      "Epoch 81/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 86.2745 - mean_squared_error: 12104228.0000\n",
      "Epoch 81: val_loss did not improve from 85.73042\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 86.1971 - mean_squared_error: 12077311.0000 - val_loss: 87.5970 - val_mean_squared_error: 11819700.0000\n",
      "Epoch 82/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 83.5769 - mean_squared_error: 12099352.0000\n",
      "Epoch 82: val_loss improved from 85.73042 to 85.42120, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.5566 - mean_squared_error: 12077225.0000 - val_loss: 85.4212 - val_mean_squared_error: 11819749.0000\n",
      "Epoch 83/5000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 83.5141 - mean_squared_error: 12126033.0000\n",
      "Epoch 83: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 83.4731 - mean_squared_error: 12077232.0000 - val_loss: 85.9912 - val_mean_squared_error: 11819755.0000\n",
      "Epoch 84/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.9545 - mean_squared_error: 12077249.0000\n",
      "Epoch 84: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 83.9545 - mean_squared_error: 12077249.0000 - val_loss: 86.4629 - val_mean_squared_error: 11819586.0000\n",
      "Epoch 85/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.7460 - mean_squared_error: 12077230.0000\n",
      "Epoch 85: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.7460 - mean_squared_error: 12077230.0000 - val_loss: 86.4153 - val_mean_squared_error: 11819716.0000\n",
      "Epoch 86/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 84.3220 - mean_squared_error: 12038808.0000\n",
      "Epoch 86: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 84.3618 - mean_squared_error: 12077247.0000 - val_loss: 87.4773 - val_mean_squared_error: 11819573.0000\n",
      "Epoch 87/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.0056 - mean_squared_error: 12077241.0000\n",
      "Epoch 87: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.0056 - mean_squared_error: 12077241.0000 - val_loss: 86.3241 - val_mean_squared_error: 11819597.0000\n",
      "Epoch 88/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.2973 - mean_squared_error: 12077244.0000\n",
      "Epoch 88: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 84.2973 - mean_squared_error: 12077244.0000 - val_loss: 86.3272 - val_mean_squared_error: 11819665.0000\n",
      "Epoch 89/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 84.2754 - mean_squared_error: 12081336.0000\n",
      "Epoch 89: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 11s 13ms/step - loss: 84.2601 - mean_squared_error: 12077263.0000 - val_loss: 86.3228 - val_mean_squared_error: 11819455.0000\n",
      "Epoch 90/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.6548 - mean_squared_error: 12077219.0000\n",
      "Epoch 90: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.6548 - mean_squared_error: 12077219.0000 - val_loss: 86.0591 - val_mean_squared_error: 11819685.0000\n",
      "Epoch 91/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 84.7207 - mean_squared_error: 12119510.0000\n",
      "Epoch 91: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.6297 - mean_squared_error: 12077274.0000 - val_loss: 85.8346 - val_mean_squared_error: 11819634.0000\n",
      "Epoch 92/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.9294 - mean_squared_error: 12087806.0000\n",
      "Epoch 92: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.9466 - mean_squared_error: 12077249.0000 - val_loss: 87.5011 - val_mean_squared_error: 11819962.0000\n",
      "Epoch 93/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 84.0550 - mean_squared_error: 12089402.0000\n",
      "Epoch 93: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.0614 - mean_squared_error: 12077260.0000 - val_loss: 85.7869 - val_mean_squared_error: 11819895.0000\n",
      "Epoch 94/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.9623 - mean_squared_error: 12064396.0000\n",
      "Epoch 94: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.9967 - mean_squared_error: 12077249.0000 - val_loss: 86.4159 - val_mean_squared_error: 11819614.0000\n",
      "Epoch 95/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 84.4579 - mean_squared_error: 12090441.0000\n",
      "Epoch 95: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 84.5161 - mean_squared_error: 12077260.0000 - val_loss: 86.3629 - val_mean_squared_error: 11819674.0000\n",
      "Epoch 96/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.5659 - mean_squared_error: 12079497.0000\n",
      "Epoch 96: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.5676 - mean_squared_error: 12077222.0000 - val_loss: 85.8565 - val_mean_squared_error: 11819797.0000\n",
      "Epoch 97/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.1209 - mean_squared_error: 12077256.0000\n",
      "Epoch 97: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 84.1209 - mean_squared_error: 12077256.0000 - val_loss: 87.2295 - val_mean_squared_error: 11819215.0000\n",
      "Epoch 98/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 83.7912 - mean_squared_error: 12056736.0000\n",
      "Epoch 98: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.8250 - mean_squared_error: 12077229.0000 - val_loss: 86.4175 - val_mean_squared_error: 11819367.0000\n",
      "Epoch 99/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 84.0431 - mean_squared_error: 12089094.0000\n",
      "Epoch 99: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 84.0172 - mean_squared_error: 12077249.0000 - val_loss: 86.4211 - val_mean_squared_error: 11819453.0000\n",
      "Epoch 100/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 83.8610 - mean_squared_error: 12079826.0000\n",
      "Epoch 100: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.9148 - mean_squared_error: 12077256.0000 - val_loss: 86.3690 - val_mean_squared_error: 11819840.0000\n",
      "Epoch 101/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.8005 - mean_squared_error: 12087032.0000\n",
      "Epoch 101: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.7672 - mean_squared_error: 12077242.0000 - val_loss: 86.6444 - val_mean_squared_error: 11819097.0000\n",
      "Epoch 102/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 83.5467 - mean_squared_error: 12071664.0000\n",
      "Epoch 102: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.5672 - mean_squared_error: 12077234.0000 - val_loss: 85.9152 - val_mean_squared_error: 11819822.0000\n",
      "Epoch 103/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.0754 - mean_squared_error: 12077260.0000\n",
      "Epoch 103: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.0754 - mean_squared_error: 12077260.0000 - val_loss: 87.1991 - val_mean_squared_error: 11819396.0000\n",
      "Epoch 104/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 83.9861 - mean_squared_error: 12091883.0000\n",
      "Epoch 104: val_loss did not improve from 85.42120\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 83.9518 - mean_squared_error: 12077254.0000 - val_loss: 86.3979 - val_mean_squared_error: 11819629.0000\n",
      "Epoch 105/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.5188 - mean_squared_error: 12079721.0000\n",
      "Epoch 105: val_loss improved from 85.42120 to 85.11552, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 83.5149 - mean_squared_error: 12077220.0000 - val_loss: 85.1155 - val_mean_squared_error: 11819748.0000\n",
      "Epoch 106/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 109.6530 - mean_squared_error: 12090008.0000\n",
      "Epoch 106: val_loss did not improve from 85.11552\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 109.5488 - mean_squared_error: 12077563.0000 - val_loss: 88.1382 - val_mean_squared_error: 11819622.0000\n",
      "Epoch 107/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.4857 - mean_squared_error: 12077256.0000\n",
      "Epoch 107: val_loss did not improve from 85.11552\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 84.4857 - mean_squared_error: 12077256.0000 - val_loss: 86.0431 - val_mean_squared_error: 11819602.0000\n",
      "Epoch 108/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.4972 - mean_squared_error: 12078552.0000\n",
      "Epoch 108: val_loss did not improve from 85.11552\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.4986 - mean_squared_error: 12077221.0000 - val_loss: 85.5499 - val_mean_squared_error: 11819499.0000\n",
      "Epoch 109/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.2974 - mean_squared_error: 12079764.0000\n",
      "Epoch 109: val_loss did not improve from 85.11552\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.2876 - mean_squared_error: 12077212.0000 - val_loss: 85.4299 - val_mean_squared_error: 11819672.0000\n",
      "Epoch 110/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.3788 - mean_squared_error: 12081666.0000\n",
      "Epoch 110: val_loss did not improve from 85.11552\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.3890 - mean_squared_error: 12077221.0000 - val_loss: 85.3763 - val_mean_squared_error: 11819632.0000\n",
      "Epoch 111/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 83.1821 - mean_squared_error: 12081222.0000\n",
      "Epoch 111: val_loss did not improve from 85.11552\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.2522 - mean_squared_error: 12077226.0000 - val_loss: 85.5453 - val_mean_squared_error: 11819565.0000\n",
      "Epoch 112/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 83.2656 - mean_squared_error: 12085179.0000\n",
      "Epoch 112: val_loss improved from 85.11552 to 85.08859, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 83.2682 - mean_squared_error: 12077220.0000 - val_loss: 85.0886 - val_mean_squared_error: 11819625.0000\n",
      "Epoch 113/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.3186 - mean_squared_error: 12081816.0000\n",
      "Epoch 113: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.3479 - mean_squared_error: 12077228.0000 - val_loss: 85.6310 - val_mean_squared_error: 11819566.0000\n",
      "Epoch 114/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.5839 - mean_squared_error: 12077237.0000\n",
      "Epoch 114: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.5839 - mean_squared_error: 12077237.0000 - val_loss: 85.6515 - val_mean_squared_error: 11819483.0000\n",
      "Epoch 115/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.4785 - mean_squared_error: 12071525.0000\n",
      "Epoch 115: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 83.4986 - mean_squared_error: 12077237.0000 - val_loss: 86.1217 - val_mean_squared_error: 11819504.0000\n",
      "Epoch 116/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 83.8463 - mean_squared_error: 12099591.0000\n",
      "Epoch 116: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.8404 - mean_squared_error: 12077256.0000 - val_loss: 86.2421 - val_mean_squared_error: 11819910.0000\n",
      "Epoch 117/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 83.3810 - mean_squared_error: 12111565.0000\n",
      "Epoch 117: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.4266 - mean_squared_error: 12077227.0000 - val_loss: 86.1176 - val_mean_squared_error: 11819950.0000\n",
      "Epoch 118/5000\n",
      "873/879 [============================>.] - ETA: 0s - loss: 83.5522 - mean_squared_error: 12091346.0000\n",
      "Epoch 118: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.4972 - mean_squared_error: 12077232.0000 - val_loss: 85.6662 - val_mean_squared_error: 11819719.0000\n",
      "Epoch 119/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.6987 - mean_squared_error: 12079775.0000\n",
      "Epoch 119: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 83.6914 - mean_squared_error: 12077249.0000 - val_loss: 85.9962 - val_mean_squared_error: 11819624.0000\n",
      "Epoch 120/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.6958 - mean_squared_error: 12079530.0000\n",
      "Epoch 120: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 83.6933 - mean_squared_error: 12077241.0000 - val_loss: 86.8869 - val_mean_squared_error: 11819414.0000\n",
      "Epoch 121/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.5672 - mean_squared_error: 12077240.0000\n",
      "Epoch 121: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.5672 - mean_squared_error: 12077240.0000 - val_loss: 85.4635 - val_mean_squared_error: 11819553.0000\n",
      "Epoch 122/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.8786 - mean_squared_error: 12076277.0000\n",
      "Epoch 122: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.8681 - mean_squared_error: 12077246.0000 - val_loss: 86.7374 - val_mean_squared_error: 11819257.0000\n",
      "Epoch 123/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.4805 - mean_squared_error: 12078664.0000\n",
      "Epoch 123: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.4880 - mean_squared_error: 12077232.0000 - val_loss: 85.4642 - val_mean_squared_error: 11819618.0000\n",
      "Epoch 124/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.4474 - mean_squared_error: 12077228.0000\n",
      "Epoch 124: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.4474 - mean_squared_error: 12077228.0000 - val_loss: 86.0490 - val_mean_squared_error: 11819590.0000\n",
      "Epoch 125/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 84.6801 - mean_squared_error: 12077262.0000\n",
      "Epoch 125: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.6801 - mean_squared_error: 12077262.0000 - val_loss: 87.3518 - val_mean_squared_error: 11819670.0000\n",
      "Epoch 126/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 85.0260 - mean_squared_error: 12079112.0000\n",
      "Epoch 126: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 85.0227 - mean_squared_error: 12077251.0000 - val_loss: 88.6325 - val_mean_squared_error: 11819429.0000\n",
      "Epoch 127/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 85.0060 - mean_squared_error: 12077256.0000\n",
      "Epoch 127: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 85.0060 - mean_squared_error: 12077256.0000 - val_loss: 87.6880 - val_mean_squared_error: 11819344.0000\n",
      "Epoch 128/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 85.8027 - mean_squared_error: 12077304.0000\n",
      "Epoch 128: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 85.8027 - mean_squared_error: 12077304.0000 - val_loss: 88.5277 - val_mean_squared_error: 11819656.0000\n",
      "Epoch 129/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 84.8612 - mean_squared_error: 12087373.0000\n",
      "Epoch 129: val_loss did not improve from 85.08859\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.8530 - mean_squared_error: 12077247.0000 - val_loss: 86.0330 - val_mean_squared_error: 11819631.0000\n",
      "Epoch 130/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.0868 - mean_squared_error: 12083226.0000\n",
      "Epoch 130: val_loss improved from 85.08859 to 84.98478, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 83.0817 - mean_squared_error: 12077216.0000 - val_loss: 84.9848 - val_mean_squared_error: 11819681.0000\n",
      "Epoch 131/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 82.9701 - mean_squared_error: 12036492.0000\n",
      "Epoch 131: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.0543 - mean_squared_error: 12077215.0000 - val_loss: 85.9131 - val_mean_squared_error: 11819288.0000\n",
      "Epoch 132/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.5013 - mean_squared_error: 12079291.0000\n",
      "Epoch 132: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.4984 - mean_squared_error: 12077235.0000 - val_loss: 85.5741 - val_mean_squared_error: 11819602.0000\n",
      "Epoch 133/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 83.3817 - mean_squared_error: 12101603.0000\n",
      "Epoch 133: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.4174 - mean_squared_error: 12077226.0000 - val_loss: 85.4438 - val_mean_squared_error: 11819772.0000\n",
      "Epoch 134/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.1379 - mean_squared_error: 12077219.0000\n",
      "Epoch 134: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 83.1379 - mean_squared_error: 12077219.0000 - val_loss: 86.2269 - val_mean_squared_error: 11819340.0000\n",
      "Epoch 135/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 84.9792 - mean_squared_error: 12110092.0000\n",
      "Epoch 135: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 84.9201 - mean_squared_error: 12077274.0000 - val_loss: 87.7919 - val_mean_squared_error: 11819503.0000\n",
      "Epoch 136/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.4092 - mean_squared_error: 12077247.0000\n",
      "Epoch 136: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.4092 - mean_squared_error: 12077247.0000 - val_loss: 85.3248 - val_mean_squared_error: 11819561.0000\n",
      "Epoch 137/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 82.9106 - mean_squared_error: 12090210.0000\n",
      "Epoch 137: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 82.9059 - mean_squared_error: 12077219.0000 - val_loss: 85.2433 - val_mean_squared_error: 11819434.0000\n",
      "Epoch 138/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 83.5524 - mean_squared_error: 12075964.0000\n",
      "Epoch 138: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.5572 - mean_squared_error: 12077236.0000 - val_loss: 86.0814 - val_mean_squared_error: 11819646.0000\n",
      "Epoch 139/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.4069 - mean_squared_error: 12091441.0000\n",
      "Epoch 139: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.3730 - mean_squared_error: 12077225.0000 - val_loss: 85.3947 - val_mean_squared_error: 11819684.0000\n",
      "Epoch 140/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 83.0084 - mean_squared_error: 12110159.0000\n",
      "Epoch 140: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 82.9333 - mean_squared_error: 12077213.0000 - val_loss: 85.4156 - val_mean_squared_error: 11819881.0000\n",
      "Epoch 141/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.6529 - mean_squared_error: 12083002.0000\n",
      "Epoch 141: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.6441 - mean_squared_error: 12077242.0000 - val_loss: 85.7729 - val_mean_squared_error: 11819632.0000\n",
      "Epoch 142/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 82.9608 - mean_squared_error: 12099290.0000\n",
      "Epoch 142: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 82.9663 - mean_squared_error: 12077215.0000 - val_loss: 85.4160 - val_mean_squared_error: 11819724.0000\n",
      "Epoch 143/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 83.7725 - mean_squared_error: 12080499.0000\n",
      "Epoch 143: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 83.7779 - mean_squared_error: 12077260.0000 - val_loss: 86.3349 - val_mean_squared_error: 11819575.0000\n",
      "Epoch 144/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.0482 - mean_squared_error: 12090182.0000\n",
      "Epoch 144: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 83.0452 - mean_squared_error: 12077205.0000 - val_loss: 85.4923 - val_mean_squared_error: 11819810.0000\n",
      "Epoch 145/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.2015 - mean_squared_error: 12077289.0000\n",
      "Epoch 145: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.2063 - mean_squared_error: 12077226.0000 - val_loss: 87.7223 - val_mean_squared_error: 11819082.0000\n",
      "Epoch 146/5000\n",
      "875/879 [============================>.] - ETA: 0s - loss: 83.1304 - mean_squared_error: 12059655.0000\n",
      "Epoch 146: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.1616 - mean_squared_error: 12077225.0000 - val_loss: 85.1263 - val_mean_squared_error: 11819337.0000\n",
      "Epoch 147/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 83.2576 - mean_squared_error: 12061681.0000\n",
      "Epoch 147: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.2823 - mean_squared_error: 12077223.0000 - val_loss: 86.0422 - val_mean_squared_error: 11819361.0000\n",
      "Epoch 148/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 83.4218 - mean_squared_error: 12079754.0000\n",
      "Epoch 148: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.4178 - mean_squared_error: 12077227.0000 - val_loss: 86.1155 - val_mean_squared_error: 11819341.0000\n",
      "Epoch 149/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.0014 - mean_squared_error: 12077218.0000\n",
      "Epoch 149: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.0014 - mean_squared_error: 12077218.0000 - val_loss: 85.0078 - val_mean_squared_error: 11819552.0000\n",
      "Epoch 150/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.6966 - mean_squared_error: 12077241.0000\n",
      "Epoch 150: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.6966 - mean_squared_error: 12077241.0000 - val_loss: 86.2057 - val_mean_squared_error: 11819367.0000\n",
      "Epoch 151/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 83.5124 - mean_squared_error: 12047470.0000\n",
      "Epoch 151: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 83.6093 - mean_squared_error: 12077223.0000 - val_loss: 85.2885 - val_mean_squared_error: 11819541.0000\n",
      "Epoch 152/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 82.7316 - mean_squared_error: 12077213.0000\n",
      "Epoch 152: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 82.7316 - mean_squared_error: 12077213.0000 - val_loss: 85.3710 - val_mean_squared_error: 11819696.0000\n",
      "Epoch 153/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.2779 - mean_squared_error: 11972882.0000\n",
      "Epoch 153: val_loss did not improve from 84.98478\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.2664 - mean_squared_error: 12077225.0000 - val_loss: 85.0811 - val_mean_squared_error: 11819726.0000\n",
      "Epoch 154/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 82.8477 - mean_squared_error: 12078004.0000\n",
      "Epoch 154: val_loss improved from 84.98478 to 84.80698, saving model to ./models/dnn_cost_tweedie\n",
      "INFO:tensorflow:Assets written to: ./models/dnn_cost_tweedie/assets\n",
      "879/879 [==============================] - 13s 15ms/step - loss: 82.8618 - mean_squared_error: 12077221.0000 - val_loss: 84.8070 - val_mean_squared_error: 11819692.0000\n",
      "Epoch 155/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.4046 - mean_squared_error: 12077162.0000\n",
      "Epoch 155: val_loss did not improve from 84.80698\n",
      "879/879 [==============================] - 13s 14ms/step - loss: 83.3891 - mean_squared_error: 12077242.0000 - val_loss: 85.6891 - val_mean_squared_error: 11819603.0000\n",
      "Epoch 156/5000\n",
      "874/879 [============================>.] - ETA: 0s - loss: 83.0681 - mean_squared_error: 12114810.0000\n",
      "Epoch 156: val_loss did not improve from 84.80698\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.0123 - mean_squared_error: 12077222.0000 - val_loss: 85.0229 - val_mean_squared_error: 11819676.0000\n",
      "Epoch 157/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.1555 - mean_squared_error: 12077230.0000\n",
      "Epoch 157: val_loss did not improve from 84.80698\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.1555 - mean_squared_error: 12077230.0000 - val_loss: 85.2633 - val_mean_squared_error: 11819684.0000\n",
      "Epoch 158/5000\n",
      "879/879 [==============================] - ETA: 0s - loss: 83.1076 - mean_squared_error: 12077235.0000\n",
      "Epoch 158: val_loss did not improve from 84.80698\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 83.1076 - mean_squared_error: 12077235.0000 - val_loss: 85.5831 - val_mean_squared_error: 11819756.0000\n",
      "Epoch 159/5000\n",
      "877/879 [============================>.] - ETA: 0s - loss: 83.0842 - mean_squared_error: 12076111.0000\n",
      "Epoch 159: val_loss did not improve from 84.80698\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 83.1176 - mean_squared_error: 12077228.0000 - val_loss: 85.7334 - val_mean_squared_error: 11819721.0000\n",
      "Epoch 160/5000\n",
      "876/879 [============================>.] - ETA: 0s - loss: 83.2762 - mean_squared_error: 12091813.0000\n",
      "Epoch 160: val_loss did not improve from 84.80698\n",
      "879/879 [==============================] - 12s 13ms/step - loss: 83.2635 - mean_squared_error: 12077227.0000 - val_loss: 85.4356 - val_mean_squared_error: 11819628.0000\n",
      "Epoch 161/5000\n",
      "878/879 [============================>.] - ETA: 0s - loss: 82.8385 - mean_squared_error: 12078798.0000\n",
      "Epoch 161: val_loss did not improve from 84.80698\n",
      "879/879 [==============================] - 12s 14ms/step - loss: 82.8499 - mean_squared_error: 12077218.0000 - val_loss: 85.0927 - val_mean_squared_error: 11819497.0000\n",
      "Epoch 162/5000\n",
      "855/879 [============================>.] - ETA: 0s - loss: 82.8347 - mean_squared_error: 11942437.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mymac/travelers/main.ipynb Cell 86\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y312sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model_checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\u001b[39m'\u001b[39m\u001b[39m./models/dnn_cost_tweedie\u001b[39m\u001b[39m'\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y312sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m cost_model_tweedie\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39mtweedie_loss, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y312sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m history \u001b[39m=\u001b[39m cost_model_tweedie\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y312sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     cost_x_train, cost_y_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y312sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y312sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49mn_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y312sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     validation_data\u001b[39m=\u001b[39;49m(cost_x_test, cost_y_test),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y312sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stop, model_checkpoint]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y312sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def tweedie_loss(y_true, y_pred, power=1.5):\n",
    "    p = power\n",
    "\n",
    "    a = y_true * tf.exp(y_pred * (1 - p)) / (1 - p)\n",
    "    b = tf.exp(y_pred * (2 - p)) / (2 - p)\n",
    "\n",
    "    loss = -a + b\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "cost_model_tweedie = keras.Sequential([\n",
    "    layers.Input(shape=(cost_x_train.shape[-1],)),  # Input layer with input_dim features\n",
    "    layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n",
    "    layers.Dense(64, activation='relu'),   # Additional hidden layer\n",
    "    layers.Dense(1, activation='relu')\n",
    "])\n",
    "\n",
    "# cost_model = keras.Sequential([\n",
    "#     layers.Input(shape=(clm_x_train.shape[-1],)),\n",
    "#     layers.Dense(256, activation='relu'),\n",
    "#     layers.Dropout(0.25),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.Dropout(0.25),\n",
    "#     layers.Dense(32, activation='relu'),\n",
    "#     layers.Dense(1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "n_epochs = 5000\n",
    "batch_size = 32\n",
    "early_stop = EarlyStopping(monitor='val_loss', verbose=1, patience=int(n_epochs * 0.1))\n",
    "model_checkpoint = ModelCheckpoint('./models/dnn_cost_tweedie', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "cost_model_tweedie.compile(optimizer='adam', loss=tweedie_loss, metrics=['mean_squared_error'])\n",
    "\n",
    "history = cost_model_tweedie.fit(\n",
    "    cost_x_train, cost_y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epochs,\n",
    "    validation_data=(cost_x_test, cost_y_test),\n",
    "    callbacks=[early_stop, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_combos = []\n",
    "\n",
    "# for n_estimators in [5, 10, 15, 20, 25, 50, 100, 200, 300]:\n",
    "#     for min_samples_leaf in [5, 10, 15, 20, 25, 50]:\n",
    "#         for max_depth in [3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "#             for min_samples_split in [2, 3, 4, 5, 10, 15]:\n",
    "#                 for criterion in ['squared_error', 'absolute_error', 'friedman_mse', 'poisson']:\n",
    "#                     all_combos.append((n_estimators, min_samples_leaf, max_depth, min_samples_split, criterion))\n",
    "\n",
    "# percentage_to_try = 0.01\n",
    "# n_runs = int(percentage_to_try * len(all_combos))\n",
    "# combos_to_try = random.sample(all_combos, n_runs)\n",
    "# print(f'Num runs: {n_runs}')\n",
    "\n",
    "# best_test_mse, cost_rf_model = np.inf, None\n",
    "\n",
    "# for n_estimators, min_samples_leaf, max_depth, min_samples_split, criterion in combos_to_try:\n",
    "#     curr_model = RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_depth=max_depth, min_samples_split=min_samples_split, criterion=criterion)\n",
    "#     curr_model.fit(cost_x_train, cost_y_train)\n",
    "\n",
    "#     cost_y_train_pred = curr_model.predict(cost_x_train)\n",
    "#     cost_y_test_pred = curr_model.predict(cost_x_test)\n",
    "\n",
    "#     train_mse = mean_squared_error(cost_y_train, cost_y_train_pred)\n",
    "#     test_mse = mean_squared_error(cost_y_test, cost_y_test_pred)\n",
    "\n",
    "#     if test_mse < best_test_mse:\n",
    "#         best_test_mse, cost_rf_model = test_mse, curr_model\n",
    "#         print(f'Best results so far: train = {train_mse}, test = {test_mse}')\n",
    "\n",
    "#         # Early stopping\n",
    "#         if best_test_mse == 0.0:\n",
    "#             break\n",
    "\n",
    "#     n_runs -= 1\n",
    "#     print(f'Remaining runs: {n_runs}')\n",
    "\n",
    "# print(best_test_mse)\n",
    "# print(cost_rf_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_cost_rf_pred = np.exp(cost_rf_model.predict(original_cost_x)) - 1\n",
    "\n",
    "# print(mean_squared_error(original_cost_y, original_cost_rf_pred))\n",
    "# print(calculate_normalized_gini_regression(original_cost_y, original_cost_rf_pred))\n",
    "# print(original_cost_y.max(), original_cost_rf_pred.max())\n",
    "# print(original_cost_y.min(), original_cost_rf_pred.min())\n",
    "# print(np.median(original_cost_y), np.median(original_cost_rf_pred))\n",
    "# print(original_cost_y.mean(), original_cost_rf_pred.mean())\n",
    "\n",
    "# plt.hist(original_cost_y, bins=50)\n",
    "# plt.hist(original_cost_rf_pred, bins=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_combos = []\n",
    "\n",
    "# for n_estimators in [5, 10, 15, 20, 25, 50, 100, 200, 300]:\n",
    "#     for min_samples_leaf in [5, 10, 15, 20, 25, 50]:\n",
    "#         for max_depth in [3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "#             for min_samples_split in [2, 3, 4, 5, 10, 15]:\n",
    "#                 for learning_rate in [0.01, 0.1, 0.2]:\n",
    "#                     for loss in ['squared_error', 'absolute_error', 'huber', 'quantile']:\n",
    "#                         all_combos.append((n_estimators, min_samples_leaf, max_depth, min_samples_split, learning_rate, loss))\n",
    "\n",
    "# percentage_to_try = 0.0025\n",
    "# n_runs = int(percentage_to_try * len(all_combos))\n",
    "# combos_to_try = random.sample(all_combos, n_runs)\n",
    "# print(f'Num runs: {n_runs}')\n",
    "\n",
    "# best_test_mse, cost_gb_model = np.inf, None\n",
    "\n",
    "# for n_estimators, min_samples_leaf, max_depth, min_samples_split, learning_rate, loss in combos_to_try:\n",
    "#     curr_model = GradientBoostingRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_depth=max_depth, min_samples_split=min_samples_split, learning_rate=learning_rate, loss=loss)\n",
    "#     curr_model.fit(cost_x_train, cost_y_train)\n",
    "\n",
    "#     cost_y_train_pred = curr_model.predict(cost_x_train)\n",
    "#     cost_y_test_pred = curr_model.predict(cost_x_test)\n",
    "\n",
    "#     train_mse = mean_squared_error(cost_y_train, cost_y_train_pred)\n",
    "#     test_mse = mean_squared_error(cost_y_test, cost_y_test_pred)\n",
    "\n",
    "#     if test_mse < best_test_mse:\n",
    "#         best_test_mse, cost_gb_model = test_mse, curr_model\n",
    "#         print(f'Best results so far: train = {train_mse}, test = {test_mse}')\n",
    "\n",
    "#         # Early stopping\n",
    "#         if best_test_mse == 0.0:\n",
    "#             break\n",
    "\n",
    "#     n_runs -= 1\n",
    "#     print(f'Remaining runs: {n_runs}')\n",
    "\n",
    "# print(best_test_mse)\n",
    "# print(cost_gb_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_cost_gb_pred = np.clip(np.exp(cost_gb_model.predict(original_cost_x)) - 1, a_min=0, a_max=None)\n",
    "\n",
    "# print(mean_squared_error(original_cost_y, original_cost_gb_pred))\n",
    "# print(calculate_normalized_gini_regression(original_cost_y, original_cost_gb_pred))\n",
    "# print(original_cost_y.max(), original_cost_gb_pred.max())\n",
    "# print(original_cost_y.min(), original_cost_gb_pred.min())\n",
    "# print(np.median(original_cost_y), np.median(original_cost_gb_pred))\n",
    "# print(original_cost_y.mean(), original_cost_gb_pred.mean())\n",
    "\n",
    "# plt.hist(original_cost_y, bins=50)\n",
    "# plt.hist(original_cost_gb_pred, bins=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 20:45:16.980897: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707/707 [==============================] - 3s 3ms/step\n",
      "1641587.5599059456\n",
      "[0.5]\n",
      "57895.58456 10.988726\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "163.04808373789712 0.9086676\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "original_cost_pred = cost_model_tweedie.predict(original_cost_x)\n",
    "\n",
    "print(mean_squared_error(original_cost_y, original_cost_pred))\n",
    "print(calculate_normalized_gini_regression(original_cost_y, original_cost_pred))\n",
    "print(original_cost_y.max(), original_cost_pred.max())\n",
    "print(original_cost_y.min(), original_cost_pred.min())\n",
    "print(np.median(original_cost_y), np.median(original_cost_pred))\n",
    "print(original_cost_y.mean(), original_cost_pred.mean())\n",
    "\n",
    "plt.hist(original_cost_y, bins=50)\n",
    "plt.hist(original_cost_pred, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_model = cost_model_tweedie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veh_value</th>\n",
       "      <th>exposure</th>\n",
       "      <th>veh_body</th>\n",
       "      <th>veh_age</th>\n",
       "      <th>gender</th>\n",
       "      <th>area</th>\n",
       "      <th>agecat</th>\n",
       "      <th>engine_type</th>\n",
       "      <th>max_power</th>\n",
       "      <th>driving_history_score</th>\n",
       "      <th>veh_color</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>e_bill</th>\n",
       "      <th>time_of_week_driven</th>\n",
       "      <th>time_driven</th>\n",
       "      <th>trm_len</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>high_education_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.076279</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>petrol</td>\n",
       "      <td>174</td>\n",
       "      <td>83</td>\n",
       "      <td>black</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6pm - 12am</td>\n",
       "      <td>6</td>\n",
       "      <td>648.247594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.55</td>\n",
       "      <td>0.093443</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>181</td>\n",
       "      <td>65</td>\n",
       "      <td>yellow</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>12am - 6 am</td>\n",
       "      <td>12</td>\n",
       "      <td>637.752677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.04</td>\n",
       "      <td>0.157762</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>E</td>\n",
       "      <td>4</td>\n",
       "      <td>petrol</td>\n",
       "      <td>136</td>\n",
       "      <td>64</td>\n",
       "      <td>white</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>12pm - 6pm</td>\n",
       "      <td>12</td>\n",
       "      <td>661.483786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.05</td>\n",
       "      <td>0.560735</td>\n",
       "      <td>MIBUS</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "      <td>dissel</td>\n",
       "      <td>164</td>\n",
       "      <td>82</td>\n",
       "      <td>gray</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>12</td>\n",
       "      <td>647.846365</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.93</td>\n",
       "      <td>0.258275</td>\n",
       "      <td>HBACK</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "      <td>dissel</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>black</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>12</td>\n",
       "      <td>640.257550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   veh_value  exposure veh_body  veh_age gender area  agecat engine_type  \\\n",
       "0       3.40  0.076279    STNWG        2      M    B       4      petrol   \n",
       "1       2.55  0.093443    STNWG        2      F    A       3      petrol   \n",
       "2       3.04  0.157762    STNWG        2      F    E       4      petrol   \n",
       "3       2.05  0.560735    MIBUS        4      M    C       6      dissel   \n",
       "4       1.93  0.258275    HBACK        2      M    C       4      dissel   \n",
       "\n",
       "   max_power  driving_history_score veh_color marital_status  e_bill  \\\n",
       "0        174                     83     black              S       1   \n",
       "1        181                     65    yellow              M       0   \n",
       "2        136                     64     white              S       1   \n",
       "3        164                     82      gray              M       1   \n",
       "4         89                     48     black              S       0   \n",
       "\n",
       "  time_of_week_driven  time_driven  trm_len  credit_score  high_education_ind  \n",
       "0             weekday   6pm - 12am        6    648.247594                   0  \n",
       "1             weekday  12am - 6 am       12    637.752677                   0  \n",
       "2             weekday   12pm - 6pm       12    661.483786                   0  \n",
       "3             weekday   6am - 12pm       12    647.846365                   0  \n",
       "4             weekday   6am - 12pm       12    640.257550                   0  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_csv('./data/InsNova_data_2023_vh.csv')\n",
    "ids = val_df['id']\n",
    "val_df.drop(['id'], axis=1, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_dummies = pd.get_dummies(val_df, columns=['veh_body', 'veh_age', 'gender', 'area', 'agecat', 'engine_type', 'veh_color', 'marital_status', 'e_bill', 'time_of_week_driven', 'time_driven', 'trm_len', 'high_education_ind'])\n",
    "val_df_dummies = val_df_dummies.rename(columns={'high_education_ind_0': 'high_education_ind_0.0', 'high_education_ind_1': 'high_education_ind_1.0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mymac/miniforge3/envs/tf/lib/python3.9/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "val_df_dummies_clm_scaled = clm_scaler.transform(val_df_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22453\n",
       "1      167\n",
       "Name: clm, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df_dummies['clm'] = clm_model.predict(val_df_dummies_clm_scaled)\n",
    "val_df_dummies['clm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_dummies_num_claims_scaled = num_claims_scaler.transform(val_df_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22453\n",
       "1      166\n",
       "2        1\n",
       "Name: numclaims, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df_dummies['numclaims'] = num_claims_model.predict(val_df_dummies_num_claims_scaled)\n",
    "val_df_dummies['numclaims'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_dummies = cost_scaler.transform(val_df_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cost_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mymac/travelers/main.ipynb Cell 101\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y202sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predicted_costs \u001b[39m=\u001b[39m cost_model\u001b[39m.\u001b[39mpredict(val_df_dummies)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y202sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(predicted_costs\u001b[39m.\u001b[39mmin())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mymac/travelers/main.ipynb#Y202sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(predicted_costs\u001b[39m.\u001b[39mmax())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cost_model' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_costs = cost_model.predict(val_df_dummies)\n",
    "\n",
    "print(predicted_costs.min())\n",
    "print(predicted_costs.max())\n",
    "print(np.median(predicted_costs))\n",
    "print(predicted_costs.mean())\n",
    "\n",
    "plt.hist(df['claimcst0'], bins=50)\n",
    "plt.hist(predicted_costs, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Predict\n",
       "0   1      0.0\n",
       "1   2      0.0\n",
       "2   3      0.0\n",
       "3   4      0.0\n",
       "4   5      0.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'id': ids, 'Predict': predicted_costs.reshape(-1,)})\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
