{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veh_value</th>\n",
       "      <th>exposure</th>\n",
       "      <th>veh_body</th>\n",
       "      <th>veh_age</th>\n",
       "      <th>gender</th>\n",
       "      <th>area</th>\n",
       "      <th>agecat</th>\n",
       "      <th>engine_type</th>\n",
       "      <th>max_power</th>\n",
       "      <th>driving_history_score</th>\n",
       "      <th>...</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>e_bill</th>\n",
       "      <th>time_of_week_driven</th>\n",
       "      <th>time_driven</th>\n",
       "      <th>trm_len</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>high_education_ind</th>\n",
       "      <th>clm</th>\n",
       "      <th>numclaims</th>\n",
       "      <th>claimcst0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.444504</td>\n",
       "      <td>SEDAN</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>147</td>\n",
       "      <td>67.0</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6pm - 12am</td>\n",
       "      <td>6</td>\n",
       "      <td>640.448137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.45</td>\n",
       "      <td>0.562183</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>158</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>12</td>\n",
       "      <td>683.749691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.90</td>\n",
       "      <td>0.465244</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>159</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6pm - 12am</td>\n",
       "      <td>6</td>\n",
       "      <td>653.656117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.271039</td>\n",
       "      <td>PANVN</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "      <td>4</td>\n",
       "      <td>petrol</td>\n",
       "      <td>80</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>12pm - 6pm</td>\n",
       "      <td>12</td>\n",
       "      <td>642.574671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.141624</td>\n",
       "      <td>SEDAN</td>\n",
       "      <td>4</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>5</td>\n",
       "      <td>petrol</td>\n",
       "      <td>126</td>\n",
       "      <td>91.0</td>\n",
       "      <td>...</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>6</td>\n",
       "      <td>647.175035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   veh_value  exposure veh_body  veh_age gender area  agecat engine_type  \\\n",
       "0       0.77  0.444504    SEDAN        4      M    D       3      petrol   \n",
       "1       4.45  0.562183    STNWG        1      M    A       3      petrol   \n",
       "2       4.90  0.465244    STNWG        1      F    A       3      petrol   \n",
       "3       0.48  0.271039    PANVN        4      M    A       4      petrol   \n",
       "4       0.85  0.141624    SEDAN        4      F    A       5      petrol   \n",
       "\n",
       "   max_power  driving_history_score  ... marital_status e_bill  \\\n",
       "0        147                   67.0  ...              S      1   \n",
       "1        158                   76.0  ...              S      1   \n",
       "2        159                   58.0  ...              M      1   \n",
       "3         80                   72.0  ...              S      1   \n",
       "4        126                   91.0  ...              S      0   \n",
       "\n",
       "   time_of_week_driven time_driven trm_len  credit_score  high_education_ind  \\\n",
       "0              weekday  6pm - 12am       6    640.448137                 1.0   \n",
       "1              weekday  6am - 12pm      12    683.749691                 0.0   \n",
       "2              weekday  6pm - 12am       6    653.656117                 1.0   \n",
       "3              weekday  12pm - 6pm      12    642.574671                 0.0   \n",
       "4              weekday  6am - 12pm       6    647.175035                 0.0   \n",
       "\n",
       "   clm  numclaims  claimcst0  \n",
       "0    0          0        0.0  \n",
       "1    0          0        0.0  \n",
       "2    0          0        0.0  \n",
       "3    0          0        0.0  \n",
       "4    0          0        0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/InsNova_data_2023_train.csv')\n",
    "df.drop(['id'], axis=1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, columns=['veh_body', 'veh_age', 'gender', 'area', 'agecat', 'engine_type', 'veh_color', 'marital_status', 'e_bill', 'time_of_week_driven', 'time_driven', 'trm_len', 'high_education_ind'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### clm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clm\n",
      "0    21077\n",
      "1     1542\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "clm_df = df_dummies.drop(columns=['numclaims', 'claimcst0'])\n",
    "\n",
    "print(clm_df['clm'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21077, 64)\n",
      "(1542, 64)\n"
     ]
    }
   ],
   "source": [
    "majority_class = clm_df[clm_df['clm'] == 0]\n",
    "minority_class = clm_df[clm_df['clm'] == 1]\n",
    "\n",
    "print(majority_class.shape)\n",
    "print(minority_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21077, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_minority = resample(minority_class, replace=True, n_samples=len(majority_class))\n",
    "\n",
    "oversampled_minority.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clm_balanced_df = pd.concat([majority_class, oversampled_minority])\n",
    "clm_balanced_df = clm_balanced_df.sample(frac=1)\n",
    "clm_balanced_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42154, 63) (42154,)\n"
     ]
    }
   ],
   "source": [
    "clm_x, clm_y = clm_balanced_df.drop(columns=['clm']), clm_balanced_df['clm']\n",
    "\n",
    "print(clm_x.shape, clm_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18734, 63) (9368, 63) (14052, 63) (18734,) (9368,) (14052,)\n"
     ]
    }
   ],
   "source": [
    "clm_x_train, clm_x_test, clm_y_train, clm_y_test = train_test_split(clm_x, clm_y, test_size=1 / 3)\n",
    "clm_x_train, clm_x_val, clm_y_train, clm_y_val = train_test_split(clm_x_train, clm_y_train, test_size=1 / 3)\n",
    "\n",
    "print(clm_x_train.shape, clm_x_val.shape, clm_x_test.shape, clm_y_train.shape, clm_y_val.shape, clm_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num runs: 97\n",
      "Best results so far: train = 0.7824276716131099, test = 0.7481497295758611\n",
      "Remaining runs: 96\n",
      "Remaining runs: 95\n",
      "Remaining runs: 94\n",
      "Remaining runs: 93\n",
      "Remaining runs: 92\n",
      "Best results so far: train = 0.999786484466745, test = 0.967904924565898\n",
      "Remaining runs: 91\n",
      "Remaining runs: 90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mymac/travelers/xgboost_hurdle.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m n_estimators, min_samples_leaf, max_depth, min_samples_split, learning_rate \u001b[39min\u001b[39;00m combos_to_try:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     curr_model \u001b[39m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[39m=\u001b[39mn_estimators, min_samples_leaf\u001b[39m=\u001b[39mmin_samples_leaf, max_depth\u001b[39m=\u001b[39mmax_depth, min_samples_split\u001b[39m=\u001b[39mmin_samples_split, learning_rate\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     curr_model\u001b[39m.\u001b[39;49mfit(clm_x_train, clm_y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     clm_y_train_pred \u001b[39m=\u001b[39m curr_model\u001b[39m.\u001b[39mpredict(clm_x_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     clm_y_test_pred \u001b[39m=\u001b[39m curr_model\u001b[39m.\u001b[39mpredict(clm_x_test)\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:538\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resize_state()\n\u001b[1;32m    537\u001b[0m \u001b[39m# fit the boosting stages\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m n_stages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stages(\n\u001b[1;32m    539\u001b[0m     X,\n\u001b[1;32m    540\u001b[0m     y,\n\u001b[1;32m    541\u001b[0m     raw_predictions,\n\u001b[1;32m    542\u001b[0m     sample_weight,\n\u001b[1;32m    543\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rng,\n\u001b[1;32m    544\u001b[0m     X_val,\n\u001b[1;32m    545\u001b[0m     y_val,\n\u001b[1;32m    546\u001b[0m     sample_weight_val,\n\u001b[1;32m    547\u001b[0m     begin_at_stage,\n\u001b[1;32m    548\u001b[0m     monitor,\n\u001b[1;32m    549\u001b[0m )\n\u001b[1;32m    551\u001b[0m \u001b[39m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[1;32m    552\u001b[0m \u001b[39mif\u001b[39;00m n_stages \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:615\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    608\u001b[0m     old_oob_score \u001b[39m=\u001b[39m loss_(\n\u001b[1;32m    609\u001b[0m         y[\u001b[39m~\u001b[39msample_mask],\n\u001b[1;32m    610\u001b[0m         raw_predictions[\u001b[39m~\u001b[39msample_mask],\n\u001b[1;32m    611\u001b[0m         sample_weight[\u001b[39m~\u001b[39msample_mask],\n\u001b[1;32m    612\u001b[0m     )\n\u001b[1;32m    614\u001b[0m \u001b[39m# fit next stage of trees\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m raw_predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_stage(\n\u001b[1;32m    616\u001b[0m     i,\n\u001b[1;32m    617\u001b[0m     X,\n\u001b[1;32m    618\u001b[0m     y,\n\u001b[1;32m    619\u001b[0m     raw_predictions,\n\u001b[1;32m    620\u001b[0m     sample_weight,\n\u001b[1;32m    621\u001b[0m     sample_mask,\n\u001b[1;32m    622\u001b[0m     random_state,\n\u001b[1;32m    623\u001b[0m     X_csc,\n\u001b[1;32m    624\u001b[0m     X_csr,\n\u001b[1;32m    625\u001b[0m )\n\u001b[1;32m    627\u001b[0m \u001b[39m# track deviance (= loss)\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m do_oob:\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/sklearn/ensemble/_gb.py:257\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    254\u001b[0m     sample_weight \u001b[39m=\u001b[39m sample_weight \u001b[39m*\u001b[39m sample_mask\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    256\u001b[0m X \u001b[39m=\u001b[39m X_csr \u001b[39mif\u001b[39;00m X_csr \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m X\n\u001b[0;32m--> 257\u001b[0m tree\u001b[39m.\u001b[39;49mfit(X, residual, sample_weight\u001b[39m=\u001b[39;49msample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    259\u001b[0m \u001b[39m# update tree leaves\u001b[39;00m\n\u001b[1;32m    260\u001b[0m loss\u001b[39m.\u001b[39mupdate_terminal_regions(\n\u001b[1;32m    261\u001b[0m     tree\u001b[39m.\u001b[39mtree_,\n\u001b[1;32m    262\u001b[0m     X,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     k\u001b[39m=\u001b[39mk,\n\u001b[1;32m    270\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/sklearn/tree/_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1219\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m \n\u001b[1;32m   1221\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1244\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m   1248\u001b[0m         X,\n\u001b[1;32m   1249\u001b[0m         y,\n\u001b[1;32m   1250\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1251\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m   1252\u001b[0m     )\n\u001b[1;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/sklearn/tree/_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    370\u001b[0m         splitter,\n\u001b[1;32m    371\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_combos = []\n",
    "\n",
    "for n_estimators in [5, 10, 15, 20, 25, 50, 100, 200, 300]:\n",
    "    for min_samples_leaf in [5, 10, 15, 20, 25, 50]:\n",
    "        for max_depth in [3, 4, 5, 6, 7, 8, 9, 10, 15, 20]:\n",
    "            for min_samples_split in [2, 3, 4, 5, 10, 15]:\n",
    "                for learning_rate in [0.01, 0.1, 0.2]:\n",
    "                    all_combos.append((n_estimators, min_samples_leaf, max_depth, min_samples_split, learning_rate))\n",
    "\n",
    "percentage_to_try = 0.01\n",
    "n_runs = int(percentage_to_try * len(all_combos))\n",
    "combos_to_try = random.sample(all_combos, n_runs)\n",
    "print(f'Num runs: {n_runs}')\n",
    "\n",
    "best_test_accuracy, clm_model = -np.inf, None\n",
    "\n",
    "for n_estimators, min_samples_leaf, max_depth, min_samples_split, learning_rate in combos_to_try:\n",
    "    curr_model = GradientBoostingClassifier(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, max_depth=max_depth, min_samples_split=min_samples_split, learning_rate=learning_rate)\n",
    "    curr_model.fit(clm_x_train, clm_y_train)\n",
    "\n",
    "    clm_y_train_pred = curr_model.predict(clm_x_train)\n",
    "    clm_y_test_pred = curr_model.predict(clm_x_test)\n",
    "\n",
    "    train_accuracy = accuracy_score(clm_y_train, clm_y_train_pred)\n",
    "    test_accuracy = accuracy_score(clm_y_test, clm_y_test_pred)\n",
    "\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy, clm_model = test_accuracy, curr_model\n",
    "        print(f'Best results so far: train = {train_accuracy}, test = {test_accuracy}')\n",
    "\n",
    "        # Early stopping\n",
    "        if best_test_accuracy >= 0.99:\n",
    "            break\n",
    "\n",
    "    n_runs -= 1\n",
    "    print(f'Remaining runs: {n_runs}')\n",
    "\n",
    "print(best_test_accuracy)\n",
    "print(clm_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6073868488471392                          \n",
      "Test loss: 0.6008397381155708                                                     \n",
      "Test loss: 0.6040421292342727                                                     \n",
      "Test loss: 0.6083831483062909                                                     \n",
      "Test loss: 0.6126530031312268                                                     \n",
      "Test loss: 0.6069598633646456                                                     \n",
      "Test loss: 0.6058212354113294                                                     \n",
      "Test loss: 0.622046683746086                                                      \n",
      "Test loss: 0.6040421292342727                                                     \n",
      "Test loss: 0.6058212354113294                                                    \n",
      "Test loss: 0.6083831483062909                                                     \n",
      "Test loss: 0.6058212354113294                                                     \n",
      "Test loss: 0.6213350412752633                                                     \n",
      "Test loss: 0.6040421292342727                                                     \n",
      "Test loss: 0.6206945630515229                                                     \n",
      "Test loss: 0.604397950469684                                                      \n",
      "Test loss: 0.6106604042129234                                                     \n",
      "Test loss: 0.6066040421292342                                                     \n",
      "Test loss: 0.6208368915456874                                                     \n",
      "Test loss: 0.6058212354113294                                                     \n",
      "Test loss: 0.6224025049814973                                                     \n",
      "Test loss: 0.6206233988044406                                                      \n",
      "Test loss: 0.6197694278394534                                                      \n",
      "Test loss: 0.6191289496157131                                                      \n",
      "Test loss: 0.6094506120125249                                                      \n",
      "Test loss: 0.6169228579561629                                                      \n",
      "Test loss: 0.6162823797324224                                                      \n",
      "Test loss: 0.5219185881013378                                                      \n",
      "Test loss: 0.6238257899231426                                                      \n",
      "Test loss: 0.62311414745232                                                        \n",
      "Test loss: 0.6263165385710219                                                      \n",
      "Test loss: 0.6216908625106746                                                      \n",
      "Test loss: 0.6253914033589525                                                      \n",
      "Test loss: 0.6250355821235412                                                      \n",
      "Test loss: 0.6058212354113294                                                      \n",
      "Test loss: 0.6185596356390549                                                      \n",
      "Test loss: 0.6116567036720751                                                      \n",
      "Test loss: 0.6074580130942214                                                      \n",
      "Test loss: 0.6113008824366638                                                      \n",
      "Test loss: 0.6165670367207515                                                      \n",
      "Test loss: 0.6004839168801595                                                      \n",
      "Test loss: 0.6169228579561629                                                      \n",
      "Test loss: 0.6263877028181042                                                      \n",
      "Test loss: 0.6063193851409052                                                      \n",
      "Test loss: 0.6216196982635924                                                      \n",
      "Test loss: 0.6053230856817535                                                      \n",
      "Test loss: 0.6104469114716766                                                      \n",
      "Test loss: 0.6058212354113294                                                      \n",
      "Test loss: 0.6215485340165101                                                      \n",
      "Test loss: 0.604397950469684                                                       \n",
      "Test loss: 0.49843438656419015                                                     \n",
      "Test loss: 0.6147879305436948                                                      \n",
      "Test loss: 0.609735269000854                                                       \n",
      "Test loss: 0.6280244805009964                                                      \n",
      "Test loss: 0.6216196982635924                                                      \n",
      "Test loss: 0.616709365214916                                                       \n",
      "Test loss: 0.6069598633646456                                                      \n",
      "Test loss: 0.6211215485340165                                                      \n",
      "Test loss: 0.6179903216623968                                                      \n",
      "Test loss: 0.6004839168801595                                                      \n",
      "Test loss: 0.6106604042129234                                                      \n",
      "Test loss: 0.6178479931682322                                                      \n",
      "Test loss: 0.6229718189581555                                                      \n",
      "Test loss: 0.6004839168801595                                                      \n",
      "Test loss: 0.6154284087674352                                                      \n",
      "Test loss: 0.6246797608881298                                                      \n",
      "Test loss: 0.6208368915456874                                                      \n",
      "Test loss: 0.6287361229718189                                                      \n",
      "Test loss: 0.6201252490748648                                                      \n",
      "Test loss: 0.6253914033589525                                                      \n",
      "Test loss: 0.6201252490748648                                                      \n",
      "Test loss: 0.6261742100768574                                                      \n",
      "Test loss: 0.6192001138627954                                                      \n",
      "Test loss: 0.6198405920865357                                                      \n",
      "Test loss: 0.6271705095360091                                                      \n",
      "Test loss: 0.6163535439795047                                                      \n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 76/100 [00:42<00:13,  1.78trial/s, best loss: -0.6287361229718189]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mymac/travelers/xgboost_hurdle.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39minf, \u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m: STATUS_OK }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m trials \u001b[39m=\u001b[39m Trials()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m best_hyperparams \u001b[39m=\u001b[39m fmin(fn\u001b[39m=\u001b[39;49mobjective,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                         space\u001b[39m=\u001b[39;49mspace,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                         algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m                         max_evals\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m                         trials\u001b[39m=\u001b[39;49mtrials)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBest hyperparameters: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mbest_hyperparams\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[39m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[39mif\u001b[39;00m allow_trials_fmin \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(trials, \u001b[39m\"\u001b[39m\u001b[39mfmin\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m trials\u001b[39m.\u001b[39;49mfmin(\n\u001b[1;32m    541\u001b[0m         fn,\n\u001b[1;32m    542\u001b[0m         space,\n\u001b[1;32m    543\u001b[0m         algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    544\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    545\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    546\u001b[0m         loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    547\u001b[0m         max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    548\u001b[0m         rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    549\u001b[0m         pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    550\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    551\u001b[0m         catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    552\u001b[0m         return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    553\u001b[0m         show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    554\u001b[0m         early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    555\u001b[0m         trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    556\u001b[0m     )\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m trials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[39m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[39m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[39m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfmin\u001b[39;00m \u001b[39mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[39mreturn\u001b[39;00m fmin(\n\u001b[1;32m    672\u001b[0m     fn,\n\u001b[1;32m    673\u001b[0m     space,\n\u001b[1;32m    674\u001b[0m     algo\u001b[39m=\u001b[39;49malgo,\n\u001b[1;32m    675\u001b[0m     max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[1;32m    676\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    677\u001b[0m     loss_threshold\u001b[39m=\u001b[39;49mloss_threshold,\n\u001b[1;32m    678\u001b[0m     trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    679\u001b[0m     rstate\u001b[39m=\u001b[39;49mrstate,\n\u001b[1;32m    680\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    681\u001b[0m     max_queue_len\u001b[39m=\u001b[39;49mmax_queue_len,\n\u001b[1;32m    682\u001b[0m     allow_trials_fmin\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,  \u001b[39m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m     pass_expr_memo_ctrl\u001b[39m=\u001b[39;49mpass_expr_memo_ctrl,\n\u001b[1;32m    684\u001b[0m     catch_eval_exceptions\u001b[39m=\u001b[39;49mcatch_eval_exceptions,\n\u001b[1;32m    685\u001b[0m     return_argmin\u001b[39m=\u001b[39;49mreturn_argmin,\n\u001b[1;32m    686\u001b[0m     show_progressbar\u001b[39m=\u001b[39;49mshow_progressbar,\n\u001b[1;32m    687\u001b[0m     early_stop_fn\u001b[39m=\u001b[39;49mearly_stop_fn,\n\u001b[1;32m    688\u001b[0m     trials_save_file\u001b[39m=\u001b[39;49mtrials_save_file,\n\u001b[1;32m    689\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[1;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[1;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[1;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "\u001b[1;32m/Users/mymac/travelers/xgboost_hurdle.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m clf \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(space[\u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m]), max_depth\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(space[\u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m]), gamma\u001b[39m=\u001b[39mspace[\u001b[39m'\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                        reg_alpha\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(space[\u001b[39m'\u001b[39m\u001b[39mreg_alpha\u001b[39m\u001b[39m'\u001b[39m]), min_child_weight\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(space[\u001b[39m'\u001b[39m\u001b[39mmin_child_weight\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                        colsample_bytree\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(space[\u001b[39m'\u001b[39m\u001b[39mcolsample_bytree\u001b[39m\u001b[39m'\u001b[39m]), objective\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreg:logistic\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m evaluation \u001b[39m=\u001b[39m [(clm_x_val, clm_y_val)]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(clm_x_train, clm_y_train, eval_set\u001b[39m=\u001b[39;49mevaluation, early_stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(clm_x_test) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# loss = accuracy_score(clm_y_test, pred)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# print(f'Test loss: {loss}')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mymac/travelers/xgboost_hurdle.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# return {'loss': -loss, 'status': STATUS_OK }\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1026\u001b[0m     params,\n\u001b[1;32m   1027\u001b[0m     train_dmatrix,\n\u001b[1;32m   1028\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1029\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1030\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1031\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1032\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1033\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1034\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1035\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1036\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1037\u001b[0m )\n\u001b[1;32m   1039\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/xgboost/training.py:186\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     bst\u001b[39m.\u001b[39mupdate(dtrain, i, obj)\n\u001b[0;32m--> 186\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39;49mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    189\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39mafter_training(bst)\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/xgboost/callback.py:247\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    245\u001b[0m     metric_score \u001b[39m=\u001b[39m [(n, \u001b[39mfloat\u001b[39m(s)) \u001b[39mfor\u001b[39;00m n, s \u001b[39min\u001b[39;00m metric_score_str]\n\u001b[1;32m    246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 247\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(c\u001b[39m.\u001b[39mafter_iteration(model, epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory)\n\u001b[1;32m    248\u001b[0m           \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/xgboost/callback.py:247\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    245\u001b[0m     metric_score \u001b[39m=\u001b[39m [(n, \u001b[39mfloat\u001b[39m(s)) \u001b[39mfor\u001b[39;00m n, s \u001b[39min\u001b[39;00m metric_score_str]\n\u001b[1;32m    246\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_history(metric_score, epoch)\n\u001b[0;32m--> 247\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(c\u001b[39m.\u001b[39;49mafter_iteration(model, epoch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistory)\n\u001b[1;32m    248\u001b[0m           \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[1;32m    249\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniforge3/envs/travelers/lib/python3.11/site-packages/xgboost/callback.py:408\u001b[0m, in \u001b[0;36mEarlyStopping.after_iteration\u001b[0;34m(self, model, epoch, evals_log)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mafter_iteration\u001b[39m(\u001b[39mself\u001b[39m, model: _Model, epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m    409\u001b[0m                     evals_log: TrainingCallback\u001b[39m.\u001b[39mEvalsLog) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    410\u001b[0m     epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarting_round  \u001b[39m# training continuation\u001b[39;00m\n\u001b[1;32m    411\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMust have at least 1 validation dataset for early stopping.\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "space = {'max_depth': hp.quniform('max_depth', 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1, 9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40, 180, 1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': hp.uniform('n_estimators', 5, 500)\n",
    "        }\n",
    "\n",
    "def objective(space):\n",
    "    clf = xgb.XGBRegressor(n_estimators=int(space['n_estimators']), max_depth=int(space['max_depth']), gamma=space['gamma'],\n",
    "                           reg_alpha=int(space['reg_alpha']), min_child_weight=int(space['min_child_weight']),\n",
    "                           colsample_bytree=int(space['colsample_bytree']), objective='reg:logistic')\n",
    "    \n",
    "    evaluation = [(clm_x_val, clm_y_val)]\n",
    "    \n",
    "    clf.fit(clm_x_train, clm_y_train, eval_set=evaluation, early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "    pred = clf.predict(clm_x_test) >= 0.5\n",
    "\n",
    "    # loss = accuracy_score(clm_y_test, pred)\n",
    "\n",
    "    # print(f'Test loss: {loss}')\n",
    "\n",
    "    # return {'loss': -loss, 'status': STATUS_OK }\n",
    "\n",
    "    try:\n",
    "        loss = accuracy_score(clm_y_test, pred)\n",
    "\n",
    "        print(f'Test loss: {loss}')\n",
    "\n",
    "        return {'loss': -loss, 'status': STATUS_OK }\n",
    "    \n",
    "    except:\n",
    "        print('here')\n",
    "        return {'loss': np.inf, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn=objective,\n",
    "                        space=space,\n",
    "                        algo=tpe.suggest,\n",
    "                        max_evals=100,\n",
    "                        trials=trials)\n",
    "\n",
    "print(f'Best hyperparameters: \\n{best_hyperparams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8180099505659084, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=1.0643130726713133, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=16,\n",
       "             max_leaves=None, min_child_weight=9, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=485, n_jobs=None,\n",
       "             num_parallel_tree=None, objective=&#x27;reg:logistic&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8180099505659084, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=1.0643130726713133, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=16,\n",
       "             max_leaves=None, min_child_weight=9, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=485, n_jobs=None,\n",
       "             num_parallel_tree=None, objective=&#x27;reg:logistic&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8180099505659084, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=1.0643130726713133, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=16,\n",
       "             max_leaves=None, min_child_weight=9, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=485, n_jobs=None,\n",
       "             num_parallel_tree=None, objective='reg:logistic', predictor=None, ...)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params = {'colsample_bytree': 0.8180099505659084, 'gamma': 1.0643130726713133, 'max_depth': 16.0, 'min_child_weight': 9.0, 'n_estimators': 485.8553449406332, 'reg_alpha': 40.0, 'reg_lambda': 0.8070198480292471}\n",
    "# params['max_depth'] = int(params['max_depth'])\n",
    "# params['min_child_weight'] = int(params['min_child_weight'])\n",
    "# params['n_estimators'] = int(params['n_estimators'])\n",
    "# params['reg_alpha'] = int(params['reg_alpha'])\n",
    "# params['objective'] = 'reg:logistic'\n",
    "# clm_model = xgb.XGBRegressor(**params)\n",
    "# clm_model.fit(clm_x_train, clm_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7847281525761457"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clm_model.predict(clm_x_test) >= 0.5\n",
    "accuracy_score(clm_y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([False,  True]), array([16180,  6439]))\n",
      "clm\n",
      "0    21077\n",
      "1     1542\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "all_preds = clm_model.predict(clm_df.drop(columns=['clm'])) >= 0.5\n",
    "\n",
    "print(np.unique(all_preds, return_counts=True))\n",
    "print(clm_df['clm'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### numclaims model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numclaims\n",
       "0    21077\n",
       "1     1439\n",
       "2       94\n",
       "3        9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_claims_df = df_dummies.drop(columns=['clm', 'claimcst0'])\n",
    "\n",
    "num_claims_df['numclaims'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21077, 64)\n",
      "(1439, 64)\n",
      "(94, 64)\n",
      "(9, 64)\n"
     ]
    }
   ],
   "source": [
    "majority_class = num_claims_df[num_claims_df['numclaims'] == 0]\n",
    "minority_class_1 = num_claims_df[num_claims_df['numclaims'] == 1]\n",
    "minority_class_2 = num_claims_df[num_claims_df['numclaims'] == 2]\n",
    "minority_class_3 = num_claims_df[num_claims_df['numclaims'] == 3]\n",
    "\n",
    "print(majority_class.shape)\n",
    "print(minority_class_1.shape)\n",
    "print(minority_class_2.shape)\n",
    "print(minority_class_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21077, 64)\n",
      "(21077, 64)\n",
      "(21077, 64)\n"
     ]
    }
   ],
   "source": [
    "oversampled_minority_1 = resample(minority_class_1, replace=True, n_samples=len(majority_class))\n",
    "oversampled_minority_2 = resample(minority_class_2, replace=True, n_samples=len(majority_class))\n",
    "oversampled_minority_3 = resample(minority_class_3, replace=True, n_samples=len(majority_class))\n",
    "\n",
    "print(oversampled_minority_1.shape)\n",
    "print(oversampled_minority_2.shape)\n",
    "print(oversampled_minority_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84308, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_claims_balanced_df = pd.concat([majority_class, oversampled_minority_1, oversampled_minority_2, oversampled_minority_3])\n",
    "num_claims_balanced_df = num_claims_balanced_df.sample(frac=1)\n",
    "num_claims_balanced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "num_claims_balanced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84308, 63) (84308,)\n"
     ]
    }
   ],
   "source": [
    "num_claims_x, num_claims_y = num_claims_balanced_df.drop(columns=['numclaims']), num_claims_balanced_df['numclaims']\n",
    "\n",
    "print(num_claims_x.shape, num_claims_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37470, 63) (18735, 63) (28103, 63) (37470,) (18735,) (28103,)\n"
     ]
    }
   ],
   "source": [
    "num_claims_x_train, num_claims_x_test, num_claims_y_train, num_claims_y_test = train_test_split(num_claims_x, num_claims_y, test_size=1 / 3)\n",
    "num_claims_x_train, num_claims_x_val, num_claims_y_train, num_claims_y_val = train_test_split(num_claims_x_train, num_claims_y_train, test_size=1 / 3)\n",
    "\n",
    "print(num_claims_x_train.shape, num_claims_x_val.shape, num_claims_x_test.shape, num_claims_y_train.shape, num_claims_y_val.shape, num_claims_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.10956342182033899                         \n",
      "Test loss: 0.11052146920544302                                                    \n",
      "Test loss: 0.13748135363514838                                                    \n",
      "Test loss: 0.10040556184421702                                                    \n",
      "Test loss: 0.13081125291720178                                                    \n",
      "Test loss: 0.0881548858106303                                                     \n",
      "Test loss: 0.1639894841113023                                                     \n",
      "Test loss: 0.11344351499730741                                                   \n",
      "Test loss: 0.12920616060158988                                                   \n",
      "Test loss: 0.11016853604392085                                                   \n",
      "Test loss: 0.10817553786503083                                                    \n",
      "Test loss: 0.10832634493197686                                                    \n",
      "Test loss: 0.11670937844549713                                                    \n",
      "Test loss: 0.08875255833655771                                                    \n",
      "Test loss: 0.11032159488036411                                                    \n",
      "Test loss: 0.0935092231107261                                                     \n",
      "Test loss: 0.09128392227892734                                                    \n",
      "Test loss: 0.10675886714499427                                                    \n",
      "Test loss: 0.12544361551367786                                                    \n",
      "Test loss: 0.101060535420764                                                      \n",
      "Test loss: 0.07529765750099651                                                    \n",
      "Test loss: 0.07572675555750252                                                     \n",
      "Test loss: 0.07833332822320668                                                     \n",
      "Test loss: 0.07442008282532207                                                     \n",
      "Test loss: 0.07649892748707954                                                     \n",
      "Test loss: 0.11223502165628886                                                     \n",
      "Test loss: 0.08164212099102437                                                     \n",
      "Test loss: 0.11463737214993971                                                     \n",
      "Test loss: 0.09853258263475503                                                     \n",
      "Test loss: 0.08771392631858033                                                     \n",
      "Test loss: 0.09024265969939779                                                     \n",
      "Test loss: 0.07637000865408025                                                     \n",
      "Test loss: 0.08786033200478251                                                     \n",
      "Test loss: 0.0829202584567849                                                      \n",
      "Test loss: 0.08055891556377995                                                     \n",
      "Test loss: 0.10046347008573088                                                     \n",
      "Test loss: 0.1115822967674532                                                      \n",
      "Test loss: 0.10041665569490497                                                     \n",
      "Test loss: 0.09827670391628136                                                     \n",
      "Test loss: 0.11462693910000364                                                     \n",
      "Test loss: 0.08357437819590419                                                     \n",
      "Test loss: 0.10136876541250134                                                     \n",
      "Test loss: 0.09275178893229982                                                     \n",
      "Test loss: 0.10402406834022002                                                     \n",
      "Test loss: 0.0850958772869983                                                      \n",
      "Test loss: 0.09563330050012135                                                     \n",
      "Test loss: 0.11452522947315412                                                     \n",
      "Test loss: 0.268341251124463                                                       \n",
      "Test loss: 0.10191293715060727                                                     \n",
      "Test loss: 0.09559441040376376                                                     \n",
      "Test loss: 0.07802369725484226                                                     \n",
      "Test loss: 0.09131621212287601                                                     \n",
      "Test loss: 0.11233621078402442                                                     \n",
      "Test loss: 0.10765862352307881                                                     \n",
      "Test loss: 0.08063700754694283                                                     \n",
      "Test loss: 0.10141206797656088                                                     \n",
      "Test loss: 0.11130522113815877                                                     \n",
      "Test loss: 0.12043229919723458                                                     \n",
      "Test loss: 0.07746936647665711                                                     \n",
      "Test loss: 0.10382374873684967                                                     \n",
      "Test loss: 0.08545824465556653                                                     \n",
      "Test loss: 0.11442926138596642                                                     \n",
      "Test loss: 0.0814067514135089                                                      \n",
      "Test loss: 0.08868609919580653                                                     \n",
      "Test loss: 0.10469887122211716                                                     \n",
      "Test loss: 0.07312019089692426                                                     \n",
      "Test loss: 0.07846819180373218                                                     \n",
      "Test loss: 0.07307143123917263                                                     \n",
      "Test loss: 0.07978504210740528                                                     \n",
      "Test loss: 0.07847312911037906                                                     \n",
      "Test loss: 0.07499519058957291                                                     \n",
      "Test loss: 0.08878834210066176                                                     \n",
      "Test loss: 0.07353253767827699                                                     \n",
      "Test loss: 0.0941855788897963                                                      \n",
      "Test loss: 0.07844727568183284                                                     \n",
      "Test loss: 0.08162995229739628                                                     \n",
      "Test loss: 0.09153019975289278                                                     \n",
      "Test loss: 0.08907249651705948                                                     \n",
      "Test loss: 0.0735125113599167                                                      \n",
      "Test loss: 0.08892550484515772                                                     \n",
      "Test loss: 0.10307097893193834                                                     \n",
      "Test loss: 0.0854868947214417                                                      \n",
      "Test loss: 0.10816247309182772                                                     \n",
      "Test loss: 0.10012761597395237                                                     \n",
      "Test loss: 0.11204099322392796                                                     \n",
      "Test loss: 0.07782433946288023                                                     \n",
      "Test loss: 0.12817024148913506                                                     \n",
      "Test loss: 0.08532585069731943                                                     \n",
      "Test loss: 0.08068324857600927                                                     \n",
      "Test loss: 0.10196304163780721                                                     \n",
      "Test loss: 0.10381845939099152                                                     \n",
      "Test loss: 0.09460571784775926                                                     \n",
      "Test loss: 0.0960169781665489                                                      \n",
      "Test loss: 0.08559600115271078                                                     \n",
      "Test loss: 0.0938634879976823                                                      \n",
      "Test loss: 0.09886340542982545                                                     \n",
      "Test loss: 0.07709762471163803                                                     \n",
      "Test loss: 0.10003948746326832                                                     \n",
      "Test loss: 0.07997686996007969                                                     \n",
      "Test loss: 0.11374742297694213                                                     \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:11<00:00,  1.31s/trial, best loss: 0.07307143123917263]\n",
      "Best hyperparameters: \n",
      "{'colsample_bytree': 0.9549427100408518, 'gamma': 1.531490222686555, 'max_depth': 17.0, 'min_child_weight': 2.0, 'n_estimators': 328.50293938973755, 'reg_alpha': 46.0, 'reg_lambda': 0.006139409044508717, 'tweedie_variance_power': 1.8646542717837375}\n"
     ]
    }
   ],
   "source": [
    "space = {'max_depth': hp.quniform('max_depth', 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1, 9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40, 180, 1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': hp.uniform('n_estimators', 5, 500),\n",
    "        'tweedie_variance_power': hp.uniform('tweedie_variance_power', 1, 2)\n",
    "        }\n",
    "\n",
    "def objective(space):\n",
    "    power = space['tweedie_variance_power']\n",
    "\n",
    "    clf = xgb.XGBRegressor(n_estimators=int(space['n_estimators']), max_depth=int(space['max_depth']), gamma=space['gamma'],\n",
    "                           reg_alpha=int(space['reg_alpha']), min_child_weight=int(space['min_child_weight']),\n",
    "                           colsample_bytree=int(space['colsample_bytree']), objective='reg:tweedie', tweedie_variance_power=power)\n",
    "    \n",
    "    evaluation = [(num_claims_x_train, num_claims_y_train), (num_claims_x_val, num_claims_y_val)]\n",
    "    \n",
    "    clf.fit(num_claims_x_train, num_claims_y_train, eval_set=evaluation, early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "\n",
    "    pred = clf.predict(num_claims_x_test)\n",
    "\n",
    "    try:\n",
    "        # pred = np.clip(pred, a_min=0, a_max=None)\n",
    "        # loss = tweedie_loss(y_test, pred, power)\n",
    "        loss = mean_squared_error(num_claims_y_test, pred)\n",
    "\n",
    "        print(f'Test loss: {loss}')\n",
    "\n",
    "        return {'loss': loss, 'status': STATUS_OK }\n",
    "    \n",
    "    except:\n",
    "        return {'loss': np.inf, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn=objective,\n",
    "                        space=space,\n",
    "                        algo=tpe.suggest,\n",
    "                        max_evals=100,\n",
    "                        trials=trials)\n",
    "\n",
    "print(f'Best hyperparameters: \\n{best_hyperparams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.9549427100408518, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=1.531490222686555, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=17,\n",
       "             max_leaves=None, min_child_weight=2, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=328, n_jobs=None,\n",
       "             num_parallel_tree=None, objective=&#x27;reg:tweedie&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.9549427100408518, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=1.531490222686555, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=17,\n",
       "             max_leaves=None, min_child_weight=2, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=328, n_jobs=None,\n",
       "             num_parallel_tree=None, objective=&#x27;reg:tweedie&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.9549427100408518, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=1.531490222686555, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=17,\n",
       "             max_leaves=None, min_child_weight=2, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=328, n_jobs=None,\n",
       "             num_parallel_tree=None, objective='reg:tweedie', predictor=None, ...)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'colsample_bytree': 0.9549427100408518, 'gamma': 1.531490222686555, 'max_depth': 17.0, 'min_child_weight': 2.0, 'n_estimators': 328.50293938973755, 'reg_alpha': 46.0, 'reg_lambda': 0.006139409044508717, 'tweedie_variance_power': 1.8646542717837375}\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "params['min_child_weight'] = int(params['min_child_weight'])\n",
    "params['n_estimators'] = int(params['n_estimators'])\n",
    "params['reg_alpha'] = int(params['reg_alpha'])\n",
    "params['objective'] = 'reg:tweedie'\n",
    "power = params['tweedie_variance_power']\n",
    "num_claims_model = xgb.XGBRegressor(**params)\n",
    "num_claims_model.fit(num_claims_x_train, num_claims_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01161597806028189"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = num_claims_model.predict(num_claims_x_test)\n",
    "mean_squared_log_error(num_claims_y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_df = df_dummies.drop(columns=['clm', 'numclaims'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn on the impure num claims data\n",
    "num_claims_preds = num_claims_model.predict(cost_df.drop(columns=['claimcst0']))\n",
    "assert len(num_claims_preds) == len(cost_df)\n",
    "cost_df['numclaims'] = num_claims_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21077, 65)\n",
      "(1542, 65)\n"
     ]
    }
   ],
   "source": [
    "majority_class = cost_df[cost_df['claimcst0'] == 0]\n",
    "minority_class = cost_df[cost_df['claimcst0'] != 0]\n",
    "\n",
    "print(majority_class.shape)\n",
    "print(minority_class.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21077, 65)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_minority = resample(minority_class, replace=True, n_samples=len(majority_class))\n",
    "\n",
    "oversampled_minority.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21077, 65)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_balanced_df = pd.concat([majority_class, oversampled_minority])\n",
    "cost_balanced_df = cost_balanced_df.sample(frac=1)\n",
    "cost_balanced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "cost_balanced_df[cost_balanced_df['claimcst0'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_x, cost_y = cost_balanced_df.drop(columns=['claimcst0']), cost_balanced_df['claimcst0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18734, 64) (18734,) (9368, 64) (9368,) (14052, 64) (14052,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(cost_x, cost_y, test_size=1 / 3)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1 / 3)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.121907530584813                           \n",
      "Test loss: 2.3903727713796434                                                   \n",
      "Test loss: 15.279686535868855                                                    \n",
      "Test loss: 1.580606209756158                                                     \n",
      "Test loss: 14.44633011795658                                                     \n",
      "Test loss: 5.133898179780475                                                    \n",
      "Test loss: 4.23363280074671                                                     \n",
      "Test loss: 9.1027925031824                                                      \n",
      "Test loss: 1.750025499282127                                                    \n",
      "Test loss: 14.653343969484354                                                   \n",
      "Test loss: 1.9649807200693572                                                    \n",
      "Test loss: 4.709625852000694                                                     \n",
      "Test loss: 6.868266042631986                                                     \n",
      "Test loss: 1.4786290415214194                                                    \n",
      "Test loss: 1.5636629925833971                                                     \n",
      "Test loss: 0.8042544491561809                                                     \n",
      "Test loss: 1.9636651330891806                                                     \n",
      "Test loss: 1.6432582915930485                                                     \n",
      "Test loss: 1.357597329187812                                                      \n",
      "Test loss: 10.860810436718783                                                     \n",
      "Test loss: 1.1742084145382428                                                     \n",
      "Test loss: 0.8907028475642583                                                     \n",
      "Test loss: 0.9869806819184395                                                     \n",
      "Test loss: 0.7307021037243656                                                     \n",
      "Test loss: 0.5832903030563115                                                     \n",
      "Test loss: 0.7666424679795806                                                     \n",
      "Test loss: 0.9238803120393214                                                     \n",
      "Test loss: 0.7018324219799102                                                     \n",
      "Test loss: 2.4844376346801695                                                     \n",
      "Test loss: 0.8909768885416499                                                     \n",
      "Test loss: 8.067467748966099                                                      \n",
      "Test loss: 0.6270193315946706                                                     \n",
      "Test loss: 2.5789115687408883                                                     \n",
      "Test loss: 9.831670810267505                                                      \n",
      "Test loss: 1.6214658334318341                                                     \n",
      "Test loss: 5.422525903460954                                                      \n",
      "Test loss: 5.19012358065778                                                       \n",
      "Test loss: 1.118518064656731                                                      \n",
      "Test loss: 1.5423875487971912                                                     \n",
      "Test loss: 2.5959897071487297                                                     \n",
      "Test loss: 2.0083703780063478                                                     \n",
      "Test loss: 1.0461539860052338                                                     \n",
      "Test loss: 4.187895306643708                                                      \n",
      "Test loss: 6.952381849124857                                                      \n",
      "Test loss: 5.0342805495441025                                                     \n",
      "Test loss: 10.08884222533978                                                      \n",
      "Test loss: 4.801225584101017                                                      \n",
      "Test loss: 1.120176110084006                                                      \n",
      "Test loss: 0.9905832403567731                                                     \n",
      "Test loss: 0.7671220470564482                                                     \n",
      "Test loss: 9.274997876324486                                                      \n",
      "Test loss: 3.322409792067526                                                      \n",
      "Test loss: 5.696234325953548                                                      \n",
      "Test loss: 0.9663709244868074                                                     \n",
      "Test loss: 1.3054137233351601                                                     \n",
      "Test loss: 0.78794848447964                                                       \n",
      "Test loss: 0.883055395228241                                                      \n",
      "Test loss: 1.1377475933969297                                                     \n",
      "Test loss: 5.283835899733731                                                      \n",
      "Test loss: 8.160364001643654                                                      \n",
      "Test loss: 0.7600241089156196                                                     \n",
      "Test loss: 4.0352866195102965                                                     \n",
      "Test loss: 4.245900137714256                                                      \n",
      "Test loss: 0.9502132698562784                                                     \n",
      "Test loss: 16.891006505606125                                                     \n",
      "Test loss: 1.7520594959337787                                                     \n",
      "Test loss: 0.6858170800568015                                                     \n",
      "Test loss: 2.282548457441294                                                      \n",
      "Test loss: 0.643242331120031                                                      \n",
      "Test loss: 3.2492426845009175                                                     \n",
      "Test loss: 0.6775179419378389                                                     \n",
      "Test loss: 1.9495299040392706                                                     \n",
      "Test loss: 0.61949301487424                                                       \n",
      "Test loss: 1.1087647083817331                                                     \n",
      "Test loss: 1.3967394495404848                                                     \n",
      "Test loss: 1.0382461712676803                                                     \n",
      "Test loss: 0.7101907444420802                                                     \n",
      "Test loss: 2.992866496266919                                                      \n",
      "Test loss: 1.268690139880942                                                      \n",
      "Test loss: 1.1263242544302083                                                     \n",
      "Test loss: 14.676919484428748                                                     \n",
      "Test loss: 5.031071106484717                                                      \n",
      "Test loss: 2.7868948450280344                                                     \n",
      "Test loss: 1.0791740456201286                                                     \n",
      "Test loss: 1.8211890507595845                                                     \n",
      "Test loss: 3.4311801545935885                                                     \n",
      "Test loss: 1.4255364927579044                                                     \n",
      "Test loss: 1.3017330319552791                                                     \n",
      "Test loss: 2.6328797150158296                                                     \n",
      "Test loss: 4.946333637218932                                                      \n",
      "Test loss: 12.709027666826822                                                     \n",
      "Test loss: 2.505166538629358                                                      \n",
      "Test loss: 0.9485128933033009                                                     \n",
      "Test loss: 3.975653270992458                                                      \n",
      "Test loss: 0.8315595706203296                                                     \n",
      "Test loss: 14.832871457526416                                                     \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:32<00:00,  1.08trial/s, best loss: 0.5832903030563115]\n",
      "Best hyperparameters: \n",
      "{'colsample_bytree': 0.805489057457027, 'gamma': 3.7512949147285575, 'max_depth': 14.0, 'min_child_weight': 2.0, 'n_estimators': 484.51694842102376, 'reg_alpha': 101.0, 'reg_lambda': 0.29574903970770366, 'tweedie_variance_power': 1.0127779031760553}\n"
     ]
    }
   ],
   "source": [
    "space = {'max_depth': hp.quniform('max_depth', 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1, 9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40, 180, 1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0, 1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5, 1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': hp.uniform('n_estimators', 5, 500),\n",
    "        'tweedie_variance_power': hp.uniform('tweedie_variance_power', 1, 2)\n",
    "        }\n",
    "\n",
    "def objective(space):\n",
    "    power = space['tweedie_variance_power']\n",
    "\n",
    "    clf = xgb.XGBRegressor(n_estimators=int(space['n_estimators']), max_depth=int(space['max_depth']), gamma=space['gamma'],\n",
    "                           reg_alpha=int(space['reg_alpha']), min_child_weight=int(space['min_child_weight']),\n",
    "                           colsample_bytree=int(space['colsample_bytree']), objective='reg:tweedie', tweedie_variance_power=power)\n",
    "    \n",
    "    evaluation = [(x_train, y_train), (x_val, y_val)]\n",
    "    \n",
    "    clf.fit(x_train, y_train, eval_set=evaluation, early_stopping_rounds=100, verbose=False)\n",
    "    \n",
    "\n",
    "    pred = clf.predict(x_test)\n",
    "\n",
    "    try:\n",
    "        loss = mean_squared_log_error(y_test, pred)\n",
    "\n",
    "        print(f'Test loss: {loss}')\n",
    "\n",
    "        return {'loss': loss, 'status': STATUS_OK }\n",
    "    \n",
    "    except:\n",
    "        return {'loss': np.inf, 'status': STATUS_OK }\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = fmin(fn=objective,\n",
    "                        space=space,\n",
    "                        algo=tpe.suggest,\n",
    "                        max_evals=100,\n",
    "                        trials=trials)\n",
    "\n",
    "print(f'Best hyperparameters: \\n{best_hyperparams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.805489057457027, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=3.7512949147285575, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=14,\n",
       "             max_leaves=None, min_child_weight=2, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=484, n_jobs=None,\n",
       "             num_parallel_tree=None, objective=&#x27;reg:tweedie&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.805489057457027, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=3.7512949147285575, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=14,\n",
       "             max_leaves=None, min_child_weight=2, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=484, n_jobs=None,\n",
       "             num_parallel_tree=None, objective=&#x27;reg:tweedie&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.805489057457027, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=3.7512949147285575, gpu_id=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
       "             max_cat_to_onehot=None, max_delta_step=None, max_depth=14,\n",
       "             max_leaves=None, min_child_weight=2, missing=nan,\n",
       "             monotone_constraints=None, n_estimators=484, n_jobs=None,\n",
       "             num_parallel_tree=None, objective='reg:tweedie', predictor=None, ...)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'colsample_bytree': 0.805489057457027, 'gamma': 3.7512949147285575, 'max_depth': 14.0, 'min_child_weight': 2.0, 'n_estimators': 484.51694842102376, 'reg_alpha': 101.0, 'reg_lambda': 0.29574903970770366, 'tweedie_variance_power': 1.0127779031760553}\n",
    "params['max_depth'] = int(params['max_depth'])\n",
    "params['min_child_weight'] = int(params['min_child_weight'])\n",
    "params['n_estimators'] = int(params['n_estimators'])\n",
    "params['reg_alpha'] = int(params['reg_alpha'])\n",
    "params['objective'] = 'reg:tweedie'\n",
    "power = params['tweedie_variance_power']\n",
    "cost_model = xgb.XGBRegressor(**params)\n",
    "cost_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2633299709330187"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = cost_model.predict(x_test)\n",
    "mean_squared_log_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(pred, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Predictons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>veh_value</th>\n",
       "      <th>exposure</th>\n",
       "      <th>veh_body</th>\n",
       "      <th>veh_age</th>\n",
       "      <th>gender</th>\n",
       "      <th>area</th>\n",
       "      <th>agecat</th>\n",
       "      <th>engine_type</th>\n",
       "      <th>max_power</th>\n",
       "      <th>driving_history_score</th>\n",
       "      <th>veh_color</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>e_bill</th>\n",
       "      <th>time_of_week_driven</th>\n",
       "      <th>time_driven</th>\n",
       "      <th>trm_len</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>high_education_ind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.40</td>\n",
       "      <td>0.076279</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "      <td>petrol</td>\n",
       "      <td>174</td>\n",
       "      <td>83</td>\n",
       "      <td>black</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6pm - 12am</td>\n",
       "      <td>6</td>\n",
       "      <td>648.247594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.55</td>\n",
       "      <td>0.093443</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>A</td>\n",
       "      <td>3</td>\n",
       "      <td>petrol</td>\n",
       "      <td>181</td>\n",
       "      <td>65</td>\n",
       "      <td>yellow</td>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>12am - 6 am</td>\n",
       "      <td>12</td>\n",
       "      <td>637.752677</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.04</td>\n",
       "      <td>0.157762</td>\n",
       "      <td>STNWG</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>E</td>\n",
       "      <td>4</td>\n",
       "      <td>petrol</td>\n",
       "      <td>136</td>\n",
       "      <td>64</td>\n",
       "      <td>white</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>12pm - 6pm</td>\n",
       "      <td>12</td>\n",
       "      <td>661.483786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.05</td>\n",
       "      <td>0.560735</td>\n",
       "      <td>MIBUS</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>C</td>\n",
       "      <td>6</td>\n",
       "      <td>dissel</td>\n",
       "      <td>164</td>\n",
       "      <td>82</td>\n",
       "      <td>gray</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>12</td>\n",
       "      <td>647.846365</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.93</td>\n",
       "      <td>0.258275</td>\n",
       "      <td>HBACK</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>C</td>\n",
       "      <td>4</td>\n",
       "      <td>dissel</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>black</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>weekday</td>\n",
       "      <td>6am - 12pm</td>\n",
       "      <td>12</td>\n",
       "      <td>640.257550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   veh_value  exposure veh_body  veh_age gender area  agecat engine_type  \\\n",
       "0       3.40  0.076279    STNWG        2      M    B       4      petrol   \n",
       "1       2.55  0.093443    STNWG        2      F    A       3      petrol   \n",
       "2       3.04  0.157762    STNWG        2      F    E       4      petrol   \n",
       "3       2.05  0.560735    MIBUS        4      M    C       6      dissel   \n",
       "4       1.93  0.258275    HBACK        2      M    C       4      dissel   \n",
       "\n",
       "   max_power  driving_history_score veh_color marital_status  e_bill  \\\n",
       "0        174                     83     black              S       1   \n",
       "1        181                     65    yellow              M       0   \n",
       "2        136                     64     white              S       1   \n",
       "3        164                     82      gray              M       1   \n",
       "4         89                     48     black              S       0   \n",
       "\n",
       "  time_of_week_driven  time_driven  trm_len  credit_score  high_education_ind  \n",
       "0             weekday   6pm - 12am        6    648.247594                   0  \n",
       "1             weekday  12am - 6 am       12    637.752677                   0  \n",
       "2             weekday   12pm - 6pm       12    661.483786                   0  \n",
       "3             weekday   6am - 12pm       12    647.846365                   0  \n",
       "4             weekday   6am - 12pm       12    640.257550                   0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.read_csv('./data/InsNova_data_2023_vh.csv')\n",
    "ids = val_df['id']\n",
    "val_df.drop(['id'], axis=1, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_dummies = pd.get_dummies(val_df, columns=['veh_body', 'veh_age', 'gender', 'area', 'agecat', 'engine_type', 'veh_color', 'marital_status', 'e_bill', 'time_of_week_driven', 'time_driven', 'trm_len', 'high_education_ind'])\n",
    "val_df_dummies = val_df_dummies.rename(columns={'high_education_ind_0': 'high_education_ind_0.0', 'high_education_ind_1': 'high_education_ind_1.0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_dummies['numclaims'] = num_claims_model.predict(val_df_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGdCAYAAAAbudkLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxNklEQVR4nO3df1SVZb7//9cekK2x4E6kzWavyJxZRhpOR7EBtB+aBpLIsjppQ2efOONgsypZHGGV1po5dtaZtB/mnDNOHXNZTkbRmWNW62CMOJXmUbRImkhzrNHEFYgVbITjbIjuzx99vb9t8ddGCPbl87HWvRb3db/3va8323vx8tr3Bpdt27YAAAAM9IOBngAAAEB/IegAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIwVPdATGEjffPONPv/8c8XFxcnlcg30dAAAwDmwbVvHjh2Tz+fTD35w5jWbCzrofP7550pJSRnoaQAAgF5oaGjQpZdeesaaCzroxMXFSfr2GxUfHz/AswEAAOeira1NKSkpzs/xM7mgg86Jt6vi4+MJOgAARJhzue2Em5EBAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjBU90BMw2eWLKgd6CmE7uGzmQE8BAIA+w4oOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYKK+gsXbpU11xzjeLi4uTxeDR79mzt27cvpMa2bS1ZskQ+n0/Dhg3TlClT9NFHH4XUBINBLViwQImJiYqNjVV+fr4OHz4cUtPS0iK/3y/LsmRZlvx+v1pbW0NqDh06pFmzZik2NlaJiYkqLi5WZ2dnOC0BAACDhRV0tmzZonvvvVc1NTWqrq7W119/rezsbHV0dDg1jz32mJ588kmtXLlS7777rrxer2666SYdO3bMqSkpKdGGDRtUUVGhbdu2qb29XXl5eeru7nZqCgoKVFdXp6qqKlVVVamurk5+v9853t3drZkzZ6qjo0Pbtm1TRUWF1q9fr9LS0vP5fgAAAIO4bNu2e/vgo0ePyuPxaMuWLbr++utl27Z8Pp9KSkr0wAMPSPp29SYpKUmPPvqo7r77bgUCAV1yySVat26d5s6dK0n6/PPPlZKSoo0bNyonJ0d79+7V2LFjVVNTo4yMDElSTU2NsrKy9PHHHys1NVVvvPGG8vLy1NDQIJ/PJ0mqqKhQYWGhmpubFR8ff9b5t7W1ybIsBQKBc6oP1+WLKvv8nP3t4LKZAz0FAADOKJyf3+d1j04gEJAkJSQkSJIOHDigpqYmZWdnOzVut1s33HCDtm/fLkmqra1VV1dXSI3P51NaWppTs2PHDlmW5YQcScrMzJRlWSE1aWlpTsiRpJycHAWDQdXW1p5yvsFgUG1tbSEbAAAwV6+Djm3bWrhwoa699lqlpaVJkpqamiRJSUlJIbVJSUnOsaamJsXExGj48OFnrPF4PD2e0+PxhNSc/DzDhw9XTEyMU3OypUuXOvf8WJallJSUcNsGAAARpNdB57777tOf//xnvfTSSz2OuVyukH3btnuMnezkmlPV96bmuxYvXqxAIOBsDQ0NZ5wTAACIbL0KOgsWLNDrr7+ut956S5deeqkz7vV6JanHikpzc7Oz+uL1etXZ2amWlpYz1hw5cqTH8x49ejSk5uTnaWlpUVdXV4+VnhPcbrfi4+NDNgAAYK6wgo5t27rvvvv0yiuv6M0339SoUaNCjo8aNUper1fV1dXOWGdnp7Zs2aJJkyZJktLT0zVkyJCQmsbGRtXX1zs1WVlZCgQC2rVrl1Ozc+dOBQKBkJr6+no1NjY6NZs2bZLb7VZ6eno4bQEAAENFh1N877336sUXX9Rrr72muLg4Z0XFsiwNGzZMLpdLJSUleuSRRzR69GiNHj1ajzzyiC666CIVFBQ4tfPmzVNpaalGjBihhIQElZWVady4cZo+fbokacyYMZoxY4aKioq0atUqSdL8+fOVl5en1NRUSVJ2drbGjh0rv9+vxx9/XF999ZXKyspUVFTESg0AAJAUZtB5+umnJUlTpkwJGX/uuedUWFgoSbr//vt1/Phx3XPPPWppaVFGRoY2bdqkuLg4p37FihWKjo7WnDlzdPz4cU2bNk1r165VVFSUU1NeXq7i4mLn01n5+flauXKlczwqKkqVlZW65557NHnyZA0bNkwFBQV64oknwvoGAAAAc53X79GJdPwenZ74PToAgMHue/s9OgAAAIMZQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFhhB52tW7dq1qxZ8vl8crlcevXVV0OOu1yuU26PP/64UzNlypQex++4446Q87S0tMjv98uyLFmWJb/fr9bW1pCaQ4cOadasWYqNjVViYqKKi4vV2dkZbksAAMBQYQedjo4OXX311Vq5cuUpjzc2NoZszz77rFwul2677baQuqKiopC6VatWhRwvKChQXV2dqqqqVFVVpbq6Ovn9fud4d3e3Zs6cqY6ODm3btk0VFRVav369SktLw20JAAAYKjrcB+Tm5io3N/e0x71eb8j+a6+9pqlTp+qHP/xhyPhFF13Uo/aEvXv3qqqqSjU1NcrIyJAkrV69WllZWdq3b59SU1O1adMm7dmzRw0NDfL5fJKk5cuXq7CwUL/+9a8VHx8fbmsAAMAw/XqPzpEjR1RZWal58+b1OFZeXq7ExERdddVVKisr07Fjx5xjO3bskGVZTsiRpMzMTFmWpe3btzs1aWlpTsiRpJycHAWDQdXW1p5yPsFgUG1tbSEbAAAwV9grOuH4/e9/r7i4ON16660h43feeadGjRolr9er+vp6LV68WB988IGqq6slSU1NTfJ4PD3O5/F41NTU5NQkJSWFHB8+fLhiYmKcmpMtXbpUDz/8cF+0BgAAIkC/Bp1nn31Wd955p4YOHRoyXlRU5Hydlpam0aNHa+LEiXr//fc1YcIESd/e1Hwy27ZDxs+l5rsWL16shQsXOvttbW1KSUkJrykAABAx+u2tq3feeUf79u3Tz3/+87PWTpgwQUOGDNH+/fslfXufz5EjR3rUHT161FnF8Xq9PVZuWlpa1NXV1WOl5wS32634+PiQDQAAmKvfgs6aNWuUnp6uq6+++qy1H330kbq6upScnCxJysrKUiAQ0K5du5yanTt3KhAIaNKkSU5NfX29GhsbnZpNmzbJ7XYrPT29j7sBAACRKOy3rtrb2/XJJ584+wcOHFBdXZ0SEhJ02WWXSfr2LaE//OEPWr58eY/Hf/rppyovL9fNN9+sxMRE7dmzR6WlpRo/frwmT54sSRozZoxmzJihoqIi52Pn8+fPV15enlJTUyVJ2dnZGjt2rPx+vx5//HF99dVXKisrU1FRESs1AABAUi9WdN577z2NHz9e48ePlyQtXLhQ48eP169+9SunpqKiQrZt66c//WmPx8fExOhPf/qTcnJylJqaquLiYmVnZ2vz5s2Kiopy6srLyzVu3DhlZ2crOztbP/7xj7Vu3TrneFRUlCorKzV06FBNnjxZc+bM0ezZs/XEE0+E2xIAADCUy7Zte6AnMVDa2tpkWZYCgUC/rAJdvqiyz8/Z3w4umznQUwAA4IzC+fnN37oCAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADBW2EFn69atmjVrlnw+n1wul1599dWQ44WFhXK5XCFbZmZmSE0wGNSCBQuUmJio2NhY5efn6/DhwyE1LS0t8vv9sixLlmXJ7/ertbU1pObQoUOaNWuWYmNjlZiYqOLiYnV2dobbEgAAMFTYQaejo0NXX321Vq5cedqaGTNmqLGx0dk2btwYcrykpEQbNmxQRUWFtm3bpvb2duXl5am7u9upKSgoUF1dnaqqqlRVVaW6ujr5/X7neHd3t2bOnKmOjg5t27ZNFRUVWr9+vUpLS8NtCQAAGCo63Afk5uYqNzf3jDVut1ter/eUxwKBgNasWaN169Zp+vTpkqQXXnhBKSkp2rx5s3JycrR3715VVVWppqZGGRkZkqTVq1crKytL+/btU2pqqjZt2qQ9e/aooaFBPp9PkrR8+XIVFhbq17/+teLj48NtDQAAGKZf7tF5++235fF4dMUVV6ioqEjNzc3OsdraWnV1dSk7O9sZ8/l8SktL0/bt2yVJO3bskGVZTsiRpMzMTFmWFVKTlpbmhBxJysnJUTAYVG1tbX+0BQAAIkzYKzpnk5ubq9tvv10jR47UgQMH9Mtf/lI33nijamtr5Xa71dTUpJiYGA0fPjzkcUlJSWpqapIkNTU1yePx9Di3x+MJqUlKSgo5Pnz4cMXExDg1JwsGgwoGg85+W1vbefUKAAAGtz4POnPnznW+TktL08SJEzVy5EhVVlbq1ltvPe3jbNuWy+Vy9r/79fnUfNfSpUv18MMPn1MfAAAg8vX7x8uTk5M1cuRI7d+/X5Lk9XrV2dmplpaWkLrm5mZnhcbr9erIkSM9znX06NGQmpNXblpaWtTV1dVjpeeExYsXKxAIOFtDQ8N59wcAAAavfg86X375pRoaGpScnCxJSk9P15AhQ1RdXe3UNDY2qr6+XpMmTZIkZWVlKRAIaNeuXU7Nzp07FQgEQmrq6+vV2Njo1GzatElut1vp6emnnIvb7VZ8fHzIBgAAzBX2W1ft7e365JNPnP0DBw6orq5OCQkJSkhI0JIlS3TbbbcpOTlZBw8e1IMPPqjExETdcsstkiTLsjRv3jyVlpZqxIgRSkhIUFlZmcaNG+d8CmvMmDGaMWOGioqKtGrVKknS/PnzlZeXp9TUVElSdna2xo4dK7/fr8cff1xfffWVysrKVFRURIABAACSehF03nvvPU2dOtXZX7hwoSTprrvu0tNPP60PP/xQzz//vFpbW5WcnKypU6fq5ZdfVlxcnPOYFStWKDo6WnPmzNHx48c1bdo0rV27VlFRUU5NeXm5iouLnU9n5efnh/zunqioKFVWVuqee+7R5MmTNWzYMBUUFOiJJ54I/7sAAACM5LJt2x7oSQyUtrY2WZalQCDQL6tAly+q7PNz9reDy2YO9BQAADijcH5+87euAACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGCjvobN26VbNmzZLP55PL5dKrr77qHOvq6tIDDzygcePGKTY2Vj6fT//4j/+ozz//POQcU6ZMkcvlCtnuuOOOkJqWlhb5/X5ZliXLsuT3+9Xa2hpSc+jQIc2aNUuxsbFKTExUcXGxOjs7w20JAAAYKuyg09HRoauvvlorV67scez//u//9P777+uXv/yl3n//fb3yyiv6y1/+ovz8/B61RUVFamxsdLZVq1aFHC8oKFBdXZ2qqqpUVVWluro6+f1+53h3d7dmzpypjo4Obdu2TRUVFVq/fr1KS0vDbQkAABgqOtwH5ObmKjc395THLMtSdXV1yNhvf/tb/eQnP9GhQ4d02WWXOeMXXXSRvF7vKc+zd+9eVVVVqaamRhkZGZKk1atXKysrS/v27VNqaqo2bdqkPXv2qKGhQT6fT5K0fPlyFRYW6te//rXi4+PDbQ0AABim3+/RCQQCcrlcuvjii0PGy8vLlZiYqKuuukplZWU6duyYc2zHjh2yLMsJOZKUmZkpy7K0fft2pyYtLc0JOZKUk5OjYDCo2tra/m0KAABEhLBXdMLxt7/9TYsWLVJBQUHICsudd96pUaNGyev1qr6+XosXL9YHH3zgrAY1NTXJ4/H0OJ/H41FTU5NTk5SUFHJ8+PDhiomJcWpOFgwGFQwGnf22trbz7hEAAAxe/RZ0urq6dMcdd+ibb77RU089FXKsqKjI+TotLU2jR4/WxIkT9f7772vChAmSJJfL1eOctm2HjJ9LzXctXbpUDz/8cK/6AQAAkadf3rrq6urSnDlzdODAAVVXV5/1fpkJEyZoyJAh2r9/vyTJ6/XqyJEjPeqOHj3qrOJ4vd4eKzctLS3q6urqsdJzwuLFixUIBJytoaGhN+0BAIAI0edB50TI2b9/vzZv3qwRI0ac9TEfffSRurq6lJycLEnKyspSIBDQrl27nJqdO3cqEAho0qRJTk19fb0aGxudmk2bNsntdis9Pf2Uz+N2uxUfHx+yAQAAc4X91lV7e7s++eQTZ//AgQOqq6tTQkKCfD6f/v7v/17vv/++/ud//kfd3d3OqktCQoJiYmL06aefqry8XDfffLMSExO1Z88elZaWavz48Zo8ebIkacyYMZoxY4aKioqcj53Pnz9feXl5Sk1NlSRlZ2dr7Nix8vv9evzxx/XVV1+prKxMRUVFBBgAACCpFys67733nsaPH6/x48dLkhYuXKjx48frV7/6lQ4fPqzXX39dhw8f1t/93d8pOTnZ2U58WiomJkZ/+tOflJOTo9TUVBUXFys7O1ubN29WVFSU8zzl5eUaN26csrOzlZ2drR//+Mdat26dczwqKkqVlZUaOnSoJk+erDlz5mj27Nl64oknzvd7AgAADOGybdse6EkMlLa2NlmWpUAg0C+rQJcvquzzc/a3g8tmDvQUAAA4o3B+fvO3rgAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxgo76GzdulWzZs2Sz+eTy+XSq6++GnLctm0tWbJEPp9Pw4YN05QpU/TRRx+F1ASDQS1YsECJiYmKjY1Vfn6+Dh8+HFLT0tIiv98vy7JkWZb8fr9aW1tDag4dOqRZs2YpNjZWiYmJKi4uVmdnZ7gtAQAAQ4UddDo6OnT11Vdr5cqVpzz+2GOP6cknn9TKlSv17rvvyuv16qabbtKxY8ecmpKSEm3YsEEVFRXatm2b2tvblZeXp+7ubqemoKBAdXV1qqqqUlVVlerq6uT3+53j3d3dmjlzpjo6OrRt2zZVVFRo/fr1Ki0tDbclAABgKJdt23avH+xyacOGDZo9e7akb1dzfD6fSkpK9MADD0j6dvUmKSlJjz76qO6++24FAgFdcsklWrdunebOnStJ+vzzz5WSkqKNGzcqJydHe/fu1dixY1VTU6OMjAxJUk1NjbKysvTxxx8rNTVVb7zxhvLy8tTQ0CCfzydJqqioUGFhoZqbmxUfH3/W+be1tcmyLAUCgXOqD9fliyr7/Jz97eCymQM9BQAAziicn999eo/OgQMH1NTUpOzsbGfM7Xbrhhtu0Pbt2yVJtbW16urqCqnx+XxKS0tzanbs2CHLspyQI0mZmZmyLCukJi0tzQk5kpSTk6NgMKja2tq+bAsAAESo6L48WVNTkyQpKSkpZDwpKUmfffaZUxMTE6Phw4f3qDnx+KamJnk8nh7n93g8ITUnP8/w4cMVExPj1JwsGAwqGAw6+21tbeG0BwAAIky/fOrK5XKF7Nu23WPsZCfXnKq+NzXftXTpUufmZsuylJKScsY5AQCAyNanQcfr9UpSjxWV5uZmZ/XF6/Wqs7NTLS0tZ6w5cuRIj/MfPXo0pObk52lpaVFXV1ePlZ4TFi9erEAg4GwNDQ296BIAAESKPg06o0aNktfrVXV1tTPW2dmpLVu2aNKkSZKk9PR0DRkyJKSmsbFR9fX1Tk1WVpYCgYB27drl1OzcuVOBQCCkpr6+Xo2NjU7Npk2b5Ha7lZ6efsr5ud1uxcfHh2wAAMBcYd+j097erk8++cTZP3DggOrq6pSQkKDLLrtMJSUleuSRRzR69GiNHj1ajzzyiC666CIVFBRIkizL0rx581RaWqoRI0YoISFBZWVlGjdunKZPny5JGjNmjGbMmKGioiKtWrVKkjR//nzl5eUpNTVVkpSdna2xY8fK7/fr8ccf11dffaWysjIVFRURYAAAgKReBJ333ntPU6dOdfYXLlwoSbrrrru0du1a3X///Tp+/LjuuecetbS0KCMjQ5s2bVJcXJzzmBUrVig6Olpz5szR8ePHNW3aNK1du1ZRUVFOTXl5uYqLi51PZ+Xn54f87p6oqChVVlbqnnvu0eTJkzVs2DAVFBToiSeeCP+7AAAAjHRev0cn0vF7dHri9+gAAAa7Afs9OgAAAIMJQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFh9HnQuv/xyuVyuHtu9994rSSosLOxxLDMzM+QcwWBQCxYsUGJiomJjY5Wfn6/Dhw+H1LS0tMjv98uyLFmWJb/fr9bW1r5uBwAARLA+DzrvvvuuGhsbna26ulqSdPvttzs1M2bMCKnZuHFjyDlKSkq0YcMGVVRUaNu2bWpvb1deXp66u7udmoKCAtXV1amqqkpVVVWqq6uT3+/v63YAAEAEi+7rE15yySUh+8uWLdOPfvQj3XDDDc6Y2+2W1+s95eMDgYDWrFmjdevWafr06ZKkF154QSkpKdq8ebNycnK0d+9eVVVVqaamRhkZGZKk1atXKysrS/v27VNqampftwUAACJQv96j09nZqRdeeEE/+9nP5HK5nPG3335bHo9HV1xxhYqKitTc3Owcq62tVVdXl7Kzs50xn8+ntLQ0bd++XZK0Y8cOWZblhBxJyszMlGVZTs2pBINBtbW1hWwAAMBc/Rp0Xn31VbW2tqqwsNAZy83NVXl5ud58800tX75c7777rm688UYFg0FJUlNTk2JiYjR8+PCQcyUlJampqcmp8Xg8PZ7P4/E4NaeydOlS554ey7KUkpLSB10CAIDBqs/fuvquNWvWKDc3Vz6fzxmbO3eu83VaWpomTpyokSNHqrKyUrfeeutpz2Xbdsiq0He/Pl3NyRYvXqyFCxc6+21tbYQdAAAM1m9B57PPPtPmzZv1yiuvnLEuOTlZI0eO1P79+yVJXq9XnZ2damlpCVnVaW5u1qRJk5yaI0eO9DjX0aNHlZSUdNrncrvdcrvdvWkHAABEoH576+q5556Tx+PRzJkzz1j35ZdfqqGhQcnJyZKk9PR0DRkyxPm0liQ1Njaqvr7eCTpZWVkKBALatWuXU7Nz504FAgGnBgAAoF9WdL755hs999xzuuuuuxQd/f8/RXt7u5YsWaLbbrtNycnJOnjwoB588EElJibqlltukSRZlqV58+aptLRUI0aMUEJCgsrKyjRu3DjnU1hjxozRjBkzVFRUpFWrVkmS5s+fr7y8PD5xBQAAHP0SdDZv3qxDhw7pZz/7Wch4VFSUPvzwQz3//PNqbW1VcnKypk6dqpdffllxcXFO3YoVKxQdHa05c+bo+PHjmjZtmtauXauoqCinpry8XMXFxc6ns/Lz87Vy5cr+aAcAAEQol23b9kBPYqC0tbXJsiwFAgHFx8f3+fkvX1TZ5+fsbweXnfmtRgAABlo4P7/5W1cAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGP1edBZsmSJXC5XyOb1ep3jtm1ryZIl8vl8GjZsmKZMmaKPPvoo5BzBYFALFixQYmKiYmNjlZ+fr8OHD4fUtLS0yO/3y7IsWZYlv9+v1tbWvm4HAABEsH5Z0bnqqqvU2NjobB9++KFz7LHHHtOTTz6plStX6t1335XX69VNN92kY8eOOTUlJSXasGGDKioqtG3bNrW3tysvL0/d3d1OTUFBgerq6lRVVaWqqirV1dXJ7/f3RzsAACBCRffLSaOjQ1ZxTrBtW7/5zW/00EMP6dZbb5Uk/f73v1dSUpJefPFF3X333QoEAlqzZo3WrVun6dOnS5JeeOEFpaSkaPPmzcrJydHevXtVVVWlmpoaZWRkSJJWr16trKws7du3T6mpqf3RFgAAiDD9sqKzf/9++Xw+jRo1SnfccYf++te/SpIOHDigpqYmZWdnO7Vut1s33HCDtm/fLkmqra1VV1dXSI3P51NaWppTs2PHDlmW5YQcScrMzJRlWU7NqQSDQbW1tYVsAADAXH0edDIyMvT888/rj3/8o1avXq2mpiZNmjRJX375pZqamiRJSUlJIY9JSkpyjjU1NSkmJkbDhw8/Y43H4+nx3B6Px6k5laVLlzr39FiWpZSUlPPqFQAADG59HnRyc3N12223ady4cZo+fboqKyslffsW1QkulyvkMbZt9xg72ck1p6o/23kWL16sQCDgbA0NDefUEwAAiEz9/vHy2NhYjRs3Tvv373fu2zl51aW5udlZ5fF6vers7FRLS8sZa44cOdLjuY4ePdpjtei73G634uPjQzYAAGCufg86wWBQe/fuVXJyskaNGiWv16vq6mrneGdnp7Zs2aJJkyZJktLT0zVkyJCQmsbGRtXX1zs1WVlZCgQC2rVrl1Ozc+dOBQIBpwYAAKDPP3VVVlamWbNm6bLLLlNzc7P+7d/+TW1tbbrrrrvkcrlUUlKiRx55RKNHj9bo0aP1yCOP6KKLLlJBQYEkybIszZs3T6WlpRoxYoQSEhJUVlbmvBUmSWPGjNGMGTNUVFSkVatWSZLmz5+vvLw8PnEFAAAcfR50Dh8+rJ/+9Kf64osvdMkllygzM1M1NTUaOXKkJOn+++/X8ePHdc8996ilpUUZGRnatGmT4uLinHOsWLFC0dHRmjNnjo4fP65p06Zp7dq1ioqKcmrKy8tVXFzsfDorPz9fK1eu7Ot2AABABHPZtm0P9CQGSltbmyzLUiAQ6Jf7dS5fVNnn5+xvB5fNHOgpAABwRuH8/OZvXQEAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMBZBBwAAGIugAwAAjNXnQWfp0qW65pprFBcXJ4/Ho9mzZ2vfvn0hNYWFhXK5XCFbZmZmSE0wGNSCBQuUmJio2NhY5efn6/DhwyE1LS0t8vv9sixLlmXJ7/ertbW1r1sCAAARqs+DzpYtW3TvvfeqpqZG1dXV+vrrr5Wdna2Ojo6QuhkzZqixsdHZNm7cGHK8pKREGzZsUEVFhbZt26b29nbl5eWpu7vbqSkoKFBdXZ2qqqpUVVWluro6+f3+vm4JAABEqOi+PmFVVVXI/nPPPSePx6Pa2lpdf/31zrjb7ZbX6z3lOQKBgNasWaN169Zp+vTpkqQXXnhBKSkp2rx5s3JycrR3715VVVWppqZGGRkZkqTVq1crKytL+/btU2pqal+3BgAAIky/36MTCAQkSQkJCSHjb7/9tjwej6644goVFRWpubnZOVZbW6uuri5lZ2c7Yz6fT2lpadq+fbskaceOHbIsywk5kpSZmSnLspyakwWDQbW1tYVsAADAXP0adGzb1sKFC3XttdcqLS3NGc/NzVV5ebnefPNNLV++XO+++65uvPFGBYNBSVJTU5NiYmI0fPjwkPMlJSWpqanJqfF4PD2e0+PxODUnW7p0qXM/j2VZSklJ6atWAQDAINTnb11913333ac///nP2rZtW8j43Llzna/T0tI0ceJEjRw5UpWVlbr11ltPez7btuVyuZz97359uprvWrx4sRYuXOjst7W1EXYAADBYv63oLFiwQK+//rreeustXXrppWesTU5O1siRI7V//35JktfrVWdnp1paWkLqmpublZSU5NQcOXKkx7mOHj3q1JzM7XYrPj4+ZAMAAObq86Bj27buu+8+vfLKK3rzzTc1atSosz7myy+/VENDg5KTkyVJ6enpGjJkiKqrq52axsZG1dfXa9KkSZKkrKwsBQIB7dq1y6nZuXOnAoGAUwMAAC5sff7W1b333qsXX3xRr732muLi4pz7ZSzL0rBhw9Te3q4lS5botttuU3Jysg4ePKgHH3xQiYmJuuWWW5zaefPmqbS0VCNGjFBCQoLKyso0btw451NYY8aM0YwZM1RUVKRVq1ZJkubPn6+8vDw+cQUAACT1Q9B5+umnJUlTpkwJGX/uuedUWFioqKgoffjhh3r++efV2tqq5ORkTZ06VS+//LLi4uKc+hUrVig6Olpz5szR8ePHNW3aNK1du1ZRUVFOTXl5uYqLi51PZ+Xn52vlypV93RIAAIhQLtu27YGexEBpa2uTZVkKBAL9cr/O5Ysq+/yc/e3gspkDPQUAAM4onJ/f/K0rAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADGIugAAABjEXQAAICxCDoAAMBYBB0AAGAsgg4AADAWQQcAABiLoAMAAIxF0AEAAMYi6AAAAGMRdAAAgLEIOgAAwFgEHQAAYCyCDgAAMFb0QE8Ag8vliyoHegphO7hs5kBPAQAwSLGiAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADG4k9AIOLxZysAAKfDig4AADBWxAedp556SqNGjdLQoUOVnp6ud955Z6CnBAAABomIDjovv/yySkpK9NBDD2n37t267rrrlJubq0OHDg301AAAwCDgsm3bHuhJ9FZGRoYmTJigp59+2hkbM2aMZs+eraVLl5718W1tbbIsS4FAQPHx8X0+v0i8dwQ4He4rAjBYhPPzO2JvRu7s7FRtba0WLVoUMp6dna3t27ef8jHBYFDBYNDZDwQCkr79hvWHb4L/1y/nBQbCZf/8h4GeQtjqH84Z6CkA6Acnfm6fy1pNxAadL774Qt3d3UpKSgoZT0pKUlNT0ykfs3TpUj388MM9xlNSUvpljgAGlvWbgZ4BgP507NgxWZZ1xpqIDTonuFyukH3btnuMnbB48WItXLjQ2f/mm2/01VdfacSIEad9TG+1tbUpJSVFDQ0N/fK22GByIfUqXVj9Xki9ShdWvxdSr9KF1e+F0Ktt2zp27Jh8Pt9ZayM26CQmJioqKqrH6k1zc3OPVZ4T3G633G53yNjFF1/cX1OUJMXHxxv7D+1kF1Kv0oXV74XUq3Rh9Xsh9SpdWP2a3uvZVnJOiNhPXcXExCg9PV3V1dUh49XV1Zo0adIAzQoAAAwmEbuiI0kLFy6U3+/XxIkTlZWVpWeeeUaHDh3SL37xi4GeGgAAGAQiOujMnTtXX375pf71X/9VjY2NSktL08aNGzVy5MiBnprcbrf+5V/+pcdbZSa6kHqVLqx+L6RepQur3wupV+nC6vdC6vVcRPTv0QEAADiTiL1HBwAA4GwIOgAAwFgEHQAAYCyCDgAAMBZB5xw99dRTGjVqlIYOHar09HS98847Z6zfsmWL0tPTNXToUP3whz/Uf/7nf/aoWb9+vcaOHSu3262xY8dqw4YN/TX9sIXT7yuvvKKbbrpJl1xyieLj45WVlaU//vGPITVr166Vy+Xqsf3tb3/r71bOKpxe33777VP28fHHH4fUDdbXNpxeCwsLT9nrVVdd5dQM5td169atmjVrlnw+n1wul1599dWzPiZSr9twe430azbcfiP5ug2310i/bvsDQeccvPzyyyopKdFDDz2k3bt367rrrlNubq4OHTp0yvoDBw7o5ptv1nXXXafdu3frwQcfVHFxsdavX+/U7NixQ3PnzpXf79cHH3wgv9+vOXPmaOfOnd9XW6cVbr9bt27VTTfdpI0bN6q2tlZTp07VrFmztHv37pC6+Ph4NTY2hmxDhw79Plo6rXB7PWHfvn0hfYwePdo5Nlhf23B7/fd///eQHhsaGpSQkKDbb789pG4wvq6S1NHRoauvvlorV648p/pIvm7D7TWSr1kp/H5PiMTrNtxeI/267Rc2zuonP/mJ/Ytf/CJk7Morr7QXLVp0yvr777/fvvLKK0PG7r77bjszM9PZnzNnjj1jxoyQmpycHPuOO+7oo1n3Xrj9nsrYsWPthx9+2Nl/7rnnbMuy+mqKfSbcXt966y1bkt3S0nLacw7W1/Z8X9cNGzbYLpfLPnjwoDM2WF/Xk0myN2zYcMaaSL9uTziXXk8lUq7Zk51Lv5F83X5Xb17bSL5u+worOmfR2dmp2tpaZWdnh4xnZ2dr+/btp3zMjh07etTn5OTovffeU1dX1xlrTnfO70tv+j3ZN998o2PHjikhISFkvL29XSNHjtSll16qvLy8Hv97/L6dT6/jx49XcnKypk2bprfeeivk2GB8bfvidV2zZo2mT5/e4xdyDrbXtbci+bo9X5FyzZ6vSLtu+4Lp1+25IOicxRdffKHu7u4efyg0KSmpxx8UPaGpqemU9V9//bW++OKLM9ac7pzfl970e7Lly5ero6NDc+bMccauvPJKrV27Vq+//rpeeuklDR06VJMnT9b+/fv7dP7h6E2vycnJeuaZZ7R+/Xq98sorSk1N1bRp07R161anZjC+tuf7ujY2NuqNN97Qz3/+85Dxwfi69lYkX7fnK1Ku2d6K1Ov2fF0I1+25iOg/AfF9crlcIfu2bfcYO1v9yePhnvP71Nu5vfTSS1qyZIlee+01eTweZzwzM1OZmZnO/uTJkzVhwgT99re/1X/8x3/03cR7IZxeU1NTlZqa6uxnZWWpoaFBTzzxhK6//vpenfP71Nt5rV27VhdffLFmz54dMj6YX9feiPTrtjci8ZoNV6Rft711oVy3Z8OKzlkkJiYqKiqqR6pvbm7ukf5P8Hq9p6yPjo7WiBEjzlhzunN+X3rT7wkvv/yy5s2bp//6r//S9OnTz1j7gx/8QNdcc82A/g/ifHr9rszMzJA+BuNrez692ratZ599Vn6/XzExMWesHQyva29F8nXbW5F2zfalSLhuz8eFct2eC4LOWcTExCg9PV3V1dUh49XV1Zo0adIpH5OVldWjftOmTZo4caKGDBlyxprTnfP70pt+pW//V1hYWKgXX3xRM2fOPOvz2Laturo6JScnn/ece6u3vZ5s9+7dIX0Mxtf2fHrdsmWLPvnkE82bN++szzMYXtfeiuTrtjci8ZrtS5Fw3Z6PC+W6PSff//3PkaeiosIeMmSIvWbNGnvPnj12SUmJHRsb69zFvmjRItvv9zv1f/3rX+2LLrrI/ud//md7z5499po1a+whQ4bY//3f/+3U/O///q8dFRVlL1u2zN67d6+9bNkyOzo62q6pqfne+ztZuP2++OKLdnR0tP273/3ObmxsdLbW1lanZsmSJXZVVZX96aef2rt377b/6Z/+yY6OjrZ37tz5vff3XeH2umLFCnvDhg32X/7yF7u+vt5etGiRLclev369UzNYX9twez3hH/7hH+yMjIxTnnOwvq62bdvHjh2zd+/ebe/evduWZD/55JP27t277c8++8y2bbOu23B7jeRr1rbD7zeSr9twez0hUq/b/kDQOUe/+93v7JEjR9oxMTH2hAkT7C1btjjH7rrrLvuGG24IqX/77bft8ePH2zExMfbll19uP/300z3O+Yc//MFOTU21hwwZYl955ZUhF91AC6ffG264wZbUY7vrrrucmpKSEvuyyy6zY2Ji7EsuucTOzs62t2/f/j12dHrh9Proo4/aP/rRj+yhQ4faw4cPt6+99lq7srKyxzkH62sb7r/j1tZWe9iwYfYzzzxzyvMN5tf1xEeKT/fv0qTrNtxeI/2aDbffSL5ue/PvOJKv2/7gsu3/7247AAAAw3CPDgAAMBZBBwAAGIugAwAAjEXQAQAAxiLoAAAAYxF0AACAsQg6AADAWAQdAABgLIIOAAAwFkEHAAAYi6ADAACMRdABAADG+n8MEKx/HGTdxQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(val_df_dummies['numclaims'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9297384e-11\n",
      "2408.1428\n",
      "1.1419866e-08\n",
      "10.456493\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkYAAAGdCAYAAAD3zLwdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAhpElEQVR4nO3db1CVdf7/8dcJgYiBa0GEw1nJmB1zNVhnopY//dFKUUZkXZvRojmjMw7Wmjr8hCndbuTu7IprpXuDrXWdJstsab5Tts1gJI1KMYIaK5OYOTZp4soR0+MBjT0QXr8bfbxmj5iKHkT0+Zg5M55zvc851/XhNDy7OAdctm3bAgAAgG4b7B0AAAC4URBGAAAABmEEAABgEEYAAAAGYQQAAGAQRgAAAAZhBAAAYBBGAAAAxrDB3oHBdO7cOR07dkxxcXFyuVyDvTsAAOAK2Latzs5OeTwe3XZbeM/x3NJhdOzYMaWlpQ32bgAAgKvQ2tqqkSNHhvUxb+kwiouLk/TjwsbHxw/y3gAAgCvR0dGhtLQ05/t4ON3SYXT+x2fx8fGEEQAAQ8xAvA2GN18DAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIAxbLB34GZ219Lqy84cXjntOuwJAAC4EpwxAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACjX2FUUVGh+++/X3FxcUpOTtaMGTN04MCBkBnbtrV8+XJ5PB7FxMRo4sSJ2rdvX8hMMBjUokWLlJSUpNjYWBUVFeno0aMhM36/X16vV5ZlybIseb1enT59OmTmyJEjmj59umJjY5WUlKTFixeru7u7P4cEAADg6FcY1dXV6dlnn1VjY6Nqa2v1ww8/KD8/X2fPnnVmVq1apdWrV6uyslK7d++W2+3W5MmT1dnZ6cyUlpZq06ZNqqqqUn19vc6cOaPCwkL19vY6M8XFxWpublZNTY1qamrU3Nwsr9frbO/t7dW0adN09uxZ1dfXq6qqSu+9957KysquZT0AAMAtzGXbtn21dz5x4oSSk5NVV1enhx9+WLZty+PxqLS0VM8//7ykH88OpaSk6C9/+YuefvppBQIBjRgxQhs2bNDs2bMlSceOHVNaWpo2b96sKVOmaP/+/Ro3bpwaGxuVnZ0tSWpsbFRubq6++uorjRkzRh999JEKCwvV2toqj8cjSaqqqtLcuXPV3t6u+Pj4y+5/R0eHLMtSIBC4ovn+umtp9WVnDq+cFvbnBQDgZjaQ37+v6T1GgUBAkpSYmChJOnTokHw+n/Lz852Z6OhoTZgwQTt27JAkNTU1qaenJ2TG4/EoIyPDmWloaJBlWU4USVJOTo4sywqZycjIcKJIkqZMmaJgMKimpqaL7m8wGFRHR0fIBQAA4LyrDiPbtrVkyRI9+OCDysjIkCT5fD5JUkpKSshsSkqKs83n8ykqKkoJCQmXnElOTu7znMnJySEzFz5PQkKCoqKinJkLVVRUOO9ZsixLaWlp/T1sAABwE7vqMFq4cKG++OIL/fOf/+yzzeVyhVy3bbvPbRe6cOZi81cz87+WLVumQCDgXFpbWy+5TwAA4NZyVWG0aNEiffjhh9q2bZtGjhzp3O52uyWpzxmb9vZ25+yO2+1Wd3e3/H7/JWeOHz/e53lPnDgRMnPh8/j9fvX09PQ5k3RedHS04uPjQy4AAADn9SuMbNvWwoUL9f7772vr1q1KT08P2Z6eni63263a2lrntu7ubtXV1SkvL0+SlJWVpcjIyJCZtrY2tbS0ODO5ubkKBALatWuXM7Nz504FAoGQmZaWFrW1tTkzW7ZsUXR0tLKysvpzWAAAAJKkYf0ZfvbZZ/XOO+/oX//6l+Li4pwzNpZlKSYmRi6XS6WlpVqxYoVGjx6t0aNHa8WKFbrjjjtUXFzszM6bN09lZWUaPny4EhMTVV5erszMTE2aNEmSNHbsWE2dOlUlJSVau3atJGn+/PkqLCzUmDFjJEn5+fkaN26cvF6vXnrpJZ06dUrl5eUqKSnhTBAAALgq/Qqj1157TZI0ceLEkNvfeOMNzZ07V5L03HPPqaurSwsWLJDf71d2dra2bNmiuLg4Z37NmjUaNmyYZs2apa6uLj322GNav369IiIinJmNGzdq8eLFzqfXioqKVFlZ6WyPiIhQdXW1FixYoAceeEAxMTEqLi7Wyy+/3K8FAAAAOO+afo/RUMfvMQIAYOi5YX+PEQAAwM2EMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAKPfYfTpp59q+vTp8ng8crlc+uCDD0K2z507Vy6XK+SSk5MTMhMMBrVo0SIlJSUpNjZWRUVFOnr0aMiM3++X1+uVZVmyLEter1enT58OmTly5IimT5+u2NhYJSUlafHixeru7u7vIQEAAEi6ijA6e/asxo8fr8rKyp+cmTp1qtra2pzL5s2bQ7aXlpZq06ZNqqqqUn19vc6cOaPCwkL19vY6M8XFxWpublZNTY1qamrU3Nwsr9frbO/t7dW0adN09uxZ1dfXq6qqSu+9957Kysr6e0gAAACSpGH9vUNBQYEKCgouORMdHS23233RbYFAQK+//ro2bNigSZMmSZLefvttpaWl6ZNPPtGUKVO0f/9+1dTUqLGxUdnZ2ZKkdevWKTc3VwcOHNCYMWO0ZcsWffnll2ptbZXH45EkvfLKK5o7d67+/Oc/Kz4+vr+HBgAAbnED8h6j7du3Kzk5WXfffbdKSkrU3t7ubGtqalJPT4/y8/Od2zwejzIyMrRjxw5JUkNDgyzLcqJIknJycmRZVshMRkaGE0WSNGXKFAWDQTU1NV10v4LBoDo6OkIuAAAA54U9jAoKCrRx40Zt3bpVr7zyinbv3q1HH31UwWBQkuTz+RQVFaWEhISQ+6WkpMjn8zkzycnJfR47OTk5ZCYlJSVke0JCgqKiopyZC1VUVDjvWbIsS2lpadd8vAAA4ObR7x+lXc7s2bOdf2dkZOi+++7TqFGjVF1drZkzZ/7k/Wzblsvlcq7/77+vZeZ/LVu2TEuWLHGud3R0EEcAAMAx4B/XT01N1ahRo3Tw4EFJktvtVnd3t/x+f8hce3u7cwbI7Xbr+PHjfR7rxIkTITMXnhny+/3q6enpcybpvOjoaMXHx4dcAAAAzhvwMDp58qRaW1uVmpoqScrKylJkZKRqa2udmba2NrW0tCgvL0+SlJubq0AgoF27djkzO3fuVCAQCJlpaWlRW1ubM7NlyxZFR0crKytroA8LAADchPr9o7QzZ87o66+/dq4fOnRIzc3NSkxMVGJiopYvX67HH39cqampOnz4sH7/+98rKSlJv/3tbyVJlmVp3rx5Kisr0/Dhw5WYmKjy8nJlZmY6n1IbO3aspk6dqpKSEq1du1aSNH/+fBUWFmrMmDGSpPz8fI0bN05er1cvvfSSTp06pfLycpWUlHAmCAAAXJV+h9Hnn3+uRx55xLl+/j07c+bM0Wuvvaa9e/fqrbfe0unTp5WamqpHHnlE7777ruLi4pz7rFmzRsOGDdOsWbPU1dWlxx57TOvXr1dERIQzs3HjRi1evNj59FpRUVHI706KiIhQdXW1FixYoAceeEAxMTEqLi7Wyy+/3P9VAAAAkOSybdse7J0YLB0dHbIsS4FAYEDOMt21tPqyM4dXTgv78wIAcDMbyO/f/K00AAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAwCCMAAACDMAIAADAIIwAAAIMwAgAAMAgjAAAAgzACAAAw+h1Gn376qaZPny6PxyOXy6UPPvggZLtt21q+fLk8Ho9iYmI0ceJE7du3L2QmGAxq0aJFSkpKUmxsrIqKinT06NGQGb/fL6/XK8uyZFmWvF6vTp8+HTJz5MgRTZ8+XbGxsUpKStLixYvV3d3d30MCAACQdBVhdPbsWY0fP16VlZUX3b5q1SqtXr1alZWV2r17t9xutyZPnqzOzk5nprS0VJs2bVJVVZXq6+t15swZFRYWqre315kpLi5Wc3OzampqVFNTo+bmZnm9Xmd7b2+vpk2bprNnz6q+vl5VVVV67733VFZW1t9DAgAAkCS5bNu2r/rOLpc2bdqkGTNmSPrxbJHH41Fpaamef/55ST+eHUpJSdFf/vIXPf300woEAhoxYoQ2bNig2bNnS5KOHTumtLQ0bd68WVOmTNH+/fs1btw4NTY2Kjs7W5LU2Nio3NxcffXVVxozZow++ugjFRYWqrW1VR6PR5JUVVWluXPnqr29XfHx8Zfd/46ODlmWpUAgcEXz/XXX0urLzhxeOS3szwsAwM1sIL9/h/U9RocOHZLP51N+fr5zW3R0tCZMmKAdO3ZIkpqamtTT0xMy4/F4lJGR4cw0NDTIsiwniiQpJydHlmWFzGRkZDhRJElTpkxRMBhUU1NTOA8LAADcIoaF88F8Pp8kKSUlJeT2lJQUffvtt85MVFSUEhIS+sycv7/P51NycnKfx09OTg6ZufB5EhISFBUV5cxcKBgMKhgMOtc7Ojr6c3gAAOAmNyCfSnO5XCHXbdvuc9uFLpy52PzVzPyviooK583clmUpLS3tkvsEAABuLWENI7fbLUl9zti0t7c7Z3fcbre6u7vl9/svOXP8+PE+j3/ixImQmQufx+/3q6enp8+ZpPOWLVumQCDgXFpbW6/iKAEAwM0qrGGUnp4ut9ut2tpa57bu7m7V1dUpLy9PkpSVlaXIyMiQmba2NrW0tDgzubm5CgQC2rVrlzOzc+dOBQKBkJmWlha1tbU5M1u2bFF0dLSysrIuun/R0dGKj48PuQAAAJzX7/cYnTlzRl9//bVz/dChQ2publZiYqLuvPNOlZaWasWKFRo9erRGjx6tFStW6I477lBxcbEkybIszZs3T2VlZRo+fLgSExNVXl6uzMxMTZo0SZI0duxYTZ06VSUlJVq7dq0kaf78+SosLNSYMWMkSfn5+Ro3bpy8Xq9eeuklnTp1SuXl5SopKSF4AADAVel3GH3++ed65JFHnOtLliyRJM2ZM0fr16/Xc889p66uLi1YsEB+v1/Z2dnasmWL4uLinPusWbNGw4YN06xZs9TV1aXHHntM69evV0REhDOzceNGLV682Pn0WlFRUcjvToqIiFB1dbUWLFigBx54QDExMSouLtbLL7/c/1UAAADQNf4eo6GO32MEAMDQM2R+jxEAAMBQRhgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIBBGAEAABiEEQAAgEEYAQAAGIQRAACAQRgBAAAYhBEAAIAR9jBavny5XC5XyMXtdjvbbdvW8uXL5fF4FBMTo4kTJ2rfvn0hjxEMBrVo0SIlJSUpNjZWRUVFOnr0aMiM3++X1+uVZVmyLEter1enT58O9+EAAIBbyICcMbrnnnvU1tbmXPbu3etsW7VqlVavXq3Kykrt3r1bbrdbkydPVmdnpzNTWlqqTZs2qaqqSvX19Tpz5owKCwvV29vrzBQXF6u5uVk1NTWqqalRc3OzvF7vQBwOAAC4RQwbkAcdNizkLNF5tm3rr3/9q1544QXNnDlTkvTmm28qJSVF77zzjp5++mkFAgG9/vrr2rBhgyZNmiRJevvtt5WWlqZPPvlEU6ZM0f79+1VTU6PGxkZlZ2dLktatW6fc3FwdOHBAY8aMGYjDAgAAN7kBOWN08OBBeTwepaen64knntA333wjSTp06JB8Pp/y8/Od2ejoaE2YMEE7duyQJDU1NamnpydkxuPxKCMjw5lpaGiQZVlOFElSTk6OLMtyZi4mGAyqo6Mj5AIAAHBe2MMoOztbb731lj7++GOtW7dOPp9PeXl5OnnypHw+nyQpJSUl5D4pKSnONp/Pp6ioKCUkJFxyJjk5uc9zJycnOzMXU1FR4bwnybIspaWlXdOxAgCAm0vYw6igoECPP/64MjMzNWnSJFVXV0v68Udm57lcrpD72Lbd57YLXThzsfnLPc6yZcsUCAScS2tr6xUdEwAAuDUM+Mf1Y2NjlZmZqYMHDzrvO7rwrE57e7tzFsntdqu7u1t+v/+SM8ePH+/zXCdOnOhzNup/RUdHKz4+PuQCAABw3oCHUTAY1P79+5Wamqr09HS53W7V1tY627u7u1VXV6e8vDxJUlZWliIjI0Nm2tra1NLS4szk5uYqEAho165dzszOnTsVCAScGQAAgP4K+6fSysvLNX36dN15551qb2/Xn/70J3V0dGjOnDlyuVwqLS3VihUrNHr0aI0ePVorVqzQHXfcoeLiYkmSZVmaN2+eysrKNHz4cCUmJqq8vNz50ZwkjR07VlOnTlVJSYnWrl0rSZo/f74KCwv5RBoAALhqYQ+jo0eP6sknn9R3332nESNGKCcnR42NjRo1apQk6bnnnlNXV5cWLFggv9+v7OxsbdmyRXFxcc5jrFmzRsOGDdOsWbPU1dWlxx57TOvXr1dERIQzs3HjRi1evNj59FpRUZEqKyvDfTgAAOAW4rJt2x7snRgsHR0dsixLgUBgQN5vdNfS6svOHF45LezPCwDAzWwgv3/zt9IAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAAzCCAAAwCCMAAAADMIIAADAIIwAAAAMwggAAMAgjAAAAIwhH0avvvqq0tPTdfvttysrK0ufffbZYO8SAAAYooZ0GL377rsqLS3VCy+8oD179uihhx5SQUGBjhw5Mti7BgAAhiCXbdv2YO/E1crOzta9996r1157zblt7NixmjFjhioqKi57/46ODlmWpUAgoPj4+LDv311Lq8PyOIdXTgvL4wAAcDMYyO/fw8L6aNdRd3e3mpqatHTp0pDb8/PztWPHjoveJxgMKhgMOtcDgYCkHxd4IJwLfh+Wx7nz//1fWB6n5Q9TwvI4AAAMpvPftwfi3M6QDaPvvvtOvb29SklJCbk9JSVFPp/vovepqKjQH/7whz63p6WlDcg+3misvw72HgAAED6dnZ2yLCusjzlkw+g8l8sVct227T63nbds2TItWbLEuX7u3DmdOnVKw4cP/8n7XK2Ojg6lpaWptbV1QH5Mh75Y88HBug8O1v36Y80Hx8XW3bZtdXZ2yuPxhP35hmwYJSUlKSIios/Zofb29j5nkc6Ljo5WdHR0yG0/+9nPBmoXJUnx8fH8B3SdseaDg3UfHKz79ceaD44L1z3cZ4rOG7KfSouKilJWVpZqa2tDbq+trVVeXt4g7RUAABjKhuwZI0lasmSJvF6v7rvvPuXm5uof//iHjhw5omeeeWawdw0AAAxBQzqMZs+erZMnT+qPf/yj2tralJGRoc2bN2vUqFGDvWuKjo7Wiy++2OdHdxg4rPngYN0HB+t+/bHmg+N6r/uQ/j1GAAAA4TRk32MEAAAQboQRAACAQRgBAAAYhBEAAIBBGA2AV199Venp6br99tuVlZWlzz77bLB3achavny5XC5XyMXtdjvbbdvW8uXL5fF4FBMTo4kTJ2rfvn0hjxEMBrVo0SIlJSUpNjZWRUVFOnr06PU+lBvap59+qunTp8vj8cjlcumDDz4I2R6udfb7/fJ6vbIsS5Zlyev16vTp0wN8dDemy6353Llz+7z2c3JyQmZY8/6pqKjQ/fffr7i4OCUnJ2vGjBk6cOBAyAyv9fC7knW/kV7vhFGYvfvuuyotLdULL7ygPXv26KGHHlJBQYGOHDky2Ls2ZN1zzz1qa2tzLnv37nW2rVq1SqtXr1ZlZaV2794tt9utyZMnq7Oz05kpLS3Vpk2bVFVVpfr6ep05c0aFhYXq7e0djMO5IZ09e1bjx49XZWXlRbeHa52Li4vV3Nysmpoa1dTUqLm5WV6vd8CP70Z0uTWXpKlTp4a89jdv3hyynTXvn7q6Oj377LNqbGxUbW2tfvjhB+Xn5+vs2bPODK/18LuSdZduoNe7jbD69a9/bT/zzDMht/3yl7+0ly5dOkh7NLS9+OKL9vjx4y+67dy5c7bb7bZXrlzp3Pbf//7XtizL/vvf/27btm2fPn3ajoyMtKuqqpyZ//znP/Ztt91m19TUDOi+D1WS7E2bNjnXw7XOX375pS3JbmxsdGYaGhpsSfZXX301wEd1Y7twzW3btufMmWP/5je/+cn7sObXrr293ZZk19XV2bbNa/16uXDdbfvGer1zxiiMuru71dTUpPz8/JDb8/PztWPHjkHaq6Hv4MGD8ng8Sk9P1xNPPKFvvvlGknTo0CH5fL6Q9Y6OjtaECROc9W5qalJPT0/IjMfjUUZGBl+TKxSudW5oaJBlWcrOznZmcnJyZFkWX4ufsH37diUnJ+vuu+9WSUmJ2tvbnW2s+bULBAKSpMTEREm81q+XC9f9vBvl9U4YhdF3332n3t7ePn/ENiUlpc8fu8WVyc7O1ltvvaWPP/5Y69atk8/nU15enk6ePOms6aXW2+fzKSoqSgkJCT85g0sL1zr7fD4lJyf3efzk5GS+FhdRUFCgjRs3auvWrXrllVe0e/duPfroowoGg5JY82tl27aWLFmiBx98UBkZGZJ4rV8PF1t36cZ6vQ/pPwlyo3K5XCHXbdvucxuuTEFBgfPvzMxM5ebm6he/+IXefPNN5415V7PefE36LxzrfLF5vhYXN3v2bOffGRkZuu+++zRq1ChVV1dr5syZP3k/1vzKLFy4UF988YXq6+v7bOO1PnB+at1vpNc7Z4zCKCkpSREREX3KtL29vc//geDqxMbGKjMzUwcPHnQ+nXap9Xa73eru7pbf7//JGVxauNbZ7Xbr+PHjfR7/xIkTfC2uQGpqqkaNGqWDBw9KYs2vxaJFi/Thhx9q27ZtGjlypHM7r/WB9VPrfjGD+XonjMIoKipKWVlZqq2tDbm9trZWeXl5g7RXN5dgMKj9+/crNTVV6enpcrvdIevd3d2turo6Z72zsrIUGRkZMtPW1qaWlha+JlcoXOucm5urQCCgXbt2OTM7d+5UIBDga3EFTp48qdbWVqWmpkpiza+GbdtauHCh3n//fW3dulXp6ekh23mtD4zLrfvFDOrr/Yrfpo0rUlVVZUdGRtqvv/66/eWXX9qlpaV2bGysffjw4cHetSGprKzM3r59u/3NN9/YjY2NdmFhoR0XF+es58qVK23Lsuz333/f3rt3r/3kk0/aqampdkdHh/MYzzzzjD1y5Ej7k08+sf/973/bjz76qD1+/Hj7hx9+GKzDuuF0dnbae/bssffs2WNLslevXm3v2bPH/vbbb23bDt86T5061f7Vr35lNzQ02A0NDXZmZqZdWFh43Y/3RnCpNe/s7LTLysrsHTt22IcOHbK3bdtm5+bm2j//+c9Z82vwu9/9zrYsy96+fbvd1tbmXL7//ntnhtd6+F1u3W+01zthNAD+9re/2aNGjbKjoqLse++9N+Qjieif2bNn26mpqXZkZKTt8XjsmTNn2vv27XO2nzt3zn7xxRdtt9ttR0dH2w8//LC9d+/ekMfo6uqyFy5caCcmJtoxMTF2YWGhfeTIket9KDe0bdu22ZL6XObMmWPbdvjW+eTJk/ZTTz1lx8XF2XFxcfZTTz1l+/3+63SUN5ZLrfn3339v5+fn2yNGjLAjIyPtO++8054zZ06f9WTN++di6y3JfuONN5wZXuvhd7l1v9Fe7y6z0wAAALc83mMEAABgEEYAAAAGYQQAAGAQRgAAAAZhBAAAYBBGAAAABmEEAABgEEYAAAAGYQQAAGAQRgAAAAZhBAAAYBBGAAAAxv8HI7Ak1ZmNrFkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_costs = cost_model.predict(val_df_dummies)\n",
    "\n",
    "print(predicted_costs.min())\n",
    "print(predicted_costs.max())\n",
    "print(np.median(predicted_costs))\n",
    "print(predicted_costs.mean())\n",
    "\n",
    "# plt.hist(df['claimcst0'], bins=50)\n",
    "plt.hist(predicted_costs, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.660214e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2.459705e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.356506e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2.039466e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.974446e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       Predict\n",
       "0   1  2.660214e-08\n",
       "1   2  2.459705e-08\n",
       "2   3  1.356506e-08\n",
       "3   4  2.039466e-08\n",
       "4   5  1.974446e-08"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({'id': ids, 'Predict': predicted_costs.reshape(-1,)})\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "travelers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
